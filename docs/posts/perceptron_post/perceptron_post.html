<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andre Xiao">
<meta name="dcterms.date" content="2024-04-07">
<meta name="description" content="Implementing and experimenting with the perceptron algorithm.">

<title>CSCI 0451 Machine Learning - Perceptron</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CSCI 0451 Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Perceptron</h1>
                  <div>
        <div class="description">
          Implementing and experimenting with the perceptron algorithm.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andre Xiao </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 7, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*b7kNF1-TcrcogZAh2i5l4Q.png" class="img-fluid figure-img"></p>
<figcaption>Image source: https://miro.medium.com/v2/resize:fit:1400/1*b7kNF1-TcrcogZAh2i5l4Q.png</figcaption>
</figure>
</div>
<p>Perceptron Implementation: <a href="https://github.com/andrexiao85/andrexiao85.github.io/blob/main/posts/perceptron_post/perceptron.py">perceptron.py</a></p>
<section id="abstract" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="abstract"><span class="header-section-number">1</span> Abstract</h2>
<p>This post implements the perceptron algorithm and minibatch perceptron algorithm and performs various experiments using both. For the simple perceptron algorithm, I run the following experiments:</p>
<ol type="1">
<li>Visualize the updates of the perceptron for linearly separable data and the final separating line.</li>
<li>Visualize the updates of the perceptron for not linearly separable data and the final boundary line in the final iteration since it does not converge.</li>
<li>Visualize the loss of the perceptron for data with 5 feature and determine if it converges or not.</li>
</ol>
<p>For the minibatch perceptron algorithm, I run the following experiments:</p>
<ol type="1">
<li>Visualize the updates of the perceptron with <code>k=1</code> and show that it performs similarly to the regular perceptron.</li>
<li>Visualize the updates of the perceptron with <code>k=10</code> and show it can still find a separating line in 2d.</li>
<li>Visualize the updates of the perceptron with <code>k=n</code> and show that it can still converge even if the data is not linearly separable, provided that the learning rate <span class="math inline">\(\alpha\)</span> is not separable.</li>
</ol>
<p>At the end, I discuss the runtime complexity of a single iteration of both the regular perceptron algorithm and the minibatch perceptron algorithm.</p>
</section>
<section id="implementing-perceptron" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="implementing-perceptron"><span class="header-section-number">2</span> Implementing Perceptron</h2>
<p>The perceptron algorithm aims to find a good choice of weights <span class="math inline">\(\mathbf{w}\)</span> that makes the loss small using the following algorithm:</p>
<ol type="1">
<li>Start with a <em>random</em> <span class="math inline">\(\mathbf{w}^{(0)}\)</span>.</li>
<li>In each time-step <span class="math inline">\(t\)</span>:
<ul>
<li>Pick a random data point <span class="math inline">\(i\in{1,...,n}\)</span></li>
<li>Compute <span class="math inline">\(s^{(t)}_i = \langle\mathbf{w}^{(t)}, \mathbf{x}_i\rangle\)</span>.</li>
<li>If <span class="math inline">\(s^{(t)}_i y_i \geq 0\)</span>, then point <span class="math inline">\(i\)</span> is correctly classified â€“ do nothing!</li>
<li>Else, if <span class="math inline">\(s^{(t)}_i y_i &lt; 0\)</span>, then perform update <span class="math display">\[\mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + y_i\mathbf{x}_i.\]</span></li>
<li>These steps can also be written as: <span class="math display">\[\mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \mathbb{1}[s_i y_i &lt; 0]y_i\mathbf{x}_i.\]</span></li>
</ul></li>
</ol>
<p>The loss is defined to be the proportion of points that are <em>misclassified</em>. The only differences with the minibatch algorithm is that for each time-step <span class="math inline">\(t\)</span>, it picks <span class="math inline">\(k\)</span> random points <span class="math inline">\(i_1,...,i_k\)</span> instead of a single point and the update is defined as: <span class="math display">\[\mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \frac{\alpha}{k}\sum_{\ell=1}^k\mathbb{1}[s_{i_\ell} y_{i_\ell} &lt; 0]y_{i_\ell}\mathbf{x}_{i_\ell}\]</span> where <span class="math inline">\(\alpha\)</span> is the learning rate hyperparameter which can be tuned to achieve good results.</p>
<p>My perceptron uses the function <code>perceptron.grad()</code> to calculate <span class="math display">\[\mathbb{1}[s_i y_i &lt; 0]y_i\mathbf{x}_i\]</span> for each iteration. The minibatch perceptron uses the function <code>perceptron.mbgrad()</code> to calculate <span class="math display">\[\frac{\alpha}{k}\sum_{\ell=1}^k\mathbb{1}[s_{i_\ell} y_{i_\ell} &lt; 0]y_{i_\ell}\mathbf{x}_{i_\ell}\]</span> for each iteration.</p>
<p>Below are my implementations of <code>perceptron.grad()</code> and <code>perceptron.mbgrad()</code> in Python.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(<span class="va">self</span>, X, y):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> X<span class="op">@</span>self.w</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (s<span class="op">*</span>y <span class="op">&lt;</span> <span class="dv">0</span>) <span class="op">*</span> y<span class="op">*</span>X</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mbgrad(<span class="va">self</span>, X, y, k <span class="op">=</span> <span class="dv">2</span>, alpha <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> X<span class="op">@</span>self.w</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(((s<span class="op">*</span>y <span class="op">&lt;</span> <span class="dv">0</span>) <span class="op">*</span> y)[:, <span class="va">None</span>] <span class="op">*</span>X) <span class="op">*</span> alpha <span class="op">/</span> k</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To test my implementation of the regular perceptron algorithm, I will generate a data set and run a minimal training loop that eventually achieves <code>loss = 0</code>.</p>
<div id="cell-4" class="cell" data-execution_count="69">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> perceptron <span class="im">import</span> Perceptron, PerceptronOptimizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
<p>But before running the test, I will first define some functions that will be used to generate and visualize the data.</p>
<div id="cell-6" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="annotated-cell-3"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="annotated-cell-3-2"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1">1</button><span id="annotated-cell-3-3" class="code-annotation-target"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perceptron_data(n_points <span class="op">=</span> <span class="dv">300</span>, noise <span class="op">=</span> <span class="fl">0.2</span>, dims <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="annotated-cell-3-4"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="annotated-cell-3-5"><a href="#annotated-cell-3-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.arange(n_points) <span class="op">&gt;=</span> <span class="bu">int</span>(n_points<span class="op">/</span><span class="dv">2</span>)</span>
<span id="annotated-cell-3-6"><a href="#annotated-cell-3-6" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> y[:, <span class="va">None</span>] <span class="op">+</span> torch.normal(<span class="fl">0.0</span>, noise, size <span class="op">=</span> (n_points, dims))</span>
<span id="annotated-cell-3-7"><a href="#annotated-cell-3-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.cat((X, torch.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>))), <span class="dv">1</span>)</span>
<span id="annotated-cell-3-8"><a href="#annotated-cell-3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-9"><a href="#annotated-cell-3-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert y from {0, 1} to {-1, 1}</span></span>
<span id="annotated-cell-3-10"><a href="#annotated-cell-3-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>y <span class="op">-</span> <span class="dv">1</span></span>
<span id="annotated-cell-3-11"><a href="#annotated-cell-3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-12"><a href="#annotated-cell-3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="annotated-cell-3-13"><a href="#annotated-cell-3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-14"><a href="#annotated-cell-3-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="annotated-cell-3-15"><a href="#annotated-cell-3-15" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2">2</button><span id="annotated-cell-3-16" class="code-annotation-target"><a href="#annotated-cell-3-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_perceptron_data(X, y, ax):</span>
<span id="annotated-cell-3-17"><a href="#annotated-cell-3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> X.shape[<span class="dv">1</span>] <span class="op">==</span> <span class="dv">3</span>, <span class="st">"This function only works for data created with p_dims == 2"</span></span>
<span id="annotated-cell-3-18"><a href="#annotated-cell-3-18" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="annotated-cell-3-19"><a href="#annotated-cell-3-19" aria-hidden="true" tabindex="-1"></a>    markers <span class="op">=</span> [<span class="st">"o"</span> , <span class="st">","</span>]</span>
<span id="annotated-cell-3-20"><a href="#annotated-cell-3-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="annotated-cell-3-21"><a href="#annotated-cell-3-21" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> y <span class="op">==</span> targets[i]</span>
<span id="annotated-cell-3-22"><a href="#annotated-cell-3-22" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[ix,<span class="dv">0</span>], X[ix,<span class="dv">1</span>], s <span class="op">=</span> <span class="dv">20</span>,  c <span class="op">=</span> y[ix], facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"darkgrey"</span>, cmap <span class="op">=</span> <span class="st">"BrBG"</span>, vmin <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>, vmax <span class="op">=</span> <span class="dv">2</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>, marker <span class="op">=</span> markers[i])</span>
<span id="annotated-cell-3-23"><a href="#annotated-cell-3-23" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$x_1$"</span>, ylabel <span class="op">=</span> <span class="vs">r"$x_2$"</span>)</span>
<span id="annotated-cell-3-24"><a href="#annotated-cell-3-24" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="3">3</button><span id="annotated-cell-3-25" class="code-annotation-target"><a href="#annotated-cell-3-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_line(w, x_min, x_max, ax, <span class="op">**</span>kwargs):</span>
<span id="annotated-cell-3-26"><a href="#annotated-cell-3-26" aria-hidden="true" tabindex="-1"></a>    w_ <span class="op">=</span> w.flatten()</span>
<span id="annotated-cell-3-27"><a href="#annotated-cell-3-27" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.linspace(x_min, x_max, <span class="dv">101</span>)</span>
<span id="annotated-cell-3-28"><a href="#annotated-cell-3-28" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="op">-</span>(w_[<span class="dv">0</span>]<span class="op">*</span>x <span class="op">+</span> w_[<span class="dv">2</span>])<span class="op">/</span>w_[<span class="dv">1</span>]</span>
<span id="annotated-cell-3-29"><a href="#annotated-cell-3-29" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> ax.plot(x, y, <span class="op">**</span>kwargs)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="3,5,6,7,10,12" data-code-annotation="1">Generates perceptron data.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="16,17,18,19,20,21,22,23" data-code-annotation="2">Plots perceptron data.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="25,26,27,28,29" data-code-annotation="3">Draws decision boundary line.</span>
</dd>
</dl>
</div>
</div>
<p>I will also define a training loop function that runs the perceptron until <code>loss = 0</code> or the max amount of iterations is reached and a function that plots the decision boundary and loss for only updates during the training loop.</p>
<div id="cell-9" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="annotated-cell-4"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-4-1"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> <span class="bu">reduce</span></span>
<span id="annotated-cell-4-2"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="1">1</button><span id="annotated-cell-4-3" class="code-annotation-target"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dims(n):</span>
<span id="annotated-cell-4-4"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a>    lower_factors <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">int</span>(n<span class="op">**</span><span class="fl">0.5</span>) <span class="op">+</span> <span class="dv">1</span>) <span class="cf">if</span> n <span class="op">%</span> i <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="annotated-cell-4-5"><a href="#annotated-cell-4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lower_factors[<span class="op">-</span><span class="dv">1</span>], n<span class="op">//</span>lower_factors[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="annotated-cell-4-6"><a href="#annotated-cell-4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-4-7"><a href="#annotated-cell-4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_loop(X, y, k <span class="op">=</span> <span class="dv">1</span>, alpha <span class="op">=</span> <span class="dv">1</span>, max_iters <span class="op">=</span> <span class="dv">1000</span>):</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="2">2</button><span id="annotated-cell-4-8" class="code-annotation-target"><a href="#annotated-cell-4-8" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> Perceptron()</span>
<span id="annotated-cell-4-9"><a href="#annotated-cell-4-9" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> PerceptronOptimizer(p)</span>
<span id="annotated-cell-4-10"><a href="#annotated-cell-4-10" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(<span class="dv">1</span>)</span>
<span id="annotated-cell-4-11"><a href="#annotated-cell-4-11" aria-hidden="true" tabindex="-1"></a>    p.loss(X, y)</span>
<span id="annotated-cell-4-12"><a href="#annotated-cell-4-12" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="3">3</button><span id="annotated-cell-4-13" class="code-annotation-target"><a href="#annotated-cell-4-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-14"><a href="#annotated-cell-4-14" aria-hidden="true" tabindex="-1"></a>    loss_vec <span class="op">=</span> []</span>
<span id="annotated-cell-4-15"><a href="#annotated-cell-4-15" aria-hidden="true" tabindex="-1"></a>    old_w_vec <span class="op">=</span> []</span>
<span id="annotated-cell-4-16"><a href="#annotated-cell-4-16" aria-hidden="true" tabindex="-1"></a>    p_w <span class="op">=</span> []</span>
<span id="annotated-cell-4-17"><a href="#annotated-cell-4-17" aria-hidden="true" tabindex="-1"></a>    local_loss_vec <span class="op">=</span> []</span>
<span id="annotated-cell-4-18"><a href="#annotated-cell-4-18" aria-hidden="true" tabindex="-1"></a>    ix_vec <span class="op">=</span> []</span>
<span id="annotated-cell-4-19"><a href="#annotated-cell-4-19" aria-hidden="true" tabindex="-1"></a>    iters <span class="op">=</span> <span class="dv">0</span></span>
<span id="annotated-cell-4-20"><a href="#annotated-cell-4-20" aria-hidden="true" tabindex="-1"></a>    updates <span class="op">=</span> <span class="dv">0</span></span>
<span id="annotated-cell-4-21"><a href="#annotated-cell-4-21" aria-hidden="true" tabindex="-1"></a>    seed <span class="op">=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-22"><a href="#annotated-cell-4-22" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="4">4</button><span id="annotated-cell-4-23" class="code-annotation-target"><a href="#annotated-cell-4-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> loss <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> iters <span class="op">&lt;</span> max_iters:</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="5">5</button><span id="annotated-cell-4-24" class="code-annotation-target"><a href="#annotated-cell-4-24" aria-hidden="true" tabindex="-1"></a>        old_w <span class="op">=</span> torch.clone(p.w)</span>
<span id="annotated-cell-4-25"><a href="#annotated-cell-4-25" aria-hidden="true" tabindex="-1"></a>        </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="6">6</button><span id="annotated-cell-4-26" class="code-annotation-target"><a href="#annotated-cell-4-26" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(seed)</span>
<span id="annotated-cell-4-27"><a href="#annotated-cell-4-27" aria-hidden="true" tabindex="-1"></a>        </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="7">7</button><span id="annotated-cell-4-28" class="code-annotation-target"><a href="#annotated-cell-4-28" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.randperm(X.size(<span class="dv">0</span>))[:k]</span>
<span id="annotated-cell-4-29"><a href="#annotated-cell-4-29" aria-hidden="true" tabindex="-1"></a>        x_ix <span class="op">=</span> X[ix,:]</span>
<span id="annotated-cell-4-30"><a href="#annotated-cell-4-30" aria-hidden="true" tabindex="-1"></a>        y_ix <span class="op">=</span> y[ix]</span>
<span id="annotated-cell-4-31"><a href="#annotated-cell-4-31" aria-hidden="true" tabindex="-1"></a>        local_loss <span class="op">=</span> opt.step(x_ix, y_ix, alpha <span class="op">=</span> alpha)</span>
<span id="annotated-cell-4-32"><a href="#annotated-cell-4-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="annotated-cell-4-33"><a href="#annotated-cell-4-33" aria-hidden="true" tabindex="-1"></a>        seed <span class="op">+=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-34"><a href="#annotated-cell-4-34" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="8">8</button><span id="annotated-cell-4-35" class="code-annotation-target"><a href="#annotated-cell-4-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> local_loss <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="annotated-cell-4-36"><a href="#annotated-cell-4-36" aria-hidden="true" tabindex="-1"></a>            local_loss_vec.append(local_loss)</span>
<span id="annotated-cell-4-37"><a href="#annotated-cell-4-37" aria-hidden="true" tabindex="-1"></a>            ix_vec.append(ix)</span>
<span id="annotated-cell-4-38"><a href="#annotated-cell-4-38" aria-hidden="true" tabindex="-1"></a>            old_w_vec.append(old_w)</span>
<span id="annotated-cell-4-39"><a href="#annotated-cell-4-39" aria-hidden="true" tabindex="-1"></a>            p_w.append(torch.clone(p.w))</span>
<span id="annotated-cell-4-40"><a href="#annotated-cell-4-40" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> p.loss(X, y).item()</span>
<span id="annotated-cell-4-41"><a href="#annotated-cell-4-41" aria-hidden="true" tabindex="-1"></a>            loss_vec.append(loss)</span>
<span id="annotated-cell-4-42"><a href="#annotated-cell-4-42" aria-hidden="true" tabindex="-1"></a>            updates <span class="op">+=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-43"><a href="#annotated-cell-4-43" aria-hidden="true" tabindex="-1"></a>        iters <span class="op">+=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-44"><a href="#annotated-cell-4-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> iters <span class="op">&gt;=</span> max_iters:</span>
<span id="annotated-cell-4-45"><a href="#annotated-cell-4-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Max iterations reached.'</span>)</span>
<span id="annotated-cell-4-46"><a href="#annotated-cell-4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-4-47"><a href="#annotated-cell-4-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ix_vec, loss_vec, old_w_vec, p_w, local_loss_vec, updates</span>
<span id="annotated-cell-4-48"><a href="#annotated-cell-4-48" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="9">9</button><span id="annotated-cell-4-49" class="code-annotation-target"><a href="#annotated-cell-4-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_updates(X, y, ix_vec, loss_vec, local_loss, old_w, p_w):</span>
<span id="annotated-cell-4-50"><a href="#annotated-cell-4-50" aria-hidden="true" tabindex="-1"></a>    updates <span class="op">=</span> <span class="bu">len</span>(loss_vec)</span>
<span id="annotated-cell-4-51"><a href="#annotated-cell-4-51" aria-hidden="true" tabindex="-1"></a>    rows, cols <span class="op">=</span> plot_dims(updates)</span>
<span id="annotated-cell-4-52"><a href="#annotated-cell-4-52" aria-hidden="true" tabindex="-1"></a>    plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (cols<span class="op">*</span><span class="dv">2</span>, rows<span class="op">*</span><span class="dv">2</span>)</span>
<span id="annotated-cell-4-53"><a href="#annotated-cell-4-53" aria-hidden="true" tabindex="-1"></a>    fig, axarr <span class="op">=</span> plt.subplots(rows, cols, sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="annotated-cell-4-54"><a href="#annotated-cell-4-54" aria-hidden="true" tabindex="-1"></a>    markers <span class="op">=</span> [<span class="st">"o"</span>, <span class="st">","</span>]</span>
<span id="annotated-cell-4-55"><a href="#annotated-cell-4-55" aria-hidden="true" tabindex="-1"></a>    marker_map <span class="op">=</span> {<span class="op">-</span><span class="dv">1</span> : <span class="dv">0</span>, <span class="dv">1</span> : <span class="dv">1</span>}</span>
<span id="annotated-cell-4-56"><a href="#annotated-cell-4-56" aria-hidden="true" tabindex="-1"></a>    current_ax <span class="op">=</span> <span class="dv">0</span></span>
<span id="annotated-cell-4-57"><a href="#annotated-cell-4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-4-58"><a href="#annotated-cell-4-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(updates):</span>
<span id="annotated-cell-4-59"><a href="#annotated-cell-4-59" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axarr.ravel()[current_ax]</span>
<span id="annotated-cell-4-60"><a href="#annotated-cell-4-60" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> ix_vec[j]</span>
<span id="annotated-cell-4-61"><a href="#annotated-cell-4-61" aria-hidden="true" tabindex="-1"></a>        plot_perceptron_data(X, y, ax)</span>
<span id="annotated-cell-4-62"><a href="#annotated-cell-4-62" aria-hidden="true" tabindex="-1"></a>        draw_line(old_w[j], x_min <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, x_max <span class="op">=</span> <span class="dv">2</span>, ax <span class="op">=</span> ax, color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"dashed"</span>)</span>
<span id="annotated-cell-4-63"><a href="#annotated-cell-4-63" aria-hidden="true" tabindex="-1"></a>        draw_line(p_w[j], x_min <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, x_max <span class="op">=</span> <span class="dv">2</span>, ax <span class="op">=</span> ax, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="annotated-cell-4-64"><a href="#annotated-cell-4-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> i:</span>
<span id="annotated-cell-4-65"><a href="#annotated-cell-4-65" aria-hidden="true" tabindex="-1"></a>            ax.scatter(X[k,<span class="dv">0</span>],X[k,<span class="dv">1</span>], color <span class="op">=</span> <span class="st">"black"</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"black"</span>, marker <span class="op">=</span> markers[marker_map[y[k].item()]])</span>
<span id="annotated-cell-4-66"><a href="#annotated-cell-4-66" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"loss = </span><span class="sc">{</span>loss_vec[j]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="annotated-cell-4-67"><a href="#annotated-cell-4-67" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="annotated-cell-4-68"><a href="#annotated-cell-4-68" aria-hidden="true" tabindex="-1"></a>        current_ax <span class="op">+=</span> <span class="dv">1</span></span>
<span id="annotated-cell-4-69"><a href="#annotated-cell-4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-4-70"><a href="#annotated-cell-4-70" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="3,4,5" data-code-annotation="1">Generates dimensions for plotting the updates of the perceptron using the number of updates <span class="math inline">\(n\)</span> as the input.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="8,9,10,11" data-code-annotation="2">Initialize perceptron.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="13,14,15,16,17,18,19,20,21" data-code-annotation="3">Initialize variables for main loop.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="23" data-code-annotation="4">Loop until <code>loss &gt; 0</code> or max iterations reached.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="24" data-code-annotation="5">Save the old value of <span class="math inline">\(\mathbf{w}\)</span> for plotting later.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="26" data-code-annotation="6">Set seed for reproducible results.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="7">7</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="28,29,30,31" data-code-annotation="7">Make an optimization step using random point <code>x_i</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="8">8</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="35,36,37,38,39,40,41,42" data-code-annotation="8">If a change was made, save values needed for plotting.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="9">9</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="49,50,51,52,53,54,55,56,58,59,60,61,62,63,64,65,66,67,68,70" data-code-annotation="9">Function to plot updates.</span>
</dd>
</dl>
</div>
</div>
<p>Now, I can generate and plot my data set.</p>
<div id="cell-fig-test" class="cell" data-execution_count="72">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1234</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> perceptron_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.3</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X, y, ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-test" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-test-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-test-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Data to test my perceptron algorithm.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Next, I run the minimal training loop and plot each update.</p>
<div id="cell-fig-test-updates" class="cell" data-execution_count="73">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>i_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X, y, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plot_updates(X, y, i_vec, loss_vec, local_loss_vec, old_w, p_w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-test-updates" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-test-updates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-test-updates-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-test-updates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The dotted lines represent the old decision boundaries and the solid lines represent the updated decision boundaries. The highlighted point represents the random point chosen to update <span class="math inline">\(\mathbf{w}\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that it eventually achieves <code>loss = 0</code> showing that my implementation was successful.</p>
</section>
<section id="experimentation-perceptron" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="experimentation-perceptron"><span class="header-section-number">3</span> Experimentation: Perceptron</h2>
<p>For the regular perceptron, I will run the following experiments:</p>
<ol type="1">
<li>Visualize the updates of the perceptron for linearly separable data and the final separating line.</li>
<li>Visualize the updates of the perceptron for not linearly separable data and the final boundary line in the final iteration since it does not converge.</li>
<li>Visualize the loss of the perceptron for data with 5 feature and determine if it converges or not.</li>
</ol>
<section id="sec-p1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="sec-p1"><span class="header-section-number">3.1</span> Linearly Separable Data</h3>
<p>First, I generate a linearly separable data set with 50 points. Below is the plot of the data.</p>
<div id="cell-fig-e1" class="cell" data-execution_count="74">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X1, y1 <span class="op">=</span> perceptron_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.3</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X1, y1, ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-e1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-e1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-e1-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-e1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Plot of linearly separable data (n = 50).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, I can run the minimal training loop and plot the loss and decision boundary for updates only.</p>
<div id="cell-fig-e1-updates" class="cell" data-execution_count="75">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>i_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X1, y1, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plot_updates(X1, y1, i_vec, loss_vec, local_loss_vec, old_w, p_w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-e1-updates" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-e1-updates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-e1-updates-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-e1-updates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Updates for P.1.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As we can see from the plots, the perceptron algorithm can successfully achieve <code>loss = 0</code> and found a separating line for linearly separable data.</p>
</section>
<section id="sec-p2" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-p2"><span class="header-section-number">3.2</span> Not Linearly Separable</h3>
<p>First, I will generate not linearly separable data with 50 points of data.</p>
<div id="cell-fig-e2" class="cell" data-execution_count="76">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X2, y2 <span class="op">=</span> perceptron_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.99</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> X2.size()[<span class="dv">0</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X2, y2, ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-e2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-e2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-e2-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-e2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Not linearly separable data (n = 50).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, I can run the minimal training loop on the data. Because the data is not linearly separable, the perceptron will not converge, so I set the max iterations <code>max_iters = 1000</code>.</p>
<div id="cell-25" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>i_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X2, y2, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Max iterations reached.</code></pre>
</div>
</div>
<p>We can see that the message confirms that the perceptron algorithm did not converge withing 1000 iterations. Below is the final decision boundary in the final iteration.</p>
<div id="cell-fig-e2-result" class="cell" data-execution_count="78">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X2, y2, ax)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>draw_line(p_w[<span class="op">-</span><span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-e2-result" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-e2-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-e2-result-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-e2-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The decision boundary in the final iteration.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="data-with-five-features" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="data-with-five-features"><span class="header-section-number">3.3</span> Data with Five Features</h3>
<p>The perceptron algorithm should still work on data with more than two features. In this part, I will run the perceptron on data with five features.</p>
<p>First, I will generate the necessary data.</p>
<div id="cell-30" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">37</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>X3, y3 <span class="op">=</span> perceptron_data(n_points <span class="op">=</span> <span class="dv">100</span>, noise <span class="op">=</span> <span class="fl">0.3</span>, dims <span class="op">=</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, I run the minimal training loop on the data and plot the loss over time.</p>
<div id="cell-fig-e3-result" class="cell" data-execution_count="80">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="annotated-cell-13"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-13-1"><a href="#annotated-cell-13-1" aria-hidden="true" tabindex="-1"></a>i_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X3, y3, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-13" data-target-annotation="1">1</button><span id="annotated-cell-13-2" class="code-annotation-target"><a href="#annotated-cell-13-2" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_vec, color <span class="op">=</span> <span class="st">"slategrey"</span>)</span>
<span id="annotated-cell-13-3"><a href="#annotated-cell-13-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(torch.arange(updates), loss_vec, color <span class="op">=</span> <span class="st">"slategrey"</span>)</span>
<span id="annotated-cell-13-4"><a href="#annotated-cell-13-4" aria-hidden="true" tabindex="-1"></a>labs <span class="op">=</span> plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Perceptron Iteration (Updates Only)"</span>, ylabel <span class="op">=</span> <span class="st">"loss"</span>)</span>
<span id="annotated-cell-13-5"><a href="#annotated-cell-13-5" aria-hidden="true" tabindex="-1"></a>xticks <span class="op">=</span> torch.arange(<span class="dv">0</span>, updates)</span>
<span id="annotated-cell-13-6"><a href="#annotated-cell-13-6" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>, <span class="fl">0.15</span>, <span class="fl">0.20</span>])</span>
<span id="annotated-cell-13-7"><a href="#annotated-cell-13-7" aria-hidden="true" tabindex="-1"></a>plt.xticks(xticks)<span class="op">;</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-13" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-13" data-code-lines="2,3,4,5,6,7" data-code-annotation="1">Plots loss over time for updates only.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-e3-result" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-e3-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-e3-result-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-e3-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Loss over time for updates only.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Looking above, we can see that the perceptron algorithm achieved <code>loss = 0</code> meaning it successfully converged and found a separating line.</p>
</section>
</section>
<section id="experimentation-minibatch-perceptron" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="experimentation-minibatch-perceptron"><span class="header-section-number">4</span> Experimentation: Minibatch Perceptron</h2>
<p>For the minibatch perceptron, I will be running the following experiments: 1. Visualize the updates of the perceptron with <code>k=1</code> and show that it performs similarly to the regular perceptron. 2. Visualize the updates of the perceptron with <code>k=10</code> and show it can still find a separating line in 2d. 3. Visualize the updates of the perceptron with <code>k=n</code> and show that it can still converge even if the data is not linearly separable, provided that the learning rate <span class="math inline">\(\alpha\)</span> is not separable.</p>
<p>For the experiments, I will be using the data from <a href="#sec-p1" class="quarto-xref">Section&nbsp;3.1</a> and <a href="#sec-p2" class="quarto-xref">Section&nbsp;3.2</a>.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> X1.size()[<span class="dv">0</span>]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>), sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>fig1, ax1 <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>), sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>ax1.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X1, y1, ax)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X2, y2, ax1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-mb1" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mb1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mb1" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mb1-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mb1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb1-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-mb1">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mb1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Linearly separable data from <a href="#sec-p1" class="quarto-xref">Section&nbsp;3.1</a>.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mb1" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mb1-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mb1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb1-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-mb1">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mb1-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Not linearly separable data from <a href="#sec-p2" class="quarto-xref">Section&nbsp;3.2</a>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mb1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Plots of data to be used.
</figcaption>
</figure>
</div>
<section id="k-1" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="k-1"><span class="header-section-number">4.1</span> <code>k = 1</code></h3>
<p>For this first experimentation, we want to show that the minibatch perceptron performs similarly to the regular perceptron. To do that, I will use the data from <a href="#fig-mb1-1" class="quarto-xref">Figure&nbsp;8 (a)</a> to compare results for linearly separable data and the data from <a href="#fig-mb1-2" class="quarto-xref">Figure&nbsp;8 (b)</a> for not linearly separable data.</p>
<div id="cell-fig-mb1-result1" class="cell" data-execution_count="82">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>ix_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X1, y1, k <span class="op">=</span> <span class="dv">1</span>, alpha <span class="op">=</span> <span class="dv">1</span>, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plot_updates(X1, y1, ix_vec, loss_vec, local_loss_vec, old_w, p_w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-mb1-result1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mb1-result1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb1-result1-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mb1-result1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Updates for linearly separable data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Next, letâ€™s run the loop for the not linearly separable data.</p>
<div id="cell-fig-mb1-result2" class="cell" data-execution_count="83">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ix_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X2, y2, k <span class="op">=</span> <span class="dv">1</span>, alpha <span class="op">=</span> <span class="dv">1</span>, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X2, y2, ax)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>draw_line(p_w[<span class="op">-</span><span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Max iterations reached.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-mb1-result2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mb1-result2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb1-result2-output-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mb1-result2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Decision boundary during the final iteration.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Comparing <a href="#fig-mb1-result1" class="quarto-xref">Figure&nbsp;9</a> to <a href="#fig-e1-updates" class="quarto-xref">Figure&nbsp;4</a> and <a href="#fig-mb1-result2" class="quarto-xref">Figure&nbsp;10</a> to <a href="#fig-e2-result" class="quarto-xref">Figure&nbsp;6</a>, we can see that the final decision boundary is the same which confirms that the minibatch perceptron performs similarly to the regular perceptron for <code>k = 1</code>.</p>
</section>
<section id="k-10" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="k-10"><span class="header-section-number">4.2</span> <code>k = 10</code></h3>
<p>To show that the minibatch perceptron will converge for <code>k = 10</code> for linearly separable data, I will once again use the data from <a href="#fig-mb1-1" class="quarto-xref">Figure&nbsp;8 (a)</a>.</p>
<div id="cell-fig-mb2-result" class="cell" data-execution_count="84">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>ix_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X1, y1, k <span class="op">=</span> <span class="dv">10</span>, alpha <span class="op">=</span> <span class="dv">1</span>, max_iters <span class="op">=</span> <span class="dv">1000</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plot_updates(X1, y1, ix_vec, loss_vec, local_loss_vec, old_w, p_w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-mb2-result" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mb2-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb2-result-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mb2-result-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Updates for <code>k = 10</code>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that that it eventually achieves <code>loss = 0</code> meaning that the minibatch perceptron will converge for <code>k = 10</code>.</p>
</section>
<section id="k-n" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="k-n"><span class="header-section-number">4.3</span> <code>k = n</code></h3>
<p>To show that the minibatch perceptron will converge for <code>k = n</code> even if the data is not linearly separable, I use the data from <a href="#fig-mb1-2" class="quarto-xref">Figure&nbsp;8 (b)</a>. Even though it will not be able to achieve <code>loss = 0</code>, the <code>loss</code> should still even out at some <code>loss &lt; 0.5</code>.</p>
<div id="cell-48" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> X2.size()[<span class="dv">0</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>ix_vec, loss_vec, old_w, p_w, local_loss_vec, updates <span class="op">=</span> training_loop(X2, y2, k <span class="op">=</span> n, alpha <span class="op">=</span> <span class="fl">0.001</span>, max_iters <span class="op">=</span> <span class="dv">5000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Max iterations reached.</code></pre>
</div>
</div>
<p>We can see that it did not achieve <code>loss = 0</code> within 1000 iterations. Letâ€™s take a look at the decision boundary during the final iteration, and the loss over time.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plot_perceptron_data(X2, y2, ax)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>draw_line(p_w[<span class="op">-</span><span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>fig1, ax1 <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_vec, color <span class="op">=</span> <span class="st">"slategrey"</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>ax1.scatter(torch.arange(updates), loss_vec, color <span class="op">=</span> <span class="st">"slategrey"</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>labs <span class="op">=</span> plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Perceptron Iteration (Updates Only)"</span>, ylabel <span class="op">=</span> <span class="st">"loss"</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>plt.semilogx()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-mb3" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mb3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mb3" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mb3-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mb3-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb3-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-mb3">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mb3-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Decision boundary in the final iteration.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mb3" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mb3-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mb3-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="perceptron_post_files/figure-html/fig-mb3-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-mb3">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mb3-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Loss over time for updates only.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mb3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Results of MB.3.
</figcaption>
</figure>
</div>
<p>In <a href="#fig-mb3-2" class="quarto-xref">Figure&nbsp;12 (b)</a>, we can see that the loss eventually drops down to about 0.24 then stays between 0.24 an 0.3 which confirms that the minibatch perceptron will converge to some value <code>loss &lt; 0.5</code> for <code>k = n</code>.</p>
</section>
</section>
<section id="runtime-complexity" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="runtime-complexity"><span class="header-section-number">5</span> Runtime Complexity</h2>
<p>From these experiments, we can conclude that the minibatch perceptron is more powerful than the regular perceptron. However, does it require more processing power and time to run the minibatch perceptron? More specifically, what are the runtime complexities for each iteration of both perceptron algorithms and are they dependent on the number of data points <span class="math inline">\(n\)</span> or the number of features <span class="math inline">\(p\)</span>?</p>
<section id="perceptron" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="perceptron"><span class="header-section-number">5.1</span> Perceptron</h3>
<p>To do this, letâ€™s review the perceptron algorithms and the code implementation of them. Going back to the update equation for the perceptron, we have <span class="math display">\[\mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \mathbb{1}[s_i y_i &lt; 0]y_i\mathbf{x}_i.\]</span> where <span class="math display">\[s_i = \langle\mathbf{w}^{(t)}, \mathbf{x}_i\rangle.\]</span> To calculate the score <span class="math inline">\(s_i\)</span>, we need to calculate the dot product for <span class="math inline">\(\mathbf{w}^{(t)}\)</span> and <span class="math inline">\(\mathbf{x}_i\)</span>. Given <span class="math inline">\(p\)</span> features, <span class="math inline">\(s_i = \langle\mathbf{w}^{(t)}, \mathbf{x}_i\rangle\)</span> has a runtime complexity of <span class="math inline">\(O(p)\)</span>. Then going through the rest of the equation we get:</p>
<ol type="1">
<li><span class="math inline">\(s_i y_i \implies O(p+1)\)</span></li>
<li><span class="math inline">\(\mathbb{1}[s_i y_i &lt; 0] \implies O(2p + 1)\)</span></li>
<li><span class="math inline">\(\mathbb{1}[s_i y_i &lt; 0]y_i \implies O(2p + 2)\)</span></li>
<li><span class="math inline">\((\mathbb{1}[s_i y_i &lt; 0]y_i)\mathbf{x}_i \implies O(3p+2) = O(p)\)</span>.</li>
</ol>
<p>So, the runtime complexity for the regular perceptron algorithm is <span class="math inline">\(O(p)\)</span> meaning it is only dependent on the number of features <span class="math inline">\(p\)</span>.</p>
</section>
<section id="minibatch-perceptron" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="minibatch-perceptron"><span class="header-section-number">5.2</span> Minibatch Perceptron</h3>
<p>For the minibatch perceptron, the update equation is defined as: <span class="math display">\[\mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \frac{\alpha}{k}\sum_{\ell=1}^k\mathbb{1}[s_{i_\ell} y_{i_\ell} &lt; 0]y_{i_\ell}\mathbf{x}_{i_\ell}.\]</span> Similar to <span class="math inline">\(s_i\)</span>, <span class="math inline">\(s_{i_\ell}\)</span> has a runtime complexity of <span class="math inline">\(O(p)\)</span> and likewise <span class="math inline">\((\mathbb{1}[s_{i_\ell} y_{i_\ell} &lt; 0]y_{i_\ell})\mathbf{x}_{i_\ell}\)</span> has runtime complexity <span class="math inline">\(O(p)\)</span>. We then do this <span class="math inline">\(k\)</span> times where <span class="math inline">\(k\)</span> is the number of randomly selected points, so the runtime complexity becomes <span class="math inline">\(O(kp)\)</span>. Additionally, we must compute the sum of the resulting vectors for <span class="math inline">\(k\)</span> randomly selected points. Because each vector contains <span class="math inline">\(p\)</span> values and there are <span class="math inline">\(k\)</span> total vectors, the operation will have runtime complexity <span class="math inline">\(O(kp)\)</span>. Then, the total runtime complexity is <span class="math inline">\(O(2kp) = O(kp)\)</span>.</p>
<p>To summarize, the regular perceptron has a runtime complexity of <span class="math inline">\(O(p)\)</span> and the minibatch perceptron has a runtime complexity of <span class="math inline">\(O(kp)\)</span> with the worst case scenario being <span class="math inline">\(k=n \implies O(np)\)</span>. From this, we can see that though the minibatch perceptron is more powerful than the regular perceptron but as a tradeoff, requires more processing power and time to run.</p>
</section>
</section>
<section id="discussion" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="discussion"><span class="header-section-number">6</span> Discussion</h2>
<p>From the experiments with the perceptron, we found that it could converge to a separating line for linearly separable data with two or more features, but will not be able to converge for not linearly separable data. Then from the experiments with the minibatch perceptron, we found that it performed exactly the same as the regular perceptron when <code>k = 1</code> and <span class="math inline">\(\alpha = 1\)</span> when using the same dataset and starting <span class="math inline">\(\mathbf{w}\)</span>. That is, it found the same separating line with the same amount of updates for linearly separable data and the same decision boundary in the final iteration for not linearly separable data. This is due to the fact that the update to <span class="math inline">\(\mathbf{w}\)</span> for the minibatch perceptron becomes the same as the update to <span class="math inline">\(\mathbf{w}\)</span> for the regular perceptron when <code>k = 1</code> and <span class="math inline">\(\alpha = 1\)</span>. Additionally, we found that the minibatch perceptron converges for <code>k = 10</code> for linearly separable data and it converges to a <code>loss &lt; 0.5</code> for not linearly separable when <code>k = n</code>.</p>
<p>Comparing the two algorithms, we found that the minibatch perceptron is more powerful than the regular perceptron, but as a result, requires more processing time to run.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>