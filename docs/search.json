[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Andre Xiao. I am a student-athlete majoring in Computer Science and Mathematics. This blog is for my machine learning class, CSCI 0451, and potentially other future projects. I also have 2 cats :)."
  },
  {
    "objectID": "posts/loans_post/loans.html",
    "href": "posts/loans_post/loans.html",
    "title": "Bank Loans",
    "section": "",
    "text": "Image source: techfunnel.com"
  },
  {
    "objectID": "posts/loans_post/loans.html#overview",
    "href": "posts/loans_post/loans.html#overview",
    "title": "Bank Loans",
    "section": "Overview",
    "text": "Overview\nThis study attempts to optimize the maximum total expected profit from loans for an unknown bank. I will be trying to create an automated decision-making sytem that gives each prospective borrower a score and decides whether to give them a loan based on a threshold, \\(t\\), where each score, \\(s_i\\), of borrower, \\(i\\), is defined as \\[s_i = \\langle \\mathbf{X}_i, \\mathbf{w} \\rangle \\tag{1}\\] where \\(\\mathbf{X}_i\\) is the vector of features for borrower, \\(i\\), and \\(\\mathbf{w}\\) is the vector of weights for each feature. Using this, the goal is to find \\(\\mathbf{w}\\) and \\(t\\) which maximize the total expected profit per loan for the bank.\nTo accomplish this, I use logistic regression to determine \\(\\mathbf{w}\\) and calculate the expected profit per loan for various \\(t\\)-values. In this study, we assume that each loan is a 10-year loan and 75% of the interest is used to pay for operating costs such as employee salaries. We also assume that defaults occur after three years and the bank loses 70% of the principal. That is, \\[\\begin{align*}\n    \\textbf{profit} &= \\text{loan amount}\\cdot(1+0.25\\cdot\\text{interest rate})^{10} - \\text{loan amount} \\\\\n    \\textbf{cost} &= \\text{loan amount}\\cdot(1+0.25\\cdot\\text{interest rate})^3 - 1.7\\cdot\\text{loan amount}\n\\end{align*} \\tag{2}\\]\nIn the end, the expected profit per loan for the bank was $1714.51.\nAfterwards, I discuss the impact that the automated system has on different segments of the population of prospective borrowers. I explore how the system impacts different age groups, purposes for the loan request, and income levels. I find that it is easier for younger age groups to receive loans, harder to take out student loans and loans for ventures, and it is much easier for higher income-levels to receive loans under my system."
  },
  {
    "objectID": "posts/loans_post/loans.html#preparing-the-training-data",
    "href": "posts/loans_post/loans.html#preparing-the-training-data",
    "title": "Bank Loans",
    "section": "Preparing the Training Data",
    "text": "Preparing the Training Data\n\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nThe dataset contains the following columns:\n\nperson_age: the age of the prospective borrower.\nperson_income: the income of the prospective borrower at the time of application.\nperson_home_ownership: the home ownership status of the prospective borrower at the time of application. Possible values are MORTGAGE, OWN, RENT, and OTHER.\nperson_emp_length: the length of the most recent employment for the prospective borrower, in years.\nloan_intent: the purpose of the loan request.\nloan_grade: a composite measure of the likelihood of the borrower to repay the loan. My system will give its own score to each borrower.\nloan_amnt: the amount of the loan.\nloan_int_rate, the annual interest rate on the loan in percent. This is the target variable.\nloan_status: whether (1) or not (0) the borrower defaulted on the loan.\nloan_percent_income: the amount of the loan as a proportion of the prospective borrower’s personal income. This is caluculated as loan_amnt/person_income.\ncb_person_default_on_file: whether the prospective borrower has previously defaulted on a loan in the records of a credit bureau.\ncb_person_cred_hist_length: the length of credit history of the prospective borrower, in years.\n\nIn addition to the columns given by the data set, I will add another column:\n\nint_percent_income: the amount of yearly interest as a proportion of the borrower’s income. Calculated as int_percent_income = loan_percent_income * loan_int_rate.\n\nI will also be changing loan_int_rate from a percentage to a fraction before calculating int_percent_income.\n\n1def prepare_data(df):\n2    df_ = df.copy()\n3    df_ = df_.dropna()\n4    df_['loan_int_rate'] = df_['loan_int_rate'] / 100\n5    df_['int_percent_income'] = df_['loan_percent_income'] * df_['loan_int_rate']\n6    df_ = df_[(df_['person_age'] &lt;= 100) & (df_['person_emp_length'] &lt;= 100)]\n    return df_\n\ndf_train = prepare_data(df_train)\ndf_train.head()\n\n\n1\n\nFunction to prepare data.\n\n2\n\nCopy dataframe\n\n3\n\nDrop all NaNs\n\n4\n\nConverts loan_int_rate to fraction from percent.\n\n5\n\nCalculates int_percent_income and add it as a new column.\n\n6\n\nDrops all people over the age of 100 and have employment lengths over 100 years.\n\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nint_percent_income\nage_group\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n1.347000e-19\n0\n0.12\nY\n6\n1.616400e-20\n20-29\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.510000e-20\n0\n0.27\nN\n4\n2.027700e-20\n20-29\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n1.287000e-19\n1\n0.05\nN\n4\n6.435000e-21\n20-29\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.630000e-20\n0\n0.28\nN\n10\n2.696400e-20\n20-29\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n1.491000e-19\n1\n0.25\nN\n2\n3.727500e-20\n20-29"
  },
  {
    "objectID": "posts/loans_post/loans.html#exploring-the-data",
    "href": "posts/loans_post/loans.html#exploring-the-data",
    "title": "Bank Loans",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet’s first take a look at how the loan interest rate and the yearly interest amount as a proportion of income (loan percent of income) relates to whether or not a borrower will default. The reason this may be a good place to start is because it is intuitive to think that a higher interest rate could result in more defaults. Additionally, it is also intuitive to think that the larger the loan amount is compared to a person’s income, the chance of defaulting increases. So, we can put these ideas together and conclude that interest rates and the loan percent of income of income could be closely related to the chance of defaulting.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npltt = 'Paired'\n\nint_income_plt = sns.scatterplot(df_train,\n                                 x = 'loan_percent_income',\n                                 y = 'loan_int_rate',\n                                 hue = 'loan_status',\n                                 palette = pltt)\n\nint_income_plt.set(xlabel = 'Loan Percent of Income',\n                   ylabel = 'Loan Interest Rate',\n                   title = 'Interest Cost vs. Income')\n\nint_income_plt.legend(title = 'Default');\n\n\n\n\n\n\n\n\nFigure 1: Defaults by interest percent of income.\n\n\n\n\n\nAs we can see, most defaults occur along the outer ring (dark blue dots). This confirms our intuition. As interest rate increases, more defaults occur and the same can be seen with the loan percent of income. Additionally, this figure (Figure 1) provides a good visualization of the interest percent of income (int_percent_income). With that said, loan_int_rate, loan_percent_income, and int_percent_income seem like good features to use.\nNext, let’s try to find some features that agree with this pattern. First, let’s take a look at loan_intent.\n\n\nCode\n1df_train.groupby([\"loan_intent\"]).agg({'loan_status':'mean',\n                                       'loan_int_rate': 'mean',\n                                       'loan_percent_income':'mean',\n                                       'int_percent_income':'mean'})\n\n\n\n1\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by loan_intent.\n\n\n\n\n\n\nTable 1: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by loan intent.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.282983\n0.110173\n0.170322\n0.019230\n\n\nEDUCATION\n0.171012\n0.109952\n0.168219\n0.018827\n\n\nHOMEIMPROVEMENT\n0.258327\n0.111819\n0.164516\n0.018924\n\n\nMEDICAL\n0.264455\n0.110711\n0.172422\n0.019511\n\n\nPERSONAL\n0.191385\n0.110282\n0.167568\n0.018898\n\n\nVENTURE\n0.146221\n0.109690\n0.169147\n0.018907\n\n\n\n\n\n\n\n\n\n\nFrom the table, we can see that the loan interest rate, loan percent income, and interest percent income are fairly similar between all categories of loan intent. However, there is a remarkable difference in default rates. DEBTCONSOLODATION, HOMEIMPROVEMENT, and MEDICAL all have much higher default rates than the other three categories. This doesn’t align well with our findings from Figure 1 since the default rates don’t align with loan_int_rate, loan_percent_income, and int_percent_income. As such, loan_intent may not be a good predictor.\nNext, we will look at person_home_ownership.\n\n\nCode\n1df_train.groupby([\"person_home_ownership\"]).agg({'loan_status':'mean',\n                                                 'loan_int_rate': 'mean',\n                                                 'loan_percent_income':'mean',\n                                                 'int_percent_income':'mean'})\n\n\n\n1\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by person_home_ownership.\n\n\n\n\n\n\nTable 2: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by home ownership status.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nperson_home_ownership\n\n\n\n\n\n\n\n\nMORTGAGE\n0.124496\n0.105270\n0.151172\n0.016350\n\n\nOTHER\n0.272727\n0.120592\n0.189870\n0.024013\n\n\nOWN\n0.071429\n0.109466\n0.184076\n0.020363\n\n\nRENT\n0.309761\n0.114524\n0.180943\n0.021013\n\n\n\n\n\n\n\n\n\n\nUnlike loan_intent (Table 1), person_home_ownership matches our findings from Figure 1. The default rates mostly lineup with loan_int_rate, loan_percent_income, and int_percent_income except for people who OWN a home, who have a relatively high loan_percent_income and int_percent_income, but have the lowest default rates. It is important to note however, that people who OWN a home have the second lowest loan_int_rate.\nAdditionally, as we can see below (Figure 2), RENT and OTHER have a relatively high amount of borrowers who are using the loans for DEBTCONSOLIDATION and MEDICAL. This matches up with the findings from Table 1. It seems that person_home_ownership is also a decent proxy for loan_intent while also matching the findings of Figure 1.\nAs such, we can expect person_home_ownership to possibly be a good predictor.\n\n\nCode\n1home_intent = ((df_train.groupby(['person_home_ownership', 'loan_intent']).size()\n                / df_train.groupby(['person_home_ownership']).size())\n2                .rename('proportion')\n3                .reset_index())\n\n4home_intent_plt = sns.barplot(home_intent,\n            x = 'person_home_ownership',\n            y = 'proportion',\n            hue = \"loan_intent\",\n            palette = pltt)\n\nhome_intent_plt.set(xlabel = \"Home Ownership Status\",\n                   ylabel = \"Proportion\", \n                   title = \"Loan Intent by Home Ownership Status\")\n\nhome_intent_plt.legend(title = \"Loan Intent\",\n                       fontsize = '8');\n\n\n\n1\n\nProportion of each loan_intent category for each person_home_ownership category.\n\n2\n\nRenames (1) to proportion.\n\n3\n\nResets index.\n\n4\n\nPlots loan intent by home ownership status.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Loan intent by home ownership status.\n\n\n\n\n\nLastly, let’s look at how age relates to loan_int_rate, loan_percent_income, int_percent_income, and default rates. To do this, I created age ranges of 10 years (20’s, 30’s, 40’s, etc.).\n\n\nCode\nimport numpy as np\n\n1age_labels = [\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"]\n2age_bins = np.concatenate((np.arange(19, 80, 10), [150]))\n\n3def get_age_groups(df):\n    df[\"age_group\"] = pd.cut(x = df['person_age'], bins = age_bins, labels = age_labels)\n    return df\n\n4df_train = get_age_groups(df_train)\n\n5df_train.groupby([\"age_group\"]).agg({'loan_status':'mean',\n                                     'loan_int_rate': 'mean',\n                                     'loan_percent_income':'mean',\n                                     'int_percent_income':'mean'})\n\n\n\n1\n\nList of age group labels.\n\n2\n\nSet’s bins to [19, 29, 39, …, 79, 150].\n\n3\n\nFunction that creates a new column called age_group and groups ages into 10 year intervals.\n\n4\n\nAdds age_group to df_train.\n\n5\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by age_group.\n\n\n\n\n\n\nTable 3: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by age group.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nage_group\n\n\n\n\n\n\n\n\n20-29\n0.220637\n0.110169\n0.171090\n0.019270\n\n\n30-39\n0.200233\n0.110843\n0.164211\n0.018574\n\n\n40-49\n0.193648\n0.110663\n0.160727\n0.018248\n\n\n50-59\n0.221622\n0.111235\n0.149243\n0.016920\n\n\n60-69\n0.318182\n0.109743\n0.202955\n0.022861\n\n\n70-79\n0.142857\n0.107500\n0.115714\n0.011406\n\n\n80+\n0.000000\n0.096100\n0.110000\n0.010571\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n1age_intent = ((df_train.groupby(['age_group', 'loan_intent']).size()\n               / df_train.groupby(['age_group']).size())\n2               .rename('proportion')\n3               .reset_index())\n\n4fig, ax = plt.subplots(1, 2, figsize = (15, 6))\n\n5age_intent_plt = sns.barplot(age_intent,\n                             x = 'age_group',\n                             y = 'proportion',\n                             hue = 'loan_intent',\n                             palette = pltt,\n                             ax = ax[0])\n\nage_intent_plt.set(xlabel = \"Age Group\",\n                ylabel = \"Proportion\", \n                title = \"Loan Intent by Age Group\")\n\nage_intent_plt.legend(title = \"Loan Intent\")\n\n6age_home = ((df_train.groupby(['age_group', 'person_home_ownership']).size()\n             / df_train.groupby(['age_group']).size())\n7             .rename('proportion')\n8             .reset_index())\n\n9age_home_plt = sns.barplot(age_home,\n                             x = 'age_group',\n                             y = 'proportion',\n                             hue = 'person_home_ownership',\n                             palette = pltt,\n                             ax = ax[1])\n\nage_home_plt.set(xlabel = \"Age Group\",\n                ylabel = \"Proportion\", \n                title = \"Home Ownership Status by Age Group\")\n\nage_home_plt.legend(title = \"Home Ownership Status\");\n\n\n\n1\n\nProportion of each loan_intent category for each age_group category.\n\n2\n\nRenames (1) to proportion.\n\n3\n\nResets index.\n\n4\n\nCreate 2 subplots.\n\n5\n\nPlots loan intent by age group as first subplot.\n\n6\n\nProportion of each person_home_ownership category for each age_group category.\n\n7\n\nRenames (6) to proportion.\n\n8\n\nResets index.\n\n9\n\nPlots home ownership status by age group as second subplot.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Loan intent and homeownership status by age group.\n\n\n\n\n\nFrom Table 3, we can see that age_group follows the trend found in Figure 1 with the exception for 50-59 year olds who have a higher default rate despite having a relatively low loan_percent_income and int_percent_income. However, they do have the highest loan_int_rate out of all the age groups which may be a reason as to why their default rates are so high.\nThen, comparing age_group with loan_intent (Figure 3), we can see that as borrowers get older, they spend more on MEDICAL. Yet despite spending the most on MEDICAL, 70-79 year olds have the lower default rates by far which contradicts our findings from Table 1. Additionally, there is no clear relationship between age_group and person_home_ownership.\nTherefore, we can most likely conclude that age_group is not a good predictor."
  },
  {
    "objectID": "posts/loans_post/loans.html#finding-a-threshold",
    "href": "posts/loans_post/loans.html#finding-a-threshold",
    "title": "Bank Loans",
    "section": "Finding a Threshold",
    "text": "Finding a Threshold\nUsing our findings, I will attempt to find an optimal threshold which will maximize expected profit from loans for the bank. I will do this by finding the best features through a semi-exhaustive search, then using those features, for logistic regression. I will then use the coefficients from the trained model as my weight vector, w, to compute the score, s, for each borrower (Equation 1).\nLastly, I will test multiple thresholds to find the optimal threshold that maximizes expected profit per loan for the bank. The base profit to beat,assuming that the bank gives everyone loans, is $819.12.\n\n1gain = ((df_train[df_train['loan_status'] == 0]['loan_amnt']\n         *(1 + 0.25*df_train['loan_int_rate'])**10\n         - df_train[df_train['loan_status'] == 0]['loan_amnt'])\n         .sum())\n2cost = ((df_train[df_train['loan_status'] == 1]['loan_amnt']\n         *(1 + 0.25*df_train['loan_int_rate'])**3\n         - 1.7*df_train[df_train['loan_status'] == 1]['loan_amnt'])\n         .sum())\n3base_profit = (gain+cost)/df_train.shape[0]\n4base_profit\n\n\n1\n\nCalculates gain from giving out loans to everyone.\n\n2\n\nCalcultaes cost from giving out loans to everyone.\n\n3\n\nCalculates total base profit.\n\n4\n\nOutputs total base profit\n\n\n\n\n819.1210295903265\n\n\nBefore I start however, I need to prepare the data for training. I will drop our target variable loan_status and the bank’s “score” for each borrower, loan_grade, since I will generate my own scores for each borrower. We also drop age_group since we can simply use person_age instead, and one-hot encode all the qualitative columns.\n\n\nCode\n1y_train = df_train['loan_status']\n2X_train = df_train.drop(['loan_status', 'loan_grade', 'age_group'], axis = 1)\n\n3qual_cols = list(X_train.select_dtypes(exclude=['number']).columns)\nX_train = pd.get_dummies(X_train,\n4                          columns = qual_cols)\n\nX_train.head()\n\n\n\n1\n\nSet loan_status as my target variable \\(y\\).\n\n2\n\nDrops loan_status, loan_grade, and age_group and set the resulting dataframe to be my features \\(x\\).\n\n3\n\nGet all qualitative columns.\n\n4\n\nOne-hot encodes the qualitative columns.\n\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nint_percent_income\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\n0.016164\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\n0.020277\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n0.1287\n0.05\n4\n0.006435\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n0.0963\n0.28\n10\n0.026964\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n0.1491\n0.25\n2\n0.037275\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nNow that the data is ready to be trained, I will first use my findings from @fig-int-percent-income as a starting point. My initial features will be loan_int_rate, loan_percent_income, and int_percent_income.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n1want_cols = ['loan_int_rate', 'loan_percent_income', 'int_percent_income']\n\n2LR = LogisticRegression()\n3cross_val_score(LR, X_train[want_cols], y_train, cv = 5).mean()\n\n\n1\n\nSets a list cols with my intial column names.\n\n2\n\nInitialize logistic regression instance.\n\n3\n\nOutputs the mean of cross-validation using five folds.\n\n\n\n\n0.8256407088706057\n\n\nA score of \\(\\approx .83\\) is a good start. Now I will try to find other features that may improve the score. I do this by making a power set from the rest of the columns and testing each with loan_int_rate, loan_percent_income, and int_percent_income using five-fold cross validation.\n\nfrom itertools import chain, combinations\n\n1def power_set(iterable):\n    '''power_set([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)'''\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\n2all_qual_cols = (list({col.rsplit('_', 1)[0]\n                       for col in X_train.select_dtypes(exclude=['number']).columns}))\n3all_quant_cols = list(X_train.select_dtypes(exclude=[\"bool_\",\"object_\"]).columns)\n4excess_cols = all_qual_cols + all_quant_cols[0:4] + all_quant_cols[6:8]\n\n\n5def power_set_cols(df, cols):\n    power_cols = list(list(s) for s in power_set(cols))[1:]\n\n    for i in range(0, len(power_cols)):\n        all_cols = []\n        for col in power_cols[i]:\n            all_cols = all_cols + [x_col for x_col in df.columns if col in x_col]\n            power_cols[i] = all_cols\n    return power_cols\n\n6power_cols = [list(p) for p in power_set_cols(X_train, excess_cols)]\n7all_cols = [want_cols] + [want_cols + p for p in power_cols]\n\n\n1\n\nFunction for creating a power set of a list.\n\n2\n\nGets all qualitative columns without one-hot encoded suffixes (ex. person_home_ownership_MORTGAGE \\(\\to\\) person_home_ownership)\n\n3\n\nGets all quantitative columns.\n\n4\n\nCombines (2) and (3) together.\n\n5\n\nFunction that creates a power set of (4) then adds all one-hot encoding suffixes back for each set in power set (ex. person_home_owndership_MORTGAGE, person_home_ownership_RENT, etc. for person_home_ownership).\n\n6\n\nConverts all sets in power set to lists.\n\n7\n\nPrepends want_cols to front of every list in power set.\n\n\n\n\nHere’s what one such combination looks like.\n\nall_cols[123]\n\n['loan_int_rate',\n 'loan_percent_income',\n 'int_percent_income',\n 'person_income',\n 'loan_amnt',\n 'cb_person_cred_hist_length']\n\n\nUsing the list of column combinations, I compare the mean of the cross-validation scores for each one and store the columns that have the best score.\n\n1def cross_val(model, cv):\n  best_score = ([], 0)\n  for col in all_cols:\n    cv_scores = cross_val_score(model, X_train[col], y_train, cv = cv)\n    col_scores = (col, cv_scores.mean())\n    if col_scores[1] &gt; best_score[1]:\n      best_score = col_scores\n  return best_score\n\nbest_score = cross_val(LR, 5)\nbest_score\n\n\n1\n\nUses cross-validation to find the best scores and returns ([best columns], best score).\n\n\n\n\n(['loan_int_rate',\n  'loan_percent_income',\n  'int_percent_income',\n  'person_home_ownership_MORTGAGE',\n  'person_home_ownership_OTHER',\n  'person_home_ownership_OWN',\n  'person_home_ownership_RENT'],\n 0.8474301677042732)\n\n\nWe can see that the best columns confirm the findings from exploring the data. With these columns, I can now train my model using logistic regression.\n\ncols = best_score[0]\n\nLR.fit(X_train[cols], y_train)\nscore = LR.score(X_train[cols], y_train)\n\nscore\n\n0.8469499148508799\n\n\nBelow are the coefficients which I will use as the weights for my score function.\n\nw = LR.coef_[0]\nw\n\narray([20.43778674,  8.01023218,  2.22540065, -0.0730397 ,  0.36076484,\n       -1.18065954,  0.8942643 ])\n\n\nMy score(Equation 1) function:\n\ndef linear_score(X, w):\n    return X@w\n\nWith my score function, I can now give each borrower a score and test various thresholds by profit per loan (Figure 4).\n\nnum_thresholds = 101\n1profit = np.zeros(num_thresholds)\n\n2s = linear_score(X_train[cols], w)\n3T = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\nfor i in range(num_thresholds):\n4    y_pred = s &gt;= T[i]\n5    TN = X_train[((y_pred == 0) & (y_train == 0))]\n6    FN = X_train[((y_pred == 0) & (y_train == 1))]\n7    gain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\n8    cost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\n9    total_loans = TN.shape[0] + FN.shape[0]\n10    if total_loans == 0:\n        profit[i] = 0\n    else:\n        profit[i] = (gain + cost) / total_loans\n\n11profit_plt = sns.lineplot(x = T, y = profit)\nprofit_plt.grid()\nprofit_plt.set(xlabel = r\"Threshold $t$\", ylabel = \"Expected Profit Per Loan\");\n\n\n1\n\nInitialize profit vector with 0s.\n\n2\n\nVector of scores for each borrower.\n\n3\n\nVector of 100 thresholds to test from \\([\\max(s)-1, \\max(s)+1]\\).\n\n4\n\nVector of predictions for score \\(\\geq\\) threshold (defaults).\n\n5\n\nTrue negatives.\n\n6\n\nFalse negatives.\n\n7\n\nSee Equation 2.\n\n8\n\nSee Equation 2.\n\n9\n\nTotal number of loans. (Cost is negative.)\n\n10\n\nSets profit[i] to 0 if no loans are made, otherwise to expected profit per loan.\n\n11\n\nPlots expected profit per loan for every threshold \\(t\\).\n\n\n\n\n\n\n\n\n\n\nFigure 4: Expected profit per loan for threshold \\(t\\).\n\n\n\n\n\n\nmax_profit_index = np.argmax(profit)\nprofit\nt = T[max_profit_index]\np = max(profit)\nt, p\n\n(4.721345771117955, 1757.8292670626274)\n\n\nIt looks like the best threshold is approximately 4.72 and the max expected profit per loan is approximately $1757.83. With weights, w, and threshold t, I am now ready to test the model."
  },
  {
    "objectID": "posts/loans_post/loans.html#evaluating-the-model-from-the-banks-perspective",
    "href": "posts/loans_post/loans.html#evaluating-the-model-from-the-banks-perspective",
    "title": "Bank Loans",
    "section": "Evaluating the Model from the Bank’s Perspective",
    "text": "Evaluating the Model from the Bank’s Perspective\nFirst, we need to prepare our testing data. I use the same method as my training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = prepare_data(df_test)\ny_test = df_test['loan_status']\nX_test = df_test.drop(['loan_status', 'loan_grade'], axis = 1)\nX_test = pd.get_dummies(df_test,\n                        columns = qual_cols)\n\nI use my weight vector, w, and my threshold t to compute the expected profit per loan for the bank using the testing data.\n\nnum_thresholds = 101\n\ns = linear_score(X_test[cols], w)\n\n# t = 4.721345771117955\ny_pred = s &gt;= t\nTN = X_test[((y_pred == 0) & (y_test == 0))]\nFN = X_test[((y_pred == 0) & (y_test == 1))]\ngain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\ncost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\ntotal_loans = TN.shape[0]+FN.shape[0]\nprofit = (gain + cost) / total_loans\n\nprofit\n\n1714.514369055408\n\n\nUsing my model, the bank can expect to make about $1714.51 per loan, slightly less than the training set."
  },
  {
    "objectID": "posts/loans_post/loans.html#evaluating-the-model-from-the-borrowers-perspective",
    "href": "posts/loans_post/loans.html#evaluating-the-model-from-the-borrowers-perspective",
    "title": "Bank Loans",
    "section": "Evaluating the Model from the Borrower’s Perspective",
    "text": "Evaluating the Model from the Borrower’s Perspective\nAn important aspect of good machine learning models is fairness and minimal bias. I will be exploring the impact of my autonomous decision-making system on different segments of the population of borrowers by age_group, loan_intent, and income_level.\n\nLoans by Age\n\n\nCode\nage_labels = [\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\"]\nage_bins = np.concatenate((np.arange(19, 70, 10), [150]))\ndf_test = get_age_groups(df_test)\n\ndf_test['y_pred'] = y_pred\n\nage_loans = (1-(df_test.groupby(['age_group','y_pred']).size() / df_test.groupby(['age_group']).size())).rename('proportion').reset_index()\nage_loans['y_pred'] = age_loans['y_pred'].astype('string')\n\nage_loans_plt = sns.barplot(age_loans,\n            x = 'age_group',\n            y = 'proportion',\n            hue = 'y_pred',\n            palette = pltt)\n\nage_loans_plt.set(xlabel = 'Age Group',\n                  ylabel = 'Proportion',\n                  title = 'Expected Loans by Age Group')\n\nage_loans_plt.legend(title = 'Loans');\n\n\n\n\n\n\n\n\nFigure 5: Expected loans by age group.\n\n\n\n\n\nFrom Figure 5, we can see that people older than 60 will have a much harder time getting a loan with only about 40% being approved of taking a loan.\n\n\nLoan Intent\n\n\nCode\nintent_loans = (1-df_test.groupby(['loan_intent']).agg({'loan_status':'mean', 'y_pred':'mean'})).reset_index().rename({'loan_status':'actual', 'y_pred':'predicted'}, axis = 1)\nintent_loans['difference'] = intent_loans['predicted'] - intent_loans['actual']\nintent_loans\n\n\n\n\nTable 4: Difference in actual and predicted loan rates by loan intent.\n\n\n\n\n\n\n\n\n\n\nloan_intent\nactual\npredicted\ndifference\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.712389\n0.676991\n-0.035398\n\n\n1\nEDUCATION\n0.832483\n0.693878\n-0.138605\n\n\n2\nHOMEIMPROVEMENT\n0.750000\n0.756494\n0.006494\n\n\n3\nMEDICAL\n0.715750\n0.672880\n-0.042870\n\n\n4\nPERSONAL\n0.779559\n0.688377\n-0.091182\n\n\n5\nVENTURE\n0.853734\n0.716805\n-0.136929\n\n\n\n\n\n\n\n\n\n\nFrom Table 4, we can see HOMEIMPROVEMENT was the only loan_intent to receive an increase in loans. All other intents received fewer loans from my model. The two biggest decreases in loans were for EDUCATION and VENTURE.\nAn important thing to note is the relatively small decrease in loans for MEDICAL despite the relatively large amount of defaults (Table 1). This is important because it would be unfair if those seeking medical attention could not recieve loans. I use unfair in the sense that bias against certain circumstances that are out of ones control is unfair. In the case of medical care, many illnesses or injuries are “random” and not entirely in ones control. Sometimes, unfortunate things and mistakes just happen. To be biased against those unable to pay for medical care themselves is unfair. For example, if a person develops cancer, is it fair if they are denied a loan simply because they have cancer and are more likely to default? No it is not. Everyone deserves a chance for medical care and banks should support the people in need of a loan for medical care with lower interest rates which could in turn lower default rates. As such, my model does a good job of being relatively unbiased towards MEDICAL loans.\n\n\nIncome Levels\n\n\nCode\nincome_bins = np.concatenate((np.arange(-1, 100000, 10000), [2000000]))\nincome_labels = []\n\nfor i in range(len(income_bins)-2):\n    income_labels = income_labels + [f'{income_bins[i]+1}-{income_bins[i+1]}']\n\nincome_labels = income_labels + ['100000+']\n\n\ndef get_income_class(df):\n    df_ = df.copy()\n    df_[\"income_level\"] = pd.cut(x = df_['person_income'], bins = income_bins, labels = income_labels)\n    return df_\n\ndf_test = get_income_class(df_test)\n\nincome_level_loans = 1-(df_test.groupby(['income_level'])['y_pred'].sum() / df_test.groupby(['income_level']).size())\n\nloan_income_plt = sns.lineplot(income_level_loans)\nloan_income_plt.set_xticklabels(loan_income_plt.get_xticklabels(), rotation=40, ha=\"right\")\nloan_income_plt.set(xlabel = 'Income',\n                    ylabel = 'Proportion',\n                    title = 'Proportion Given Loans by Income');\n\n\n\n\n\n\n\n\nFigure 6: Expected loans by income level.\n\n\n\n\n\nFrom Figure 6, we can see that as income level increases, more prospective borrowers are approved for loans. This makes sense, since higher income individuals are more likely to pay back loans. However, it is a bit worrying that lower income individuals will have a hard time getting loans, since they may be the ones that need it the most."
  },
  {
    "objectID": "posts/loans_post/loans.html#summary",
    "href": "posts/loans_post/loans.html#summary",
    "title": "Bank Loans",
    "section": "Summary",
    "text": "Summary\nIn this study, I built an automated decision-making system for loans. I optimized the predicted expected profit per loan for the bank by giving each prospective borrower a score(Equation 1) and then choosing a threshold to determine the borrowers that would receive a loan. Afterwards, I analyzed my model for biases by age group, loan intents, and income level.\nBy using the features loan_int_rate, loan_percent_income, int_percent_income, and person_home_ownership, the model produced an expected ouput of about $1714.51. This surpasses the base profit by about $895.39, an approximately 209% increase in profit.\nFurthermore, analysis of my model showed that it approves fewer loans to prospective borrowers over the age of 60, borrowers who intend to take a loan for education and venture, and borrowers with lower income. I also specifically analyzed the impact my model had on borrowers who are intending to take a loan out for medical reasons and found that there was a slight, but not a significant drop in loans approved. Lastly, I discussed why my model would’ve been unfair for borrowers looking to receive a medical loan if my model had a bias against them."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins_post/penguins.html",
    "href": "posts/penguins_post/penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\nThis data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins in the Palmer Archipelago, Antarctica. We will be attempting to find three features (two quantitative and one qualitative) that will be able to predict the penguins’ species with 100% testing accuracy.\n[The Palmer Penguins data was originally collected by @gormanEcologicalSexualDimorphism2014 and was nicely packaged and released for use in the data science community by @horstAllisonhorstPalmerpenguinsV02020.]"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#importing-data",
    "href": "posts/penguins_post/penguins.html#importing-data",
    "title": "Palmer Penguins",
    "section": "Importing Data",
    "text": "Importing Data\nFirst, let’s import the data from here, simplify the penguins’ species name, and look at the first five rows of data.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain['Species'] = train['Species'].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAfter importing the data, we need to clean our data and prepare our qualitative data. The below function uses one-hot encoding to turn the qualitative data into 0 and 1 columns. In this case, the values will be True and False. Additionally, the function also splits away our \\(y\\) variable which is stored in the variable y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # drop unwanted columns\n  df = df[df[\"Sex\"] != \".\"] # remove . from SEX data\n  df = df.dropna() # drop NaNs\n  y = le.transform(df[\"Species\"]) # set's species as y data\n  df = df.drop([\"Species\"], axis = 1) # drops species from x data\n  df = pd.get_dummies(df) # one-hot encoding, '0-1' columns\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "href": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "title": "Palmer Penguins",
    "section": "Visualization of a Couple Features",
    "text": "Visualization of a Couple Features\nLet’s visualize some potential features using two scatterplots and a summary table. In the two graphs below, I build up on these notes. In these notes, we discovered that Culmen Length (mm) and Culmen Depth (mm) are strong features to use to classify species type. In this study, I will be adding a qualitative data to help improve the test accuracy of the model.\n\nFigures\nThe figure on the left uses the Island column as the qualitative feature and the figure on the right uses the Sex column as the qualitative feature.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 6))\n\np1 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = train['Island'].dropna(), hue = train['Species'].dropna(), ax = ax[0])\np2 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = 'Sex_MALE', hue = train['Species'].dropna(), ax = ax[1])\np1.set_title('Species by Culmen Length, Culmen Depth, and Island')\np2.set_title('Species by Culmen Length, Culmen Depth, and Sex')\n#sns.move_legend(p1, \"upper right\", bbox_to_anchor=(1, .5))\n#plt.setp(ax[0].get_legend().get_texts(), fontsize='8')\n#sns.move_legend(p2, \"upper right\", bbox_to_anchor=(1, .5))\n\nText(0.5, 1.0, 'Species by Culmen Length, Culmen Depth, and Sex')\n\n\n\n\n\n\n\n\n\nWe can see that both figures have three distinct clusters of points. This is thanks to using Culmen Length (mm) and Culmen Depth (mm) as features. However, there are a few points that overlap regions. In the left figure, we can see that all Chinstrap and Gentoo penguins reside on Dream Island and Biscoe Island, respectively Meanwhile, Adelie penguins live on three islands, however, Adelie penguins that are near the general Chinstrap (blue) region of the figure reside on Torgersen Island.\nIn the right figure, we can see most Adelie and Gentoo penguins that are located near the Chinstrap (blue) region of the figure are male.\n\n\nSummary Table\nLet’s take a closer look at the Island feature. The table below describes the number of each species on each island.\n\n(train.groupby(by=['Species', 'Island']).aggregate({'Species':'count'}) / train.groupby(by=['Species']).aggregate({'Species':'count'})).rename(columns={'Species':'Distribution of Species'})\n\n\n\n\n\n\n\n\n\nDistribution of Species\n\n\nSpecies\nIsland\n\n\n\n\n\nAdelie\nBiscoe\n0.275\n\n\nDream\n0.375\n\n\nTorgersen\n0.350\n\n\nChinstrap\nDream\n1.000\n\n\nGentoo\nBiscoe\n1.000\n\n\n\n\n\n\n\nWe can see that Adelie penguins are spread out among all three islands, whereas, Chinstrap and Gentoo penguins reside only on Dream and Biscoe Island respectively. We can then most likely use Culmen Length (mm) and Culmen Depth (mm) to seperate the Adelie penguins from Gentoos and Chinstraps on Dream and Biscoe Island."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#finding-the-best-features",
    "href": "posts/penguins_post/penguins.html#finding-the-best-features",
    "title": "Palmer Penguins",
    "section": "Finding the Best Features",
    "text": "Finding the Best Features\nWe will try to find the best features for multiple models: logisitc regression, random forest classifer, decision tree classifier, and support vector machine.\nThe function below uses cross-validation to determine the three best features (two qualitative and one qualitative) and the respective score for a given model. We will be using five folds for cross-validation.\n\nfrom itertools import combinations\n\n#get all qualitative and quantitative columns\nall_qual_cols = list({col.split('_')[0] for col in X_train.select_dtypes(exclude=['number']).columns}) # get pre-one-hot encoded column names for qual data\nall_quant_cols = X_train.select_dtypes(exclude=[\"bool_\",\"object_\"]).columns # quant data\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\n\n# uses cross-validation to find the best scores for each combination of two quantitative columns and one qualitative column\n# returns ([best columns], best score)\ndef cross_val(model, cv, best_score):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for qual in all_qual_cols: \n      qual_cols = [col for col in X_train.columns if qual in col ]\n      for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols\n        #LR.fit(X_train[cols], y_train)\n        cv_scores = cross_val_score(model, X_train[cols], y_train, cv = cv) # cv = 5 assessments, 100 / 5 = 20% of training data withheld at a time for corss-validation\n        col_scores = (cols, cv_scores.mean())\n        if col_scores[1] &gt; best_score[1]:\n          best_score = col_scores\n    return best_score\n\nThe function below creates a plot of decision regions for a given model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"center left\", bbox_to_anchor=(1, .5))\n      \n      plt.tight_layout()\n\n\nLogistic Regression\n\nLR = LogisticRegression()\n\nbest_score_LR = ([], 0)\nbest_score_LR = cross_val(LR, 5, best_score_LR)\nprint(best_score_LR)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9961538461538462)\n\n\nFor logistic regression, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Island with a mean cross-validation score of about 99.6% accuracy.\n\n\nRandom Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nbest_score_RFC = ([], 0)\n\nbest_score_RFC = cross_val(RFC, 5, best_score_RFC)\n\nprint(best_score_RFC)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9883107088989442)\n\n\nFor the random forest classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98.8% accuracy.\n\n\nDecision Tree Classifier\nFor the decision tree classifier, we must also find the optimal max depth. We test integer values for 5 to 50 and choose the best value.\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepth = np.arange(5, 50)\nbest_score_DTC = ([], 0)\nbest_d = 10\n\nfor d in depth:\n    DTC = DecisionTreeClassifier(max_depth = d)\n    #print(d)\n    cross_val_DTC = cross_val(DTC, 5, best_score_DTC)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_DTC[1] &gt; best_score_DTC[1]:\n        best_score_DTC = cross_val_DTC\n        best_d = d\n\nprint(best_score_DTC, best_d)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9765460030165913) 6\n\n\nFor the decision tree classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 97.6% accuracy using max_depth = 6.\n\n\nSupport Vector Machine\nFor the support vector machine, we must find the best gamma value. We test values from ranging from \\(10^{-5}\\) to \\(10^5\\).\n\nfrom sklearn.svm import SVC\n\ngamma = 10**np.arange(-5, 5, dtype = float)\nbest_score_SVM = ([], 0)\nbest_g = 0\n\nfor g in gamma:\n    SVM = SVC(gamma = g)\n    #print(d)\n    cross_val_SVM = cross_val(SVM, 5, best_score_SVM)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_SVM[1] &gt; best_score_SVM[1]:\n        best_score_SVM = cross_val_SVM\n        best_g = g\n\nprint(best_score_SVM, best_g)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9805429864253394) 0.1\n\n\nFor the support vector machine, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98% accuracy using gamma = 0.1."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#training-the-data",
    "href": "posts/penguins_post/penguins.html#training-the-data",
    "title": "Palmer Penguins",
    "section": "Training the Data",
    "text": "Training the Data\n\nLogistic Regression\n\ncols_LR = best_score_LR[0]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    LR.fit(X_train[cols_LR], y_train)\n    score_LR = LR.score(X_train[cols_LR], y_train)\n\nscore_LR\n\n0.99609375\n\n\nSo, using logistic regression on Culmen Length (mm), Culmen Depth (mm), and Island has a training accuracy of &gt;99.6%.\n\n\nRandom Forest Classifier\n\ncols_RFC = best_score_RFC[0]\nRFC.fit(X_train[cols_RFC], y_train)\nRFC.score(X_train[cols_RFC], y_train)\n\n1.0\n\n\nSo, using the random forest classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 100%.\n\n\nDecision Tree Classifier\n\ncols_DTC = best_score_DTC[0]\nDTC = DecisionTreeClassifier(max_depth = best_d)\nDTC.fit(X_train[cols_DTC], y_train)\nDTC.score(X_train[cols_DTC], y_train)\n\n1.0\n\n\nSo, using the decision tree classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 98.8%.\n\n\nSupport Vector Machine\n\ncols_SVM = best_score_SVM[0]\nSVM = SVC(gamma = best_g)\nSVM.fit(X_train[cols_SVM], y_train)\nSVM.score(X_train[cols_SVM], y_train)\n\n0.9921875\n\n\nSo, using the support vector machine on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of &gt;99.2%."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#plotting-training-decision-regions",
    "href": "posts/penguins_post/penguins.html#plotting-training-decision-regions",
    "title": "Palmer Penguins",
    "section": "Plotting Training Decision Regions",
    "text": "Plotting Training Decision Regions\n\nLogisitic Regression\n\nplot_regions(LR, X_train[cols_LR], y_train)\n\n\n\n\n\n\n\n\n\n\nRandom Forest Walk Classifer\n\nplot_regions(RFC, X_train[cols_RFC], y_train)\n\n\n\n\n\n\n\n\n\n\nDecision Tree Classifier\n\nplot_regions(DTC, X_train[cols_DTC], y_train)\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machine\n\nplot_regions(SVM, X_train[cols_SVM], y_train)"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#testing",
    "href": "posts/penguins_post/penguins.html#testing",
    "title": "Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nWith our models trained, we can now test the models on the test data pulled from here.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest['Species'] = test['Species'].str.split().str.get(0)\nX_test, y_test = prepare_data(test)\n\n\nLogistic Regression\n\nLR.score(X_test[cols_LR], y_test)\n\n1.0\n\n\nLogistic regression has a test score of 100%! We have accomplished our objective, but let’s also look at the other models.\n\n\nRandom Forest Classifier\n\nRFC.score(X_test[cols_RFC], y_test)\n\n0.9852941176470589\n\n\nThe random forest classifier has a test score of about 99.5%.\n\n\nDecision Tree Classifier\n\nDTC.score(X_test[cols_DTC], y_test)\n\n0.9852941176470589\n\n\nThe decision tree classifier has a test score of about 98.5% as well.\n\n\nSupport Vector Machine Classifier\n\nSVM.score(X_test[cols_SVM], y_test)\n\n0.9852941176470589\n\n\nThe support vector machine also has a test score of about 98.5%."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#plotting-testing-decision-regions",
    "href": "posts/penguins_post/penguins.html#plotting-testing-decision-regions",
    "title": "Palmer Penguins",
    "section": "Plotting Testing Decision Regions",
    "text": "Plotting Testing Decision Regions\nBelow are the decision regions on test data for each model.\n\nLogistic Regression\n\nplot_regions(LR, X_test[cols_LR], y_test)\n\n\n\n\n\n\n\n\n\n\nRandom Forest Classifier\n\nplot_regions(RFC, X_test[cols_RFC], y_test)\n\n\n\n\n\n\n\n\n\n\nDecision Tree Classifier\n\nplot_regions(DTC, X_test[cols_DTC], y_test)\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machine\n\nplot_regions(SVM, X_test[cols_SVM], y_test)"
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html",
    "href": "posts/replication_post/bias_replication_post.html",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "",
    "text": "Image source: berkeley.edu"
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#abstract",
    "href": "posts/replication_post/bias_replication_post.html#abstract",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Abstract",
    "text": "Abstract\nThis replication study is based on the journal article “Dissecting racial bias in an algorithm used to manage the health of populations”.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\nThe article analyzes the racial bias caused by healthcare algorithms that predict healthcare costs instead of illness. In the health system studied by the article, patients with risk scores above the 97th percentile are admitted into the care management program while those in the 55th percentile are referred to their primary care physician who are asked to consider whether the patient would benefit from the program. For equal risk scores, Blacks typically have a higher number of illnesses than Whites. This results in Blacks being less likely to be admitted to the care management program than Whites are. However, it also finds that because these algorithms target patients with high costs, the results are inconsistent by algorithmic bias, such as calibration. That is, across the entire risk distribution, predictions of cost are unbiased towards Whites or Blacks by risk score. This study aimed to replicate these findings through Figure 1 and 3 from the article and calculate the cost disparity shown in Figure 3 using linear regression. After replicating the figures and calculating the cost disparity, this study confirmed the findings of the article. At similar risk scores, Blacks typically are typically more ill than Whites and conditional on risk score, there is no significant difference between predicted costs."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#overview-of-the-data",
    "href": "posts/replication_post/bias_replication_post.html#overview-of-the-data",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Overview of the Data",
    "text": "Overview of the Data\nThe authors did not share the “real” data to protect patient privacy. Instead, the data contains attributes for each patient with randomized data that preserves many of the same patterns and trends. More information on the data can be found here. The attributes used in this replication study are:\n\nrisk_score_t: the patient’s risk assigned by the algorithm\ncost_t: the patient’s total medical expenditure during the study period\nrace: the patient’s self-reported race (white or black)\ngagne_sum_t: the patient’s total number of active chronic illnesses during the study period\ndem_female: whether a patient is female (1) or not (0)\n\n\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns"
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#reproducing-figure-1",
    "href": "posts/replication_post/bias_replication_post.html#reproducing-figure-1",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Reproducing Figure 1",
    "text": "Reproducing Figure 1\nFigure 1 in the article depicts the relationship between the mean number of chronic illnesses and the percentiles of risk scores for patients divided by race and gender. To reproduce Figure 1, each risk_score_t must be converted into its percentile value. The percentiles are stored in the column risk_percentile.\n\nfrom scipy import stats\n\n1df['risk_percentile'] = stats.percentileofscore(df['risk_score_t'], df['risk_score_t'], kind = 'rank').round(decimals = 0)\n\n\n1\n\nGets the percentile for each risk_score_t by kind = 'rank' and rounds it to the nearest integer.\n\n\n\n\nHere’s what the percentiles of risk scores look like compared to the actual risk scores.\n\ndf[['risk_score_t', 'risk_percentile']].head()\n\n\n\n\n\n\n\n\nrisk_score_t\nrisk_percentile\n\n\n\n\n0\n1.987430\n35.0\n\n\n1\n7.677934\n86.0\n\n\n2\n0.407678\n4.0\n\n\n3\n0.798369\n11.0\n\n\n4\n17.513165\n98.0\n\n\n\n\n\n\n\nBelow is the replication of Figure 1.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n1risk_ill = df.groupby(['risk_percentile', 'race', 'dem_female'], observed = False).agg({'gagne_sum_t':'mean'})\n\n2risk_ill_plt = sns.relplot(data=risk_ill,\n                           col='dem_female',\n                           col_wrap=2,\n                           x = 'gagne_sum_t',\n                           y = 'risk_percentile',\n                           hue = 'race',\n                           style = 'race',\n                           kind='scatter',\n                           height=4,\n                           palette = 'viridis')\n\nrisk_ill_plt.set(xlabel = 'Mean Number of Chronic Illnesses',\n                 ylabel = 'Percentile of Risk Score (from algorithm)',\n                 xticks = [0, 2, 4, 6, 8],\n                 yticks = [0, 20, 40, 60, 80, 100],)\n\nrisk_ill_plt.axes[0].set(title = 'Male')\nrisk_ill_plt.axes[1].set(title = 'Female');\n\n\n\n1\n\nCalculates the mean number of chronic illnesses for a patient grouped by their risk score percentile, race, and gender.\n\n2\n\nPlots the relationship between the mean number of chronic illnesses and risk score percentile.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Male patients\n\n\n\n\nRisk percentile by mean number of chronic illnesses by race and gender.\n\nAs seen in Figure 1 of this study, for a given mean number of chronic illnesses, Blacks are typically given a much lower risk score than Whites are. As a result, they will be much less likely to be referred to the high-risk care management program than Whites will. This corroborates the findings of the article."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#reproducing-figure-3",
    "href": "posts/replication_post/bias_replication_post.html#reproducing-figure-3",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Reproducing Figure 3",
    "text": "Reproducing Figure 3\nFigure 3 from the article depicts the the total expenditure for patients given their percentile of risk scores and total number of chronic illnesses. Below is the replication of Figure 3.\n\n\nCode\n1risk_cost = df.groupby(['risk_percentile', 'race'], observed = False).agg({'cost_t':'mean'})\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(1, 2, figsize = (10, 5), sharey = True)\n\n2risk_cost_plt = sns.scatterplot(risk_cost,\n                               x = 'risk_percentile',\n                               y = 'cost_t',\n                               hue = 'race',\n                               style = 'race',\n                               palette = 'viridis',\n                               ax = ax[0])\n\nrisk_cost_plt.set(xlabel = 'Percentile of Risk Scores',\n                 ylabel = 'Mean Total Medical Expenditure',\n                 xticks = [0, 20, 40, 60, 80, 100],\n                 title = 'Total Expenditure by Percentile of Risk Score');\n\n3ax[0].set_yscale('log')\n\n4ill_cost = df.groupby(['gagne_sum_t', 'race'], observed = False).agg({'cost_t':'mean'}).reset_index()\n\n5ill_cost_plt = sns.scatterplot(ill_cost,\n                               x = 'gagne_sum_t',\n                               y = 'cost_t',\n                               hue = 'race',\n                               style = 'race',\n                               palette = 'viridis',\n                               ax = ax[1])\n\nill_cost_plt.set(xticks = [0, 5, 10, 15])\n\nill_cost_plt.set(xlabel = 'Total Number of Chronic Illnesses',\n                 title = 'Total Expenditure by Number of Chronic Illnesses');\n\n\n\n1\n\nCalculates the mean total cost of patients grouped by risk score percentile and race.\n\n2\n\nPlots the mean total cost of patients by risk score percentile.\n\n3\n\nSets the y-axis to be of logarithmic scale.\n\n4\n\nCalculates the mean total cost of patients grouped by the total number of chronic illnesses and race.\n\n5\n\nPlots the mean total cost of patients by total number of chronic illnesses.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Replication of Figure 3 from the article.\n\n\n\n\n\nLooking at Figure 2 from this study, we can see that both Blacks and Whites have similar total costs by both the percentile of risk scores and the total number of chronic illnesses. Additionally, it’s also important to note that most patients have five or fewer chronic illnesses, so looking at Figure 2, we can see that Blacks actually generate lower costs than Whites for patients with fewer than five chronic illnesses. In fact, the article states that Blacks generate on average about $1801 less per year in costs which Figure 1 seems to support."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#modeling-cost-disparity",
    "href": "posts/replication_post/bias_replication_post.html#modeling-cost-disparity",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Modeling Cost Disparity",
    "text": "Modeling Cost Disparity\nBuilding on the findings from Figure 1, I attempt to quantify the disparity in cost for patients with five or fewer chronic illnesses. The reason for only looking at patients with five or fewer chronic illnesses, as stated above, is because there are much fewer patients with more than five chronic illnesses. To model the mean total expenditure for patients with five or fewer chronic illnesses, I use polynomial linear regression to predict the logarithm of the costs then extract the coefficient, \\(w_b\\), given to Blacks. In the context of a log-transformed linear model, \\(e^{w_b}\\) is an estimate for the percentage of cost generated by Blacks compared to a Whites with an equal number of chronic illnesses. Additionally, the reason for a polynomial linear regression model is to account for nonlinearity.\nTo confirm the claim that most patients have five or fewer chronic illnesses, here is the percentage of patients with five or fewer chronic illnesses.\n\n1(df['gagne_sum_t'] &lt;= 5).mean() * 100\n\n\n1\n\nCalculates the percentage of patients with five or fewer chronic illnesses.\n\n\n\n\n95.53952115447689\n\n\nIndeed, more than 95% of the patients have five or fewer chronic illnesses. Next, I extract the patients with five or fewer and non-zero expenditure and calculate the logarithm of the costs. The reason for only extracting patients with non-zero expenditure is because \\(\\log{(0)}\\) is undefined.\n\n1df_train = df[(df['gagne_sum_t'] &lt;= 5) & (df['cost_t'] != 0)]\n\nimport numpy as np\n\n2df_train['log_cost_t'] = np.log(df_train['cost_t'])\n\n\n1\n\nExtracts all patients with five or fewer illnesses and non-zero expenditure into a new dataframe, df_train.\n\n2\n\nCreates new column, log_cost_t, that is the logarithm of the costs.\n\n\n\n\nThen, I split the data into the target variable log_cost_t and the predictor variables gagne_sum_t, and race. I then one-hot encode race for linear regression.\n\n1y_train = df_train['log_cost_t']\n2X_train = df_train.drop(['log_cost_t'], axis = 1)\n3X_train = X_train[['gagne_sum_t', 'race']]\n4X_train = pd.get_dummies(X_train, columns=['race'])\n\nX_train.head()\n\n\n1\n\nSet \\(y\\) data to log_cost_t.\n\n2\n\nDrops log_cost_t from \\(x\\) data.\n\n3\n\nGet only gagne_sum_t and race for \\(x\\) data.\n\n4\n\nOne-hot encodes race.\n\n\n\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace_black\nrace_white\n\n\n\n\n0\n0\nFalse\nTrue\n\n\n1\n3\nFalse\nTrue\n\n\n2\n0\nFalse\nTrue\n\n\n3\n0\nFalse\nTrue\n\n\n4\n1\nFalse\nTrue\n\n\n\n\n\n\n\nBelow is a function that constructs new columns for polynomials of various degrees.\n\n1def add_polynomial_features(X, degree):\n  X_ = X.copy()\n\n  for j in range(2, degree):\n    X_[f'poly_{j}'] = X_['gagne_sum_t']**j\n  return X_\n\n\n1\n\nCreates a new columns $2 = ^2,… , j = ^j $.\n\n\n\n\nNext, I use cross validation to find the best degree polynomial for the linear regression model.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n1LR = LinearRegression()\n\n2degree = np.arange(1, 21)\n3cross_score = np.zeros(len(degree))\n\n4for i in degree:\n    X = add_polynomial_features(X_train, i)\n    cross_score[i-1] = cross_val_score(LR, X, y_train, cv = 5).mean()\n\n\n1\n\nInitializes linear regression instance.\n\n2\n\nVector to of degrees to test.\n\n3\n\nVector of zeroes to store the mean cross validation scores.\n\n4\n\nTests all degrees in and stores the mean cross validation scores in cross_score. Uses five-fold cross validation.\n\n\n\n\nAfter, I get the degree (14) with the best score for use in the linear regression model.\n\n1best_i = np.argmax(cross_score)\n2best_d = degree[best_i]\nbest_d\n\n\n1\n\nGets index of best mean cross validation score.\n\n2\n\nGets the degree with best mean cross validation score.\n\n\n\n\n14\n\n\nNow, I train the linear regression model using the best degree polynomial and calculate the estimated percentage of cost disparity between Blacks and Whites with the same number of chronic illnesses.\n\n1X = add_polynomial_features(X_train, best_d)\n2LR.fit(X, y_train)\nscore = LR.score(X, y_train)\n3w = LR.coef_\n4np.exp(w[1])\n\n\n1\n\nSet X to be dataset with the 14 degree polynomial of gagne_sum_t.\n\n2\n\nTrain X using linear regression and retrieve the score.\n\n3\n\nGet the coefficients of the trained model and store it in w.\n\n4\n\nCalculates the estimated percentage of cost disparity between Blacks and Whites with the same number of chronic illnesses (\\(e^{w[1]}\\) where \\(w[1] = w_b\\))\n\n\n\n\n0.8680649776385329\n\n\nAs we can see, Blacks generate an estimated 86.8% the cost of Whites. This roughly confirms the findings from the article that Blacks generate less costs than Whites."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#discussion",
    "href": "posts/replication_post/bias_replication_post.html#discussion",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "Discussion",
    "text": "Discussion\nFor similar risk scores and number of illnesses, Blacks and Whites generate similar costs (Figure 1). In fact, Blacks generate an estimated 87% less cost than Whites. However, when comparing Blacks and Whites by their risk score based on their chronic number of illnesses, Blacks have a much lower risk score per number of illnesses (Figure 1). As a result, they are less likely to be accepted into the system’s care management program. Though the study meets the calibration criterion for fairness, it violates the independence criterion for fairness as Blacks have a lower acceptance rate into the system’s care managemet program than Whites. This could present a problem, since most industry-wide algorithms predict cost leading to bias against Blacks. This results in a system wide case of Blacks being unable to receive the healthcare they need, which in turn reduces their medical expenditures. To fix this bias, the authors of the article used an index variable which combined cost and health prediction, reducing the bias by 87%. The hope is that by showing that label bias is fixable other manufacturers will be prompted to make similar fixes, minimizing the risks of algorithmic predictions while still benefitting from them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Machine Learning",
    "section": "",
    "text": "Replication Study: Racial Bias in Health Algorithms\n\n\n\n\n\nA replication study of “Dissecting racial bias in an algorithm used to manage the health of populations” by Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan.\n\n\n\n\n\nMar 7, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nBank Loans\n\n\n\n\n\nOptimizing and assessing the impact of an automated decision system for bank loans.\n\n\n\n\n\nMar 3, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nPredicting penguin species from the Palmer Archipelago.\n\n\n\n\n\nFeb 21, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]