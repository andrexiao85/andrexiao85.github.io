[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Andre Xiao and this is a blog for my machine learning class!"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins_post/penguins.html",
    "href": "posts/penguins_post/penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\nThis data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins in the Palmer Archipelago, Antarctica. We will be attempting to find three features (two quantitative and one qualitative) that will be able to predict the penguins’ species with 100% testing accuracy.\n[The Palmer Penguins data was originally collected by @gormanEcologicalSexualDimorphism2014 and was nicely packaged and released for use in the data science community by @horstAllisonhorstPalmerpenguinsV02020.]"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#importing-data",
    "href": "posts/penguins_post/penguins.html#importing-data",
    "title": "Palmer Penguins",
    "section": "Importing Data",
    "text": "Importing Data\nFirst, let’s import the data from here, simplify the penguins’ species name, and look at the first five rows of data.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain['Species'] = train['Species'].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAfter importing the data, we need to clean our data and prepare our qualitative data. The below function uses one-hot encoding to turn the qualitative data into 0 and 1 columns. In this case, the values will be True and False. Additionally, the function also splits away our \\(y\\) variable which is stored in the variable y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # drop unwanted columns\n  df = df[df[\"Sex\"] != \".\"] # remove . from SEX data\n  df = df.dropna() # drop NaNs\n  y = le.transform(df[\"Species\"]) # set's species as y data\n  df = df.drop([\"Species\"], axis = 1) # drops species from x data\n  df = pd.get_dummies(df) # one-hot encoding, '0-1' columns\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "href": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "title": "Palmer Penguins",
    "section": "Visualization of a Couple Features",
    "text": "Visualization of a Couple Features\nLet’s visualize some potential features using two scatterplots and a summary table. In the two graphs below, I build up on these notes. In these notes, we discovered that Culmen Length (mm) and Culmen Depth (mm) are strong features to use to classify species type. In this study, I will be adding a qualitative data to help improve the test accuracy of the model.\n\nFigures\nThe figure on the left uses the Island column as the qualitative feature and the figure on the right uses the Sex column as the qualitative feature.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfig, ax = plt.subplots(1, 2, figsize = (15, 6))\n\np1 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = train['Island'].dropna(), hue = train['Species'].dropna(), ax = ax[0])\np2 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = 'Sex_MALE', hue = train['Species'].dropna(), ax = ax[1])\np1.set_title('Species by Culmen Length, Culmen Depth, and Island')\np2.set_title('Species by Culmen Length, Culmen Depth, and Sex')\n#sns.move_legend(p1, \"upper right\", bbox_to_anchor=(1, .5))\n#plt.setp(ax[0].get_legend().get_texts(), fontsize='8')\n#sns.move_legend(p2, \"upper right\", bbox_to_anchor=(1, .5))\n\nText(0.5, 1.0, 'Species by Culmen Length, Culmen Depth, and Sex')\n\n\n\n\n\n\n\n\n\nWe can see that both figures have three distinct clusters of points. This is thanks to using Culmen Length (mm) and Culmen Depth (mm) as features. However, there are a few points that overlap regions. In the left figure, we can see that all Chinstrap and Gentoo penguins reside on Dream Island and Biscoe Island, respectively Meanwhile, Adelie penguins live on three islands, however, Adelie penguins that are near the general Chinstrap (blue) region of the figure reside on Torgersen Island.\nIn the right figure, we can see most Adelie and Gentoo penguins that are located near the Chinstrap (blue) region of the figure are male.\n\n\nSummary Table\nLet’s take a closer look at the Island feature. The table below describes the number of each species on each island.\n\n(train.groupby(by=['Species', 'Island']).aggregate({'Species':'count'}) / train.groupby(by=['Species']).aggregate({'Species':'count'})).rename(columns={'Species':'Distribution of Species'})\n\n\n\n\n\n\n\n\n\nDistribution of Species\n\n\nSpecies\nIsland\n\n\n\n\n\nAdelie\nBiscoe\n0.275\n\n\nDream\n0.375\n\n\nTorgersen\n0.350\n\n\nChinstrap\nDream\n1.000\n\n\nGentoo\nBiscoe\n1.000\n\n\n\n\n\n\n\nWe can see that Adelie penguins are spread out among all three islands, whereas, Chinstrap and Gentoo penguins reside only on Dream and Biscoe Island respectively. We can then most likely use Culmen Length (mm) and Culmen Depth (mm) to seperate the Adelie penguins from Gentoos and Chinstraps on Dream and Biscoe Island."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#finding-the-best-features",
    "href": "posts/penguins_post/penguins.html#finding-the-best-features",
    "title": "Palmer Penguins",
    "section": "Finding the Best Features",
    "text": "Finding the Best Features\nWe will try to find the best features for multiple models: logisitc regression, random forest classifer, decision tree classifier, and support vector machine.\nThe function below uses cross-validation to determine the three best features (two qualitative and one qualitative) and the respective score for a given model. We will be using five folds for cross-validation.\n\nfrom itertools import combinations\n\n#get all qualitative and quantitative columns\nall_qual_cols = list({col.split('_')[0] for col in X_train.select_dtypes(exclude=['number']).columns}) # get pre-one-hot encoded column names for qual data\nall_quant_cols = X_train.select_dtypes(exclude=[\"bool_\",\"object_\"]).columns # quant data\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\n\n# uses cross-validation to find the best scores for each combination of two quantitative columns and one qualitative column\n# returns ([best columns], best score)\ndef cross_val(model, cv, best_score):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for qual in all_qual_cols: \n      qual_cols = [col for col in X_train.columns if qual in col ]\n      for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols\n        #LR.fit(X_train[cols], y_train)\n        cv_scores = cross_val_score(model, X_train[cols], y_train, cv = cv) # cv = 5 assessments, 100 / 5 = 20% of training data withheld at a time for corss-validation\n        col_scores = (cols, cv_scores.mean())\n        if col_scores[1] &gt; best_score[1]:\n          best_score = col_scores\n    return best_score\n\nThe function below creates a plot of decision regions for a given model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"center left\", bbox_to_anchor=(1, .5))\n      \n      plt.tight_layout()\n\n\nLogistic Regression\n\nLR = LogisticRegression()\n\nbest_score_LR = ([], 0)\nbest_score_LR = cross_val(LR, 5, best_score_LR)\nprint(best_score_LR)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9961538461538462)\n\n\nFor logistic regression, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Island with a mean cross-validation score of about 99.6% accuracy. ### Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nbest_score_RFC = ([], 0)\n\nbest_score_RFC = cross_val(RFC, 5, best_score_RFC)\n\nprint(best_score_RFC)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9883107088989442)\n\n\nFor the random forest classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98.8% accuracy. ### Decision Tree Classifier For the decision tree classifier, we must also find the optimal max depth. We test integer values for 5 to 50 and choose the best value.\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepth = np.arange(5, 50)\nbest_score_DTC = ([], 0)\nbest_d = 10\n\nfor d in depth:\n    DTC = DecisionTreeClassifier(max_depth = d)\n    #print(d)\n    cross_val_DTC = cross_val(DTC, 5, best_score_DTC)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_DTC[1] &gt; best_score_DTC[1]:\n        best_score_DTC = cross_val_DTC\n        best_d = d\n\nprint(best_score_DTC, best_d)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9765460030165913) 6\n\n\nFor the decision tree classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 97.6% accuracy using max_depth = 6. ### Support Vector Machine\nFor the support vector machine, we must find the best gamma value. We test values from ranging from \\(10^{-5}\\) to \\(10^5\\).\n\nfrom sklearn.svm import SVC\n\ngamma = 10**np.arange(-5, 5, dtype = float)\nbest_score_SVM = ([], 0)\nbest_g = 0\n\nfor g in gamma:\n    SVM = SVC(gamma = g)\n    #print(d)\n    cross_val_SVM = cross_val(SVM, 5, best_score_SVM)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_SVM[1] &gt; best_score_SVM[1]:\n        best_score_SVM = cross_val_SVM\n        best_g = g\n\nprint(best_score_SVM, best_g)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9805429864253394) 0.1\n\n\nFor the support vector machine, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98% accuracy using gamma = 0.1."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#training-the-data",
    "href": "posts/penguins_post/penguins.html#training-the-data",
    "title": "Palmer Penguins",
    "section": "Training the Data",
    "text": "Training the Data\n\nLogistic Regression\n\ncols_LR = best_score_LR[0]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    LR.fit(X_train[cols_LR], y_train)\n    score_LR = LR.score(X_train[cols_LR], y_train)\n\nscore_LR\n\n0.99609375\n\n\nSo, using logistic regression on Culmen Length (mm), Culmen Depth (mm), and Island has a training accuracy of &gt;99.6%. ### Random Forest Classifier\n\ncols_RFC = best_score_RFC[0]\nRFC.fit(X_train[cols_RFC], y_train)\nRFC.score(X_train[cols_RFC], y_train)\n\n1.0\n\n\nSo, using the random forest classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 100%.\n\n\nDecision Tree Classifier\n\ncols_DTC = best_score_DTC[0]\nDTC = DecisionTreeClassifier(max_depth = best_d)\nDTC.fit(X_train[cols_DTC], y_train)\nDTC.score(X_train[cols_DTC], y_train)\n\n1.0\n\n\nSo, using the decision tree classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 98.8%. ### Support Vector Machine\n\ncols_SVM = best_score_SVM[0]\nSVM = SVC(gamma = best_g)\nSVM.fit(X_train[cols_SVM], y_train)\nSVM.score(X_train[cols_SVM], y_train)\n\n0.9921875\n\n\nSo, using the support vector machine on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of &gt;99.2%. ## Plotting Training Decision Regions ### Logisitic Regression\n\nplot_regions(LR, X_train[cols_LR], y_train)\n\n\n\n\n\n\n\n\n\n\nRandom Forest Walk Classifer\n\nplot_regions(RFC, X_train[cols_RFC], y_train)\n\n\n\n\n\n\n\n\n\n\nDecision Tree Classifier\n\nplot_regions(DTC, X_train[cols_DTC], y_train)\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machine\n\nplot_regions(SVM, X_train[cols_SVM], y_train)"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#testing",
    "href": "posts/penguins_post/penguins.html#testing",
    "title": "Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nWith our models trained, we can now test the models on the test data pulled from here.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest['Species'] = test['Species'].str.split().str.get(0)\nX_test, y_test = prepare_data(test)\n\n\nLogistic Regression\n\nLR.score(X_test[cols_LR], y_test)\n\n1.0\n\n\nLogistic regression has a test score of 100%! We have accomplished our objective, but let’s also look at the other models.\n\n\nRandom Forest Classifier\n\nRFC.score(X_test[cols_RFC], y_test)\n\n0.9852941176470589\n\n\nThe random forest classifier has a test score of about 99.5%. ### Decision Tree Classifier\n\nDTC.score(X_test[cols_DTC], y_test)\n\n0.9852941176470589\n\n\nThe decision tree classifier has a test score of about 98.5% as well. ### Support Vector Machine Classifier\n\nSVM.score(X_test[cols_SVM], y_test)\n\n0.9852941176470589\n\n\nThe support vector machine also has a test score of about 98.5%. ## Plotting Testing Decision Regions Below are the decision regions on test data for each model.\n\n\nLogistic Regression\n\nplot_regions(LR, X_test[cols_LR], y_test)\n\n\n\n\n\n\n\n\n\n\nRandom Forest Classifier\n\nplot_regions(RFC, X_test[cols_RFC], y_test)\n\n\n\n\n\n\n\n\n\n\nDecision Tree Classifier\n\nplot_regions(DTC, X_test[cols_DTC], y_test)\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machine\n\nplot_regions(SVM, X_test[cols_SVM], y_test)"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Machine Learning",
    "section": "",
    "text": "Palmer Penguins\n\n\n\n\n\nPredicting penguin species from the Palmer Archipelago.\n\n\n\n\n\nFeb 21, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]