[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Andre Xiao. I am a student-athlete majoring in Computer Science and Mathematics. This blog is for my machine learning class, CSCI 0451, and potentially other future projects. I also have 2 cats :)."
  },
  {
    "objectID": "posts/loans_post/loans.html",
    "href": "posts/loans_post/loans.html",
    "title": "Bank Loans",
    "section": "",
    "text": "Image source: techfunnel.com"
  },
  {
    "objectID": "posts/loans_post/loans.html#overview",
    "href": "posts/loans_post/loans.html#overview",
    "title": "Bank Loans",
    "section": "1 Overview",
    "text": "1 Overview\nThis study attempts to optimize the maximum total expected profit from loans for an unknown bank. I will be trying to create an automated decision-making sytem that gives each prospective borrower a score and decides whether to give them a loan based on a threshold, \\(t\\), where each score, \\(s_i\\), of borrower, \\(i\\), is defined as \\[s_i = \\langle \\mathbf{X}_i, \\mathbf{w} \\rangle \\tag{1}\\] where \\(\\mathbf{X}_i\\) is the vector of features for borrower, \\(i\\), and \\(\\mathbf{w}\\) is the vector of weights for each feature. Using this, the goal is to find \\(\\mathbf{w}\\) and \\(t\\) which maximize the total expected profit per loan for the bank.\nTo accomplish this, I use logistic regression to determine \\(\\mathbf{w}\\) and calculate the expected profit per loan for various \\(t\\)-values. In this study, we assume that each loan is a 10-year loan and 75% of the interest is used to pay for operating costs such as employee salaries. We also assume that defaults occur after three years and the bank loses 70% of the principal. That is, \\[\\begin{align*}\n    \\textbf{profit} &= \\text{loan amount}\\cdot(1+0.25\\cdot\\text{interest rate})^{10} - \\text{loan amount} \\\\\n    \\textbf{cost} &= \\text{loan amount}\\cdot(1+0.25\\cdot\\text{interest rate})^3 - 1.7\\cdot\\text{loan amount}\n\\end{align*} \\tag{2}\\]\nIn the end, the expected profit per loan for the bank was $1714.51.\nAfterwards, I discuss the impact that the automated system has on different segments of the population of prospective borrowers. I explore how the system impacts different age groups, purposes for the loan request, and income levels. I find that it is easier for younger age groups to receive loans, harder to take out student loans and loans for ventures, and it is much easier for higher income-levels to receive loans under my system."
  },
  {
    "objectID": "posts/loans_post/loans.html#preparing-the-training-data",
    "href": "posts/loans_post/loans.html#preparing-the-training-data",
    "title": "Bank Loans",
    "section": "2 Preparing the Training Data",
    "text": "2 Preparing the Training Data\n\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nThe dataset contains the following columns:\n\nperson_age: the age of the prospective borrower.\nperson_income: the income of the prospective borrower at the time of application.\nperson_home_ownership: the home ownership status of the prospective borrower at the time of application. Possible values are MORTGAGE, OWN, RENT, and OTHER.\nperson_emp_length: the length of the most recent employment for the prospective borrower, in years.\nloan_intent: the purpose of the loan request.\nloan_grade: a composite measure of the likelihood of the borrower to repay the loan. My system will give its own score to each borrower.\nloan_amnt: the amount of the loan.\nloan_int_rate, the annual interest rate on the loan in percent. This is the target variable.\nloan_status: whether (1) or not (0) the borrower defaulted on the loan.\nloan_percent_income: the amount of the loan as a proportion of the prospective borrower’s personal income. This is caluculated as loan_amnt/person_income.\ncb_person_default_on_file: whether the prospective borrower has previously defaulted on a loan in the records of a credit bureau.\ncb_person_cred_hist_length: the length of credit history of the prospective borrower, in years.\n\nIn addition to the columns given by the data set, I will add another column:\n\nint_percent_income: the amount of yearly interest as a proportion of the borrower’s income. Calculated as int_percent_income = loan_percent_income * loan_int_rate.\n\nI will also be changing loan_int_rate from a percentage to a fraction before calculating int_percent_income.\n\n1def prepare_data(df):\n2    df_ = df.copy()\n3    df_ = df_.dropna()\n4    df_['loan_int_rate'] = df_['loan_int_rate'] / 100\n5    df_['int_percent_income'] = df_['loan_percent_income'] * df_['loan_int_rate']\n6    df_ = df_[(df_['person_age'] &lt;= 100) & (df_['person_emp_length'] &lt;= 100)]\n    return df_\n\ndf_train = prepare_data(df_train)\ndf_train.head()\n\n\n1\n\nFunction to prepare data.\n\n2\n\nCopy dataframe\n\n3\n\nDrop all NaNs\n\n4\n\nConverts loan_int_rate to fraction from percent.\n\n5\n\nCalculates int_percent_income and add it as a new column.\n\n6\n\nDrops all people over the age of 100 and have employment lengths over 100 years.\n\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nint_percent_income\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n0.016164\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n0.020277\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n0.006435\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n0.026964\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n0.1491\n1\n0.25\nN\n2\n0.037275"
  },
  {
    "objectID": "posts/loans_post/loans.html#exploring-the-data",
    "href": "posts/loans_post/loans.html#exploring-the-data",
    "title": "Bank Loans",
    "section": "3 Exploring the Data",
    "text": "3 Exploring the Data\nLet’s first take a look at how the loan interest rate and the yearly interest amount as a proportion of income (loan percent of income) relates to whether or not a borrower will default. The reason this may be a good place to start is because it is intuitive to think that a higher interest rate could result in more defaults. Additionally, it is also intuitive to think that the larger the loan amount is compared to a person’s income, the chance of defaulting increases. So, we can put these ideas together and conclude that interest rates and the loan percent of income of income could be closely related to the chance of defaulting.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npltt = 'Paired'\n\nint_income_plt = sns.scatterplot(df_train,\n                                 x = 'loan_percent_income',\n                                 y = 'loan_int_rate',\n                                 hue = 'loan_status',\n                                 palette = pltt,\n                                 alpha = 0.5)\n\nint_income_plt.set(xlabel = 'Loan Percent of Income',\n                   ylabel = 'Loan Interest Rate',\n                   title = 'Interest Cost vs. Income')\n\nint_income_plt.legend(title = 'Default');\n\n\n\n\n\n\n\n\nFigure 1: Defaults by interest percent of income.\n\n\n\n\n\nAs we can see, most defaults occur along the outer ring (dark blue dots). This confirms our intuition. As interest rate increases, more defaults occur and the same can be seen with the loan percent of income. Additionally, this figure (Figure 1) provides a good visualization of the interest percent of income (int_percent_income). With that said, loan_int_rate, loan_percent_income, and int_percent_income seem like good features to use.\nNext, let’s try to find some features that agree with this pattern. First, let’s take a look at loan_intent.\n\n\nCode\n1df_train.groupby([\"loan_intent\"]).agg({'loan_status':'mean',\n                                       'loan_int_rate': 'mean',\n                                       'loan_percent_income':'mean',\n                                       'int_percent_income':'mean'})\n\n\n\n1\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by loan_intent.\n\n\n\n\n\n\nTable 1: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by loan intent.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.282983\n0.110173\n0.170322\n0.019230\n\n\nEDUCATION\n0.171012\n0.109952\n0.168219\n0.018827\n\n\nHOMEIMPROVEMENT\n0.258327\n0.111819\n0.164516\n0.018924\n\n\nMEDICAL\n0.264455\n0.110711\n0.172422\n0.019511\n\n\nPERSONAL\n0.191385\n0.110282\n0.167568\n0.018898\n\n\nVENTURE\n0.146221\n0.109690\n0.169147\n0.018907\n\n\n\n\n\n\n\n\n\n\nFrom the table, we can see that the loan interest rate, loan percent income, and interest percent income are fairly similar between all categories of loan intent. However, there is a remarkable difference in default rates. DEBTCONSOLODATION, HOMEIMPROVEMENT, and MEDICAL all have much higher default rates than the other three categories. This doesn’t align well with our findings from Figure 1 since the default rates don’t align with loan_int_rate, loan_percent_income, and int_percent_income. As such, loan_intent may not be a good predictor.\nNext, we will look at person_home_ownership.\n\n\nCode\n1df_train.groupby([\"person_home_ownership\"]).agg({'loan_status':'mean',\n                                                 'loan_int_rate': 'mean',\n                                                 'loan_percent_income':'mean',\n                                                 'int_percent_income':'mean'})\n\n\n\n1\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by person_home_ownership.\n\n\n\n\n\n\nTable 2: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by home ownership status.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nperson_home_ownership\n\n\n\n\n\n\n\n\nMORTGAGE\n0.124496\n0.105270\n0.151172\n0.016350\n\n\nOTHER\n0.272727\n0.120592\n0.189870\n0.024013\n\n\nOWN\n0.071429\n0.109466\n0.184076\n0.020363\n\n\nRENT\n0.309761\n0.114524\n0.180943\n0.021013\n\n\n\n\n\n\n\n\n\n\nUnlike loan_intent (Table 1), person_home_ownership matches our findings from Figure 1. The default rates mostly lineup with loan_int_rate, loan_percent_income, and int_percent_income except for people who OWN a home, who have a relatively high loan_percent_income and int_percent_income, but have the lowest default rates. It is important to note however, that people who OWN a home have the second lowest loan_int_rate.\nAdditionally, as we can see below (Figure 2), RENT and OTHER have a relatively high amount of borrowers who are using the loans for DEBTCONSOLIDATION and MEDICAL. This matches up with the findings from Table 1. It seems that person_home_ownership is also a decent proxy for loan_intent while also matching the findings of Figure 1.\nAs such, we can expect person_home_ownership to possibly be a good predictor.\n\n\nCode\n1home_intent = ((df_train.groupby(['person_home_ownership', 'loan_intent']).size()\n                / df_train.groupby(['person_home_ownership']).size())\n2                .rename('proportion')\n3                .reset_index())\n\n4home_intent_plt = sns.barplot(home_intent,\n            x = 'person_home_ownership',\n            y = 'proportion',\n            hue = \"loan_intent\",\n            palette = pltt)\n\nhome_intent_plt.set(xlabel = \"Home Ownership Status\",\n                   ylabel = \"Proportion\", \n                   title = \"Loan Intent by Home Ownership Status\")\n\nhome_intent_plt.legend(title = \"Loan Intent\",\n                       fontsize = '8');\n\n\n\n1\n\nProportion of each loan_intent category for each person_home_ownership category.\n\n2\n\nRenames (1) to proportion.\n\n3\n\nResets index.\n\n4\n\nPlots loan intent by home ownership status.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Loan intent by home ownership status.\n\n\n\n\n\nLastly, let’s look at how age relates to loan_int_rate, loan_percent_income, int_percent_income, and default rates. To do this, I created age ranges of 10 years (20’s, 30’s, 40’s, etc.).\n\n\nCode\nimport numpy as np\n\n1age_labels = [\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80+\"]\n2age_bins = np.concatenate((np.arange(19, 80, 10), [150]))\n\n3def get_age_groups(df):\n    df[\"age_group\"] = pd.cut(x = df['person_age'], bins = age_bins, labels = age_labels)\n    return df\n\n4df_train = get_age_groups(df_train)\n\n5df_train.groupby([\"age_group\"]).agg({'loan_status':'mean',\n                                     'loan_int_rate': 'mean',\n                                     'loan_percent_income':'mean',\n                                     'int_percent_income':'mean'})\n\n\n\n1\n\nList of age group labels.\n\n2\n\nSet’s bins to [19, 29, 39, …, 79, 150].\n\n3\n\nFunction that creates a new column called age_group and groups ages into 10 year intervals.\n\n4\n\nAdds age_group to df_train.\n\n5\n\nGet mean of loan_status, loan_int_rate, loan_percent_income, and int_percent_income grouped by age_group.\n\n\n\n\n\n\nTable 3: Loan interest rate, loan percent of income, interest percent of income, and proportion of defaults by age group.\n\n\n\n\n\n\n\n\n\n\nloan_status\nloan_int_rate\nloan_percent_income\nint_percent_income\n\n\nage_group\n\n\n\n\n\n\n\n\n20-29\n0.220637\n0.110169\n0.171090\n0.019270\n\n\n30-39\n0.200233\n0.110843\n0.164211\n0.018574\n\n\n40-49\n0.193648\n0.110663\n0.160727\n0.018248\n\n\n50-59\n0.221622\n0.111235\n0.149243\n0.016920\n\n\n60-69\n0.318182\n0.109743\n0.202955\n0.022861\n\n\n70-79\n0.142857\n0.107500\n0.115714\n0.011406\n\n\n80+\n0.000000\n0.096100\n0.110000\n0.010571\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n1age_intent = ((df_train.groupby(['age_group', 'loan_intent']).size()\n               / df_train.groupby(['age_group']).size())\n2               .rename('proportion')\n3               .reset_index())\n\n4fig, ax = plt.subplots(1, 2, figsize = (15, 6))\n\n5age_intent_plt = sns.barplot(age_intent,\n                             x = 'age_group',\n                             y = 'proportion',\n                             hue = 'loan_intent',\n                             palette = pltt,\n                             ax = ax[0])\n\nage_intent_plt.set(xlabel = \"Age Group\",\n                ylabel = \"Proportion\", \n                title = \"Loan Intent by Age Group\")\n\nage_intent_plt.legend(title = \"Loan Intent\")\n\n6age_home = ((df_train.groupby(['age_group', 'person_home_ownership']).size()\n             / df_train.groupby(['age_group']).size())\n7             .rename('proportion')\n8             .reset_index())\n\n9age_home_plt = sns.barplot(age_home,\n                             x = 'age_group',\n                             y = 'proportion',\n                             hue = 'person_home_ownership',\n                             palette = pltt,\n                             ax = ax[1])\n\nage_home_plt.set(xlabel = \"Age Group\",\n                ylabel = \"Proportion\", \n                title = \"Home Ownership Status by Age Group\")\n\nage_home_plt.legend(title = \"Home Ownership Status\");\n\n\n\n1\n\nProportion of each loan_intent category for each age_group category.\n\n2\n\nRenames (1) to proportion.\n\n3\n\nResets index.\n\n4\n\nCreate 2 subplots.\n\n5\n\nPlots loan intent by age group as first subplot.\n\n6\n\nProportion of each person_home_ownership category for each age_group category.\n\n7\n\nRenames (6) to proportion.\n\n8\n\nResets index.\n\n9\n\nPlots home ownership status by age group as second subplot.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Loan intent and homeownership status by age group.\n\n\n\n\n\nFrom Table 3, we can see that age_group follows the trend found in Figure 1 with the exception for 50-59 year olds who have a higher default rate despite having a relatively low loan_percent_income and int_percent_income. However, they do have the highest loan_int_rate out of all the age groups which may be a reason as to why their default rates are so high.\nThen, comparing age_group with loan_intent (Figure 3), we can see that as borrowers get older, they spend more on MEDICAL. Yet despite spending the most on MEDICAL, 70-79 year olds have the lower default rates by far which contradicts our findings from Table 1. Additionally, there is no clear relationship between age_group and person_home_ownership.\nTherefore, we can most likely conclude that age_group is not a good predictor."
  },
  {
    "objectID": "posts/loans_post/loans.html#finding-a-threshold",
    "href": "posts/loans_post/loans.html#finding-a-threshold",
    "title": "Bank Loans",
    "section": "4 Finding a Threshold",
    "text": "4 Finding a Threshold\nUsing our findings, I will attempt to find an optimal threshold which will maximize expected profit from loans for the bank. I will do this by finding the best features through a semi-exhaustive search, then using those features, for logistic regression. I will then use the coefficients from the trained model as my weight vector, w, to compute the score, s, for each borrower (Equation 1).\nLastly, I will test multiple thresholds to find the optimal threshold that maximizes expected profit per loan for the bank. The base profit to beat,assuming that the bank gives everyone loans, is $819.12.\n\n1gain = ((df_train[df_train['loan_status'] == 0]['loan_amnt']\n         *(1 + 0.25*df_train['loan_int_rate'])**10\n         - df_train[df_train['loan_status'] == 0]['loan_amnt'])\n         .sum())\n2cost = ((df_train[df_train['loan_status'] == 1]['loan_amnt']\n         *(1 + 0.25*df_train['loan_int_rate'])**3\n         - 1.7*df_train[df_train['loan_status'] == 1]['loan_amnt'])\n         .sum())\n3base_profit = (gain+cost)/df_train.shape[0]\n4base_profit\n\n\n1\n\nCalculates gain from giving out loans to everyone.\n\n2\n\nCalcultaes cost from giving out loans to everyone.\n\n3\n\nCalculates total base profit.\n\n4\n\nOutputs total base profit\n\n\n\n\n819.1210295903265\n\n\nBefore I start however, I need to prepare the data for training. I will drop our target variable loan_status and the bank’s “score” for each borrower, loan_grade, since I will generate my own scores for each borrower. We also drop age_group since we can simply use person_age instead, and one-hot encode all the qualitative columns.\n\n\nCode\n1y_train = df_train['loan_status']\n2X_train = df_train.drop(['loan_status', 'loan_grade', 'age_group'], axis = 1)\n\n3qual_cols = list(X_train.select_dtypes(exclude=['number']).columns)\nX_train = pd.get_dummies(X_train,\n4                          columns = qual_cols)\n\nX_train.head()\n\n\n\n1\n\nSet loan_status as my target variable \\(y\\).\n\n2\n\nDrops loan_status, loan_grade, and age_group and set the resulting dataframe to be my features \\(x\\).\n\n3\n\nGet all qualitative columns.\n\n4\n\nOne-hot encodes the qualitative columns.\n\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nint_percent_income\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\n0.016164\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\n0.020277\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n0.1287\n0.05\n4\n0.006435\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n0.0963\n0.28\n10\n0.026964\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n0.1491\n0.25\n2\n0.037275\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nNow that the data is ready to be trained, I will first use my findings from @fig-int-percent-income as a starting point. My initial features will be loan_int_rate, loan_percent_income, and int_percent_income.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n1want_cols = ['loan_int_rate', 'loan_percent_income', 'int_percent_income']\n\n2LR = LogisticRegression()\n3cross_val_score(LR, X_train[want_cols], y_train, cv = 5).mean()\n\n\n1\n\nSets a list cols with my intial column names.\n\n2\n\nInitialize logistic regression instance.\n\n3\n\nOutputs the mean of cross-validation using five folds.\n\n\n\n\n0.8256407088706057\n\n\nA score of \\(\\approx .83\\) is a good start. Now I will try to find other features that may improve the score. I do this by making a power set from the rest of the columns and testing each with loan_int_rate, loan_percent_income, and int_percent_income using five-fold cross validation.\n\nfrom itertools import chain, combinations\n\n1def power_set(iterable):\n    '''power_set([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)'''\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\n2all_qual_cols = (list({col.rsplit('_', 1)[0]\n                       for col in X_train.select_dtypes(exclude=['number']).columns}))\n3all_quant_cols = list(X_train.select_dtypes(exclude=[\"bool_\",\"object_\"]).columns)\n4excess_cols = all_qual_cols + all_quant_cols[0:4] + all_quant_cols[6:8]\n\n\n5def power_set_cols(df, cols):\n    power_cols = list(list(s) for s in power_set(cols))[1:]\n\n    for i in range(0, len(power_cols)):\n        all_cols = []\n        for col in power_cols[i]:\n            all_cols = all_cols + [x_col for x_col in df.columns if col in x_col]\n            power_cols[i] = all_cols\n    return power_cols\n\n6power_cols = [list(p) for p in power_set_cols(X_train, excess_cols)]\n7all_cols = [want_cols] + [want_cols + p for p in power_cols]\n\n\n1\n\nFunction for creating a power set of a list.\n\n2\n\nGets all qualitative columns without one-hot encoded suffixes (ex. person_home_ownership_MORTGAGE \\(\\to\\) person_home_ownership)\n\n3\n\nGets all quantitative columns.\n\n4\n\nCombines (2) and (3) together.\n\n5\n\nFunction that creates a power set of (4) then adds all one-hot encoding suffixes back for each set in power set (ex. person_home_owndership_MORTGAGE, person_home_ownership_RENT, etc. for person_home_ownership).\n\n6\n\nConverts all sets in power set to lists.\n\n7\n\nPrepends want_cols to front of every list in power set.\n\n\n\n\nHere’s what one such combination looks like.\n\nall_cols[123]\n\n['loan_int_rate',\n 'loan_percent_income',\n 'int_percent_income',\n 'person_income',\n 'loan_amnt',\n 'cb_person_cred_hist_length']\n\n\nUsing the list of column combinations, I compare the mean of the cross-validation scores for each one and store the columns that have the best score.\n\n1def cross_val(model, cv):\n  best_score = ([], 0)\n  for col in all_cols:\n    cv_scores = cross_val_score(model, X_train[col], y_train, cv = cv)\n    col_scores = (col, cv_scores.mean())\n    if col_scores[1] &gt; best_score[1]:\n      best_score = col_scores\n  return best_score\n\nbest_score = cross_val(LR, 5)\nbest_score\n\n\n1\n\nUses cross-validation to find the best scores and returns ([best columns], best score).\n\n\n\n\n(['loan_int_rate',\n  'loan_percent_income',\n  'int_percent_income',\n  'person_home_ownership_MORTGAGE',\n  'person_home_ownership_OTHER',\n  'person_home_ownership_OWN',\n  'person_home_ownership_RENT'],\n 0.8474301677042732)\n\n\nWe can see that the best columns confirm the findings from exploring the data. With these columns, I can now train my model using logistic regression.\n\ncols = best_score[0]\n\nLR.fit(X_train[cols], y_train)\nscore = LR.score(X_train[cols], y_train)\n\nscore\n\n0.8469499148508799\n\n\nBelow are the coefficients which I will use as the weights for my score function.\n\nw = LR.coef_[0]\nw\n\narray([20.43778674,  8.01023218,  2.22540065, -0.0730397 ,  0.36076484,\n       -1.18065954,  0.8942643 ])\n\n\nMy score(Equation 1) function:\n\ndef linear_score(X, w):\n    return X@w\n\nWith my score function, I can now give each borrower a score and test various thresholds by profit per loan (Figure 4).\n\nnum_thresholds = 101\n1profit = np.zeros(num_thresholds)\n\n2s = linear_score(X_train[cols], w)\n3T = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\nfor i in range(num_thresholds):\n4    y_pred = s &gt;= T[i]\n5    TN = X_train[((y_pred == 0) & (y_train == 0))]\n6    FN = X_train[((y_pred == 0) & (y_train == 1))]\n7    gain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\n8    cost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\n9    total_loans = TN.shape[0] + FN.shape[0]\n10    if total_loans == 0:\n        profit[i] = 0\n    else:\n        profit[i] = (gain + cost) / total_loans\n\n11profit_plt = sns.lineplot(x = T, y = profit)\nprofit_plt.grid()\nprofit_plt.set(xlabel = r\"Threshold $t$\", ylabel = \"Expected Profit Per Loan\");\n\n\n1\n\nInitialize profit vector with 0s.\n\n2\n\nVector of scores for each borrower.\n\n3\n\nVector of 100 thresholds to test from \\([\\max(s)-1, \\max(s)+1]\\).\n\n4\n\nVector of predictions for score \\(\\geq\\) threshold (defaults).\n\n5\n\nTrue negatives.\n\n6\n\nFalse negatives.\n\n7\n\nSee Equation 2.\n\n8\n\nSee Equation 2.\n\n9\n\nTotal number of loans. (Cost is negative.)\n\n10\n\nSets profit[i] to 0 if no loans are made, otherwise to expected profit per loan.\n\n11\n\nPlots expected profit per loan for every threshold \\(t\\).\n\n\n\n\n\n\n\n\n\n\nFigure 4: Expected profit per loan for threshold \\(t\\).\n\n\n\n\n\n\nmax_profit_index = np.argmax(profit)\nprofit\nt = T[max_profit_index]\np = max(profit)\nt, p\n\n(4.721345771117955, 1757.8292670626274)\n\n\nIt looks like the best threshold is approximately 4.72 and the max expected profit per loan is approximately $1757.83. With weights, w, and threshold t, I am now ready to test the model."
  },
  {
    "objectID": "posts/loans_post/loans.html#evaluating-the-model-from-the-banks-perspective",
    "href": "posts/loans_post/loans.html#evaluating-the-model-from-the-banks-perspective",
    "title": "Bank Loans",
    "section": "5 Evaluating the Model from the Bank’s Perspective",
    "text": "5 Evaluating the Model from the Bank’s Perspective\nFirst, we need to prepare our testing data. I use the same method as my training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = prepare_data(df_test)\ny_test = df_test['loan_status']\nX_test = df_test.drop(['loan_status', 'loan_grade'], axis = 1)\nX_test = pd.get_dummies(df_test,\n                        columns = qual_cols)\n\nI use my weight vector, w, and my threshold t to compute the expected profit per loan for the bank using the testing data.\n\nnum_thresholds = 101\n\ns = linear_score(X_test[cols], w)\n\n# t = 4.721345771117955\ny_pred = s &gt;= t\nTN = X_test[((y_pred == 0) & (y_test == 0))]\nFN = X_test[((y_pred == 0) & (y_test == 1))]\ngain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\ncost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\ntotal_loans = TN.shape[0]+FN.shape[0]\nprofit = (gain + cost) / total_loans\n\nprofit\n\n1714.514369055408\n\n\nUsing my model, the bank can expect to make about $1714.51 per loan, slightly less than the training set."
  },
  {
    "objectID": "posts/loans_post/loans.html#evaluating-the-model-from-the-borrowers-perspective",
    "href": "posts/loans_post/loans.html#evaluating-the-model-from-the-borrowers-perspective",
    "title": "Bank Loans",
    "section": "6 Evaluating the Model from the Borrower’s Perspective",
    "text": "6 Evaluating the Model from the Borrower’s Perspective\nAn important aspect of good machine learning models is fairness and minimal bias. I will be exploring the impact of my autonomous decision-making system on different segments of the population of borrowers by age_group, loan_intent, and income_level.\n\n6.1 Loans by Age\n\n\nCode\nage_labels = [\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\"]\nage_bins = np.concatenate((np.arange(19, 70, 10), [150]))\ndf_test = get_age_groups(df_test)\n\ndf_test['y_pred'] = y_pred\n\nage_loans = (1-(df_test.groupby(['age_group','y_pred']).size() / df_test.groupby(['age_group']).size())).rename('proportion').reset_index()\nage_loans['y_pred'] = age_loans['y_pred'].astype('string')\n\nage_loans_plt = sns.barplot(age_loans,\n            x = 'age_group',\n            y = 'proportion',\n            hue = 'y_pred',\n            palette = pltt)\n\nage_loans_plt.set(xlabel = 'Age Group',\n                  ylabel = 'Proportion',\n                  title = 'Expected Loans by Age Group')\n\nage_loans_plt.legend(title = 'Loans');\n\n\n\n\n\n\n\n\nFigure 5: Expected loans by age group.\n\n\n\n\n\nFrom Figure 5, we can see that people older than 60 will have a much harder time getting a loan with only about 40% being approved of taking a loan.\n\n\n6.2 Loan Intent\n\n\nCode\nintent_loans = (1-df_test.groupby(['loan_intent']).agg({'loan_status':'mean', 'y_pred':'mean'})).reset_index().rename({'loan_status':'actual', 'y_pred':'predicted'}, axis = 1)\nintent_loans['difference'] = intent_loans['predicted'] - intent_loans['actual']\nintent_loans\n\n\n\n\nTable 4: Difference in actual and predicted loan rates by loan intent.\n\n\n\n\n\n\n\n\n\n\nloan_intent\nactual\npredicted\ndifference\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.712389\n0.676991\n-0.035398\n\n\n1\nEDUCATION\n0.832483\n0.693878\n-0.138605\n\n\n2\nHOMEIMPROVEMENT\n0.750000\n0.756494\n0.006494\n\n\n3\nMEDICAL\n0.715750\n0.672880\n-0.042870\n\n\n4\nPERSONAL\n0.779559\n0.688377\n-0.091182\n\n\n5\nVENTURE\n0.853734\n0.716805\n-0.136929\n\n\n\n\n\n\n\n\n\n\nFrom Table 4, we can see HOMEIMPROVEMENT was the only loan_intent to receive an increase in loans. All other intents received fewer loans from my model. The two biggest decreases in loans were for EDUCATION and VENTURE.\nAn important thing to note is the relatively small decrease in loans for MEDICAL despite the relatively large amount of defaults (Table 1). This is important because it would be unfair if those seeking medical attention could not recieve loans. I use unfair in the sense that bias against certain circumstances that are out of ones control is unfair. In the case of medical care, many illnesses or injuries are “random” and not entirely in ones control. Sometimes, unfortunate things and mistakes just happen. To be biased against those unable to pay for medical care themselves is unfair. For example, if a person develops cancer, is it fair if they are denied a loan simply because they have cancer and are more likely to default? No it is not. Everyone deserves a chance for medical care and banks should support the people in need of a loan for medical care with lower interest rates which could in turn lower default rates. As such, my model does a good job of being relatively unbiased towards MEDICAL loans.\n\n\n6.3 Income Levels\n\n\nCode\nincome_bins = np.concatenate((np.arange(-1, 100000, 10000), [2000000]))\nincome_labels = []\n\nfor i in range(len(income_bins)-2):\n    income_labels = income_labels + [f'{income_bins[i]+1}-{income_bins[i+1]}']\n\nincome_labels = income_labels + ['100000+']\n\n\ndef get_income_class(df):\n    df_ = df.copy()\n    df_[\"income_level\"] = pd.cut(x = df_['person_income'], bins = income_bins, labels = income_labels)\n    return df_\n\ndf_test = get_income_class(df_test)\n\nincome_level_loans = 1-(df_test.groupby(['income_level'])['y_pred'].sum() / df_test.groupby(['income_level']).size())\n\nloan_income_plt = sns.lineplot(income_level_loans)\nloan_income_plt.set_xticklabels(loan_income_plt.get_xticklabels(), rotation=40, ha=\"right\")\nloan_income_plt.set(xlabel = 'Income',\n                    ylabel = 'Proportion',\n                    title = 'Proportion Given Loans by Income');\n\n\n\n\n\n\n\n\nFigure 6: Expected loans by income level.\n\n\n\n\n\nFrom Figure 6, we can see that as income level increases, more prospective borrowers are approved for loans. This makes sense, since higher income individuals are more likely to pay back loans. However, it is a bit worrying that lower income individuals will have a hard time getting loans, since they may be the ones that need it the most."
  },
  {
    "objectID": "posts/loans_post/loans.html#summary",
    "href": "posts/loans_post/loans.html#summary",
    "title": "Bank Loans",
    "section": "7 Summary",
    "text": "7 Summary\nIn this study, I built an automated decision-making system for loans. I optimized the predicted expected profit per loan for the bank by giving each prospective borrower a score(Equation 1) and then choosing a threshold to determine the borrowers that would receive a loan. Afterwards, I analyzed my model for biases by age group, loan intents, and income level.\nBy using the features loan_int_rate, loan_percent_income, int_percent_income, and person_home_ownership, the model produced an expected ouput of about $1714.51. This surpasses the base profit by about $895.39, an approximately 209% increase in profit.\nFurthermore, analysis of my model showed that it approves fewer loans to prospective borrowers over the age of 60, borrowers who intend to take a loan for education and venture, and borrowers with lower income. I also specifically analyzed the impact my model had on borrowers who are intending to take a loan out for medical reasons and found that there was a slight, but not a significant drop in loans approved. Lastly, I discussed why my model would’ve been unfair for borrowers looking to receive a medical loan if my model had a bias against them."
  },
  {
    "objectID": "posts/logistic_post/logistic_post.html",
    "href": "posts/logistic_post/logistic_post.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Image source: https://images.spiceworks.com/wp-content/uploads/2022/04/11040522/46-4.png\nReferenced Code: logistic.py"
  },
  {
    "objectID": "posts/logistic_post/logistic_post.html#abstract",
    "href": "posts/logistic_post/logistic_post.html#abstract",
    "title": "Logistic Regression",
    "section": "1 Abstract",
    "text": "1 Abstract\nLogistic regression aims to minimize the empirical risk for a given data set by adjusting the weight vector \\(\\mathbf{w}\\) using gradient descent and momentum. In this study, I implement and experiment with logistic regression for binary classification. The first experiment uses logistic regression with vanilla gradient descent (\\(\\beta = 0\\)) and analyzes the loss over iterations and the final decision boundary. The second experiment applies logistic regression with momentum (\\(\\beta &gt; 0\\)) and compares its loss over iterations and final decision boundary with its vanilla counterpart. The last experiment analyzes overfitting when the number of features exceeds the number of data points in the data set. For each experiment, I will first generate classification data then run a training loop. For the first two experiments, I visualize the loss over iterations and the final decision boundaries, while for the last experiment, I compare the accuracy of the model for the training data to the testing data.\nThrough these experiments, I found that logistic regression with vanilla gradient descent does converge to a correct weight vector \\(\\mathbf{w}\\), while logistic regression with momentum converges faster to the correct weight vector \\(\\mathbf{w}\\) and finds a better decision boundary given the same number of iterations as vanilla logistic regression. Furthermore, when the number of features exceeds the number of data points, the model can achieve 100% training accuracy, but will do worse on testing data due to overfitting."
  },
  {
    "objectID": "posts/logistic_post/logistic_post.html#implementing-logistic-regression",
    "href": "posts/logistic_post/logistic_post.html#implementing-logistic-regression",
    "title": "Logistic Regression",
    "section": "2 Implementing Logistic Regression",
    "text": "2 Implementing Logistic Regression\nLogistic regression performs classification by minimizing the empirical risk \\[L(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n [-y_i\\log(\\sigma(s_i))-(1-y_i)\\log(1-\\sigma(s_i))]\\] where \\(s_i = \\langle w, x_i\\rangle\\) and \\(\\sigma(s_i) = \\frac{1}{1+e^{-s_i}}\\).\nTo do this, at each iteration \\(k\\), logistic regression performs the following update: \\[\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha\\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\\] where \\[\\nabla L(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n (\\sigma(s_i)-y_i)\\mathbf{x}_i\\] is the gradient of the empirical risk, \\(\\alpha\\) and \\(\\beta\\) are learning rates, and \\(\\mathbf{w}_k - \\mathbf{w}_{k-1}\\) is the momentum term.\nIn my implementation of logistic regression, loss() computes \\(L(\\mathbf{w})\\), grad() computes \\(\\nabla L(\\mathbf{w})\\), and step() computes \\(w_{k+1}\\) as seen below and in perceptron.py.\ndef loss(self, X, y):\n    sigma = 1/(1+torch.exp(-self.score(X)))\n    return (-y*torch.log(sigma) - (1-y)*torch.log(1-sigma)).mean()\n\ndef grad(self, X, y):\n    sigma = 1/(1+torch.exp(-self.score(X)))\n    return ((sigma - y)[:, None]*X).mean()\n\ndef step(self, X, y, alpha = 0.01, beta = 0):\n    old_w = torch.clone(self.model.w)\n    self.model.w += -alpha*self.model.grad(X, y) + beta*(self.model.w - self.model.w_)\n    self.model.w_ = old_w"
  },
  {
    "objectID": "posts/logistic_post/logistic_post.html#experiments",
    "href": "posts/logistic_post/logistic_post.html#experiments",
    "title": "Logistic Regression",
    "section": "3 Experiments",
    "text": "3 Experiments\nIn this section, I will be running the following three experiments using my logistic regression algorithm:\n\nVanilla gradient descent: When \\(p_{dim} = 2\\), when \\(\\alpha\\) is sufficiently small and \\(\\beta = 0\\), gradient descent for logistic regression converges to a weight vector \\(\\mathbf{w}\\) that looks visually correct. Furthermore, the loss should decrease monotonically. To demonstrate this, I will visualize the loss over iterations and the final decision boundary.\nBenefits of momentum: On the same data, gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) can converge to the correct \\(\\mathbf{w}\\) in fewer iterations than vanilla gradient descent (with \\(\\beta = 0\\)). To demonstrate this, I will visualize the loss over iterations and the final decision boundaries for both.\nOverfitting: I will generate two sets of data where p_dim &gt; n_points. The first set of data will be the training data and the second set will be the test data. I will train a logistic regression model on the first set of data with 100% accuracy, then analyze the accuracy on the test data.\n\nBefore, I run these experiments, I will create functions to generate and plot my data, train the data, and visualize the results.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n1def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n2def plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2.\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n3def draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef train(model, X, y, alpha = 0.01, beta = 0, iters = 100):\n4    opt = GradientDescentOptimizer(model)\n\n5    torch.manual_seed(1)\n    model.loss(X, y)\n\n6    loss_vec = []\n\n    for _ in range(iters):\n\n7        loss = model.loss(X, y)\n        loss_vec.append(loss)\n\n8        opt.step(X, y, alpha = alpha, beta = beta)\n\n    return loss_vec\n\n\n1\n\nCreates data for classification.\n\n2\n\nPlots data.\n\n3\n\nDraws decision boundary.\n\n4\n\nInitializes logistic regression.\n\n5\n\nSet seed for reproducible results.\n\n6\n\nInitialize for tracking.\n\n7\n\nTracks progress.\n\n8\n\nUpdates weight vector \\(\\mathbf{w}\\).\n\n\n\n\n\n3.1 Vanilla Gradient Descent\nLet’s take a look at vanilla gradient descent when \\(\\beta = 0\\). First, let’s generate and plot the data.\n\n\nCode\ntorch.manual_seed(1)\nX, y = classification_data(noise = 0.5)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 1: 300 data points with noise = 0.5 in 2 dimensions.\n\n\n\n\n\nBelow are the loss over iterations and the final decision boundary for this data set.\n\nCode\nLR1 = LogisticRegression()\nloss_vec1 = train(LR1, X, y, alpha = 0.02, beta = 0, iters = 10000)\n\nplt.figure(figsize = (4, 4))\nplt.plot(torch.arange(1, len(loss_vec1)+1), loss_vec1, color = \"black\")\nplt.semilogx()\nplt.gca().set(xlabel = \"Number of Gradient Descent Iterations\", ylabel = \"Loss (Binary Cross Entropy)\")\n\nfig2, ax2 = plt.subplots(1, 1, figsize = (4, 4)) \nplot_classification_data(X, y, ax2)\ndraw_line(LR1.w, -0.5, 1.5, ax2, color = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Loss over iterations.\n\n\n\n\n\n\n\n\n\n\n\n(b) Decision boundaries.\n\n\n\n\n\n\n\nFigure 2: Vanilla gradient descent converges to a weight vector \\(\\mathbf{w}\\).\n\n\n\nAs we can see in Figure 2, the vanilla gradient descent converges to some \\(\\mathbf{w}\\) that looks correct. More iterations will result in a more correct \\(\\mathbf{w}\\).\n\n\n3.2 Benefits of Momentum\nNow, we’ll take a look at the benefits of momentum. Gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) should converge to the correct weight vector in fewer iterations than vanilla gradient descent (\\(\\beta = 0\\)). For this experiment, let’s use \\(\\beta = 0.9\\). The loss over iterations and decision boundaries for both are displayed below.\n\n\nCode\nLR2 = LogisticRegression()\nloss_vec2 = train(LR2, X, y, alpha = 0.02, beta = 0.9, iters = 10000)\nplt.figure(figsize = (4, 4))\nplt.plot(torch.arange(1, len(loss_vec1)+1), loss_vec1, color = 'green', label = r'$\\beta$ = 0')\nplt.semilogx()\nplt.gca().set(xlabel = \"Number of Gradient Descent Iterations\", ylabel = \"Loss (Binary Cross Entropy)\")\n\nplt.plot(torch.arange(1, len(loss_vec2)+1), loss_vec2, color = 'blue', label = r'$\\beta$ = 0.9')\nplt.semilogx()\nplt.gca().set(xlabel = \"Number of Gradient Descent Iterations\", ylabel = \"Loss (Binary Cross Entropy)\")\nplt.legend();\n\n# fig, ax = plt.subplots(1, 1, figsize = (4, 4)) \n# plot_classification_data(X, y, ax)\n# draw_line(LR1.w, -0.5, 1.5, ax, color = 'green', linestyle = 'dashed')\n# draw_line(LR2.w, -0.5, 1.5, ax, color = 'blue')\n# plt.gca().get_lines()[0].set_label(r\"$\\beta$ = 0\")\n# plt.gca().get_lines()[1].set_label(r\"$\\beta$ = 0.9\")\n# plt.legend(loc = 'lower right');\n\n\n\n\n\n\n\n\nFigure 3: Gradient descent with momentum converges faster to the correct weight vector \\(\\mathbf{w}\\) than its vanilla counterpart.\n\n\n\n\n\nFrom Figure 3, we can see that gradient descent with momentum converges to the correct weight vector \\(\\mathbf{w}\\) faster than its vanilla counterpart.\n\n\n3.3 Overfitting\nIn this section, we’ll explore overfitting for the logistic regression model. I’ll first create training and test data where n_points &gt; p_dims.\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nNext, I’ll train the model over the training data X_train and y_train.\n\nLR3 = LogisticRegression()\nloss_vec3 = train(LR3, X_train, y_train, alpha = 0.01, beta = 0.9, iters = 1000)\n(1.0*(LR3.predict(X_train) == y_train)).mean()\n\ntensor(1.)\n\n\nWe can see that we have achieved an accuracy of 100%. Let’s see what the accuracy is when predicting the test data X_test and y_test.\n\n(1.0*(LR3.predict(X_test) == y_test)).mean()\n\ntensor(0.8400)\n\n\nThe accuracy was much lower for the test data (about 14%). This is most likely due to overfitting. Given the number of features (p_dims = 100), the model is not able to generalize well to new data due to insufficient data points in the training set. Let’s look at the confusion matrix.\n\n\nCode\nfrom sklearn import metrics\n\nconfusion_matrix = metrics.confusion_matrix(y_test, LR3.predict(X_test))\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n\ncm_display.plot(cmap = 'Blues')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Confusion matrix for testing data.\n\n\n\n\n\nFrom Figure 4, we can see that the model correctly predicted all the positives, but it also produced eight false positives."
  },
  {
    "objectID": "posts/logistic_post/logistic_post.html#discussion",
    "href": "posts/logistic_post/logistic_post.html#discussion",
    "title": "Logistic Regression",
    "section": "4 Discussion",
    "text": "4 Discussion\nFrom these experiments, I found that logistic regression with momentum converges faster to a correct weight vector \\(\\mathbf{w}\\) than its vanilla counterpart and produces a more accurate decision boundary in the same number of iterations. Additionally, when the number of features p_dims exceeds the number of data points n_points, logistic regression is prone to overfitting. It was able to obtain a training accuracy of 100%, but only produced a testing accuracy of 86%. This is most likely due to the lack of data points given the number of features. As a result, the model didn’t have enough information during training to generalize well to unseen data."
  },
  {
    "objectID": "posts/newton_post/newton_post.html",
    "href": "posts/newton_post/newton_post.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "A comparison of Newton’s method (red) and gradient descent (green). Image source: https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Newton_optimization_vs_grad_descent.svg/440px-Newton_optimization_vs_grad_descent.svg.png\nReferenced Code: logistic.py"
  },
  {
    "objectID": "posts/newton_post/newton_post.html#abstract",
    "href": "posts/newton_post/newton_post.html#abstract",
    "title": "Newton’s Method for Logistic Regression",
    "section": "1 Abstract",
    "text": "1 Abstract\nIn this blog post, I implement logistic regression using Newton’s method. I then perform the following experiments:\n\nWhen the learning rate \\(\\alpha\\) is chosen appropriately, Newton’s method converges to the correct choice of \\(\\mathbf{w}\\). I do this by comparing the result to logistic regression using gradient descent.\nUnder at least some circumstances, Newton’s method can converge much faster than standard gradient descent in minimizing empirical risk.\nIf \\(\\alpha\\) is too large, Newton’s method fails to converge.\n\nFollowing these experiments, I also analyze the operations needed to perform logistic regression using Newton’s method."
  },
  {
    "objectID": "posts/newton_post/newton_post.html#newtons-method",
    "href": "posts/newton_post/newton_post.html#newtons-method",
    "title": "Newton’s Method for Logistic Regression",
    "section": "2 Newton’s Method",
    "text": "2 Newton’s Method\nNewton’s method attempts to find the root of a differentiable function \\(f(x)\\). As such, Newton’s method can be applied to the derivative of a twice differentiable function \\(f\\) to find the roots of \\(f'(x)\\), or in other words, it can be used to find the critical points of \\(f\\). In optimization, it aims to find the global minima of a function \\(f\\). It does so by using the quadratic Taylor approximation of \\(f\\) for each iterate \\(x_k\\): \\[\nf(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2\n\\] If the second derivative is positive, the quadratic approximation is a convex function of \\(x\\) and its minimum can be found by setting the derivative to zero. So, \\[\\begin{aligned}\n        0 &= \\frac{d}{dx}\\left(f(x_k)+f'(x_k)(x-x_k)+\\frac{1}{2}f''(x_k)(x-x_k)^2\\right) \\\\\n        &= f'(x_k) + f''(x_k)(x-x_k)\n\\end{aligned}\n\\]\nThis is essentially the tangent line of \\(f'\\) at point \\((x_k, f'(x_k))\\) with root \\(x=x_{k+1}\\). Similarly to regular Newton’s method, \\(x_{k+1}\\) should be nearly equal to the root of \\(f'\\). Then, \\[\n    \\begin{align}\n        f'(x_k)+f''(x_k)(x_{k+1}-x_k) &= 0 \\\\\n        x_{k+1}-x_k &= -\\frac{f'(x_k)}{f''(x_k)} \\\\\n        x_{k+1} &= x_k - \\frac{f'(x_k)}{f''(x_k)}.\n    \\end{align}\n\\tag{1}\\]\nWe can apply Newton’s method for multivariate functions to solve logistic regression. Newton’s method for multivariate functions is similar to Newton’s method for univariate functions. Using Equation Equation 1 as reference, we replace \\(f'(x)\\) with the gradient of \\(f\\) denoted \\(\\nabla f(\\mathbf{x})\\) and we replace \\(f''(x)\\) with the inverse of the Hessian matrix of \\(f\\) denoted \\(H^{-1}(\\mathbf{x})\\).\nThe gradient \\(\\nabla f(\\mathbf{x})\\) is the vector of all the partial derivatives of \\(f(\\mathbf{x})\\), \\[\n    \\nabla f(\\mathbf{x}) =\n    \\begin{bmatrix}\n        \\frac{\\partial f}{\\partial x_1} \\\\\n        \\frac{\\partial f}{\\partial x_2} \\\\\n        \\vdots \\\\\n        \\frac{\\partial f}{\\partial x_n} \\\\\n    \\end{bmatrix}\\;.\n\\]\nThe Hessian \\(H(\\mathbf{x})\\) is the matrix of the second partial derivatives of \\(f(\\mathbf{x})\\), \\[\n    H(\\mathbf{x}) =\n    \\begin{bmatrix}\n        \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n}\\\\\n        \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n}\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        \\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\\\\\n    \\end{bmatrix}\\;.\n\\]\nApplying this to the empirical risk minimization problem: \\[  \\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{aligned}\n        \\hat{\\mathbf{w}} &= \\argmin_{\\mathbf{w}} L(\\mathbf{w})  \\\\\n        &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(s_i, y_i) \\\\\n        &= \\argmin_{\\mathbf{w}} \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle, y_i)\\;.\n    \\end{aligned}, \\] we get \\[\n    \\mathbf{w}_{k+1} = \\mathbf{w}_k - H(\\mathbf{w}_k)^{-1}\\nabla L(\\mathbf{w}_k)\\;.\n\\]\n\n2.1 Implementation in Python\ndef loss(self, X, y):\n    s = self.score(X)\n    sigma = torch.sigmoid(s) #1/(1+torch.exp(-s))\n    return (-y*torch.log(sigma) - (1-y)*torch.log(1 - sigma)).mean()\n\ndef grad(self, X, y):\n    s = self.score(X)\n    sigma = torch.sigmoid(s) #1/(1+torch.exp(-s))\n    sigma_y = sigma - y\n    return (sigma_y[:, None] * X).mean(0)\n\ndef hessian(self, X):\n    s = self.score(X)\n    sigma = torch.sigmoid(s)\n    D = (sigma*(1-sigma)).diag()\n    return torch.t(X)@D@X\n\ndef step(self, X, y, alpha = 0.01):\n    self.model.w -= alpha * torch.linalg.inv(self.model.hessian(X))@self.model.grad(X, y)"
  },
  {
    "objectID": "posts/newton_post/newton_post.html#experiments",
    "href": "posts/newton_post/newton_post.html#experiments",
    "title": "Newton’s Method for Logistic Regression",
    "section": "3 Experiments",
    "text": "3 Experiments\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n\n\nBelow, I define some functions to help visualize my data and findings.\n\nimport torch\nimport matplotlib.pyplot as plt\n\n1def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n2def plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2.\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n3def draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef train_newton(model, X, y, alpha = 0.01, iters = 100):\n4    opt = NewtonOptimizer(model)\n\n5    torch.manual_seed(1)\n    #model.loss(X, y)\n\n6    loss_vec = []\n\n    for _ in range(iters):\n        loss = model.loss(X, y) \n        loss_vec.append(loss)\n        opt.step(X, y, alpha)\n\n    return loss_vec\n\ndef train_grad(model, X, y, alpha = 0.01, beta = 0, iters = 100):\n    opt = GradientDescentOptimizer(model)\n\n    torch.manual_seed(1)\n    #model.loss(X, y)\n\n    loss_vec = []\n\n    for _ in range(iters):\n7        loss = model.loss(X, y)\n        loss_vec.append(loss)\n8        opt.step(X, y, alpha = alpha, beta = beta)\n\n    return loss_vec\n\n\n1\n\nCreates data for classification.\n\n2\n\nPlots data.\n\n3\n\nDraws decision boundary.\n\n4\n\nInitializes logistic regression.\n\n5\n\nSet seed for reproducible results.\n\n6\n\nInitialize for tracking.\n\n7\n\nTracks progress.\n\n8\n\nUpdates weight vector \\(\\mathbf{w}\\).\n\n\n\n\n\n3.1 Newton’s Method Works\nI will show that Newton’s method works given a small enough \\(\\alpha\\) by comparing it to standard gradient descent.\n\n\nCode\ntorch.manual_seed(1)\nX, y = classification_data(n_points=100, noise = 0.1)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 1: 300 data points with noise = 0.5 in 2 dimensions.\n\n\n\n\n\n\n\nCode\nLR1 = LogisticRegression()\nloss_vec_newton = train_newton(LR1, X, y, alpha = 1, iters = 1000)\n\nLR2 = LogisticRegression()\nloss_vec_grad = train_grad(LR2, X, y, alpha = 1, beta = 0, iters = 1000)\n\nplt.plot(torch.arange(1, len(loss_vec_newton)+1), loss_vec_newton, color = \"green\", label=\"Newton's Method\")\nplt.plot(torch.arange(1, len(loss_vec_grad)+1), loss_vec_grad, color='blue', label='Gradient Descent')\nplt.semilogx()\nplt.legend()\nplt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Loss (Binary Cross Entropy)\");\n\n\n\n\n\n\n\n\nFigure 2: Loss over iterations.\n\n\n\n\nVanilla gradient descent converges to a weight vector \\(\\mathbf{w}\\).\n\nFrom Figure 2, we can see that Newton’s method works for a sufficiently small \\(\\alpha\\) and converges to the same minimal empirical risk as standard gradient descent.\n\n\n3.2 Newton’s Method Can Converge Faster\nI’ll show that Newton’s method can converge faster than standard gradient descent under certain circumstances. In this case, I set n_points=10 and noise=0.9.\n\n\nCode\ntorch.manual_seed(1)\nX, y = classification_data(n_points=10, noise = 0.9)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 3: 10 data points with noise = 0.9 in 2 dimensions.\n\n\n\n\n\n\nLR3 = LogisticRegression()\nloss_vec_newton = train_newton(LR3, X, y, alpha = 0.9, iters = 1000)\n\nLR4 = LogisticRegression()\nloss_vec_grad = train_grad(LR4, X, y, alpha = 0.9, beta = 0, iters = 1000)\n\nplt.plot(torch.arange(1, len(loss_vec_newton)+1), loss_vec_newton, color = \"green\", label=\"Newton's Method\")\nplt.plot(torch.arange(1, len(loss_vec_grad)+1), loss_vec_grad, color='blue', label=\"Gradient Descent\")\nplt.semilogx()\nplt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Loss (Binary Cross Entropy)\");\nplt.legend();\n\n\n\n\n\n\n\nFigure 4: Loss over iterations.\n\n\n\n\nVanilla gradient descent converges to a weight vector \\(\\mathbf{w}\\).\n\nFrom Figure 4, we can see that when there are very few data points and a lot of noise, then Newton’s method will converge faster than standard gradient descent.\n\n\n3.3 Newton’s Method Might Not Converge\nIf \\(\\alpha\\) is too large, then Newton’s method might not converge. In this case, I use alpha=20.\n\nLR4 = LogisticRegression()\nloss_vec_newton = train_newton(LR4, X, y, alpha = 20, iters = 45)\n\nplt.figure(figsize = (4, 4))\nplt.plot(torch.arange(1, len(loss_vec_newton)+1), loss_vec_newton, color = \"green\")\nplt.semilogx()\nplt.gca().set(xlabel = \"Number of Iterations\", ylabel = \"Loss (Binary Cross Entropy)\");\n\n\n\n\n\n\n\n\nAs we can see, Newton’s method fails to converge when \\(\\alpha\\) is much greater than the number of data points. The loss begins to drastically increase around 40 iterations. This is because, the algorithm overshoots the minima. Each iteration, the algorithm overshoots the minima more and more due to the large \\(\\alpha\\) causing the loss to grow faster each iteration."
  },
  {
    "objectID": "posts/newton_post/newton_post.html#operation-counting",
    "href": "posts/newton_post/newton_post.html#operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "4 Operation Counting",
    "text": "4 Operation Counting\nWe are given that it costs \\(c\\) computational units to compute loss \\(L\\) and:\n\n\\(\\nabla L = 2c\\)\n\\(H(\\mathbf{w}) = pc\\)\nInvert a \\(p \\times p\\) matrix \\(= k_1p^\\gamma\\)\nMatrix-vector multiplication \\(= k_2p^2\\)\nNewton’s method converges in \\(t_{nm}\\) steps\nGradient descent converges in \\(t_{gd}\\) steps\n\nWe want to find the total computational costs of Newton’s method compared to gradient descent and how much smaller \\(t_{nm}\\) must be compared to \\(t_{gd}\\) for Newton’s method to require fewere computational units to complete? Additioanlly, we want to answer the question: when \\(p\\) becomes very large, is using Newton’s method ever going to pay off?\nWe can do this by first calculating the computational costs of Newton’s method and gradient decent.\nFor Newton’s method, we must calculate:\n\n\\(L = c\\).\n\\(\\nabla L = 2c\\).\n\\(H = pc\\).\n\\(H^{-1} = k_1p^\\gamma\\).\n\\(H^{-1}L = k_2p^2\\).\n\\(t_{nm}\\) steps \\(\\implies t_{nm}(c+2c+pc+k_1p^\\gamma+k_2p^2) = t_{nm}(3c+pc+k_1p^\\gamma+k_2p^2)\\) total computational units.\n\nFor gradient descent, we must calculate:\n\n\\(L = c\\).\n\\(\\nabla L = 2c\\).\n\\(t_{gd}\\) steps \\(\\implies t_{gd}(c+2c) = t_{nm}(3c)\\) total computational units.\n\nThen, if we set them equal to each other we can see if Newton’s method is worth using when \\(p\\) becomes very large.\n\\[\\begin{aligned}\nt_{nm}(3c+pc+k_1p^\\gamma+k_2p^2) &= t_{gd}(3c) \\\\\nt_{nm} &= \\frac{3c}{3c+pc+k_1p^\\gamma+k_2p^2}t_{gd}\n\\end{aligned}\\]\nWe can see that if \\(p\\) increases, \\(t_{nm}\\) becomes a much smaller percentage than \\(t_{gd}\\) which implies that it is most likely the case that Newton’s method is not worth using when \\(p\\) becomes large since it is unlikely that Newton’s method converges that much faster than gradient descent."
  },
  {
    "objectID": "posts/newton_post/newton_post.html#conclusion",
    "href": "posts/newton_post/newton_post.html#conclusion",
    "title": "Newton’s Method for Logistic Regression",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nNewton’s method is an iterative algorithm for rootfinding and can be applied to the derivative of a twice-differentiable function for optimization. As a result, it can be used for solving logistic regression. From the above experiments, we can see that Newton’s method works if \\(\\alpha\\) is small enough, converges faster than gradient descent when n_points is small and noise is large, and doesn’t converge if \\(\\alpha\\) is too large. Additionally, I found that Newton’s method is computationally much more expensive than gradient descent and works best for datasets that have few features. When the number of features \\(p\\) increases, Newton’s method becomes less likely to be worth using than gradient descent."
  },
  {
    "objectID": "posts/penguins_post/penguins.html",
    "href": "posts/penguins_post/penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#abstract",
    "href": "posts/penguins_post/penguins.html#abstract",
    "title": "Palmer Penguins",
    "section": "1 Abstract",
    "text": "1 Abstract\nThis data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins in the Palmer Archipelago, Antarctica. This study attempts to find three features (two quantitative and one qualitative) that will be able to predict the penguins’ species with 100% testing accuracy. To do this, I experiment with scikit-learn's logistic regression, support vector machine, random forest classifier, and decision tree classifier models. In the end, the logistic regression model was able to predict the penguins’ species with 100% testing accuracy.\nThe Palmer Penguins data was originally collected by @gormanEcologicalSexualDimorphism2014 and was nicely packaged and released for use in the data science community by @horstAllisonhorstPalmerpenguinsV02020."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#importing-data",
    "href": "posts/penguins_post/penguins.html#importing-data",
    "title": "Palmer Penguins",
    "section": "2 Importing Data",
    "text": "2 Importing Data\nFirst, let’s import the data from here, simplify the penguins’ species name, and look at the first five rows of data.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain['Species'] = train['Species'].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAfter importing the data, we need to clean our data and prepare our qualitative data. The below function uses one-hot encoding to turn the qualitative data into 0 and 1 columns. In this case, the values will be True and False. Additionally, the function also splits away our \\(y\\) variable which is stored in the variable y_train.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n1  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n2  df = df[df[\"Sex\"] != \".\"]\n3  df = df.dropna()\n4  y = le.transform(df[\"Species\"])\n5  df = df.drop([\"Species\"], axis = 1)\n6  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n1\n\nDrop unwanted columns.\n\n2\n\nRemove . from SEX data.\n\n3\n\ndrop NaNs.\n\n4\n\nSets species as \\(y\\) data.\n\n5\n\nDrops species from \\(x\\) data.\n\n6\n\nOne-hot encoding, 0-1 columns.\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "href": "posts/penguins_post/penguins.html#visualization-of-a-couple-features",
    "title": "Palmer Penguins",
    "section": "3 Visualization of a Couple Features",
    "text": "3 Visualization of a Couple Features\nLet’s visualize some potential features using two scatterplots and a summary table. In the two graphs below, I build up on these notes. In these notes, we discovered that Culmen Length (mm) and Culmen Depth (mm) are strong features to use to classify species type. In this study, I will be adding a qualitative data to help improve the test accuracy of the model.\n\n3.1 Figures\nThe figure on the left uses the Island column as the qualitative feature and the figure on the right uses the Sex column as the qualitative feature.\n\nCode\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfig, ax = plt.subplots(1, 1, figsize = (7, 6))\nfig1, ax1 = plt.subplots(1, 1, figsize = (7, 6))\n\np1 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = train['Island'].dropna(), hue = train['Species'].dropna(), ax = ax)\np2 = sns.scatterplot(X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', style = 'Sex_MALE', hue = train['Species'].dropna(), ax = ax1)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Species by Culmen Length, Culmen Depth, and Island.\n\n\n\n\n\n\n\n\n\n\n\n(b) Species by Culmen Length, Culmen Depth, and Sex.\n\n\n\n\n\n\n\nFigure 1: Compares Species by culmen length, culmen depth, island and sex.\n\n\n\nWe can see that both figures have three distinct clusters of points. This is thanks to using Culmen Length (mm) and Culmen Depth (mm) as features. However, there are a few points that overlap regions. In the left figure, we can see that all Chinstrap and Gentoo penguins reside on Dream Island and Biscoe Island, respectively Meanwhile, Adelie penguins live on three islands, however, Adelie penguins that are near the general Chinstrap (blue) region of the figure reside on Torgersen Island.\nIn the right figure, we can see most Adelie and Gentoo penguins that are located near the Chinstrap (blue) region of the figure are male.\n\n\n3.2 Summary Table\nLet’s take a closer look at the Island feature. The table below describes the number of each species on each island.\n\n\nCode\n(train.groupby(by=['Species', 'Island']).aggregate({'Species':'count'}) / train.groupby(by=['Species']).aggregate({'Species':'count'})).rename(columns={'Species':'Distribution of Species'})\n\n\n\n\nTable 1: Distribution of Species by Island.\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Species\n\n\nSpecies\nIsland\n\n\n\n\n\nAdelie\nBiscoe\n0.275\n\n\nDream\n0.375\n\n\nTorgersen\n0.350\n\n\nChinstrap\nDream\n1.000\n\n\nGentoo\nBiscoe\n1.000\n\n\n\n\n\n\n\n\n\n\nWe can see that Adelie penguins are spread out among all three islands, whereas, Chinstrap and Gentoo penguins reside only on Dream and Biscoe Island respectively. We can then most likely use Culmen Length (mm) and Culmen Depth (mm) to seperate the Adelie penguins from Gentoos and Chinstraps on Dream and Biscoe Island."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#finding-the-best-features",
    "href": "posts/penguins_post/penguins.html#finding-the-best-features",
    "title": "Palmer Penguins",
    "section": "4 Finding the Best Features",
    "text": "4 Finding the Best Features\nWe will try to find the best features for multiple models: logisitc regression, random forest classifer, decision tree classifier, and support vector machine.\nThe function below uses cross-validation to determine the three best features (two qualitative and one qualitative) and the respective score for a given model. We will be using five folds for cross-validation.\n\nfrom itertools import combinations\n\n1all_qual_cols = list({col.split('_')[0] for col in X_train.select_dtypes(exclude=['number']).columns})\n2all_quant_cols = X_train.select_dtypes(exclude=[\"bool_\",\"object_\"]).columns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\n\n# uses cross-validation to find the best scores for each combination of two quantitative columns and one qualitative column\n# returns ([best columns], best score)\n3def cross_val(model, cv, best_score):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for qual in all_qual_cols:\n      qual_cols = [col for col in X_train.columns if qual in col]\n      for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols\n        cv_scores = cross_val_score(model, X_train[cols], y_train, cv = cv)\n        col_scores = (cols, cv_scores.mean())\n        if col_scores[1] &gt; best_score[1]:\n          best_score = col_scores\n    return best_score\n\n\n1\n\nGet pre-one-hot encoded column names for qual data\n\n2\n\nGet quant data.\n\n3\n\nUses cross-validation to find the best scores for each combination of two quantitative columns and one qualitative column. Returns ([best columns], best score).\n\n\n\n\nThe function below creates a plot of decision regions for a given model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n1    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n2      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      \n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"center left\", bbox_to_anchor=(1, .5))\n      \n      plt.tight_layout()\n\n\n1\n\nCreate a grid.\n\n2\n\nUse contour plot to visualize the predictions.\n\n\n\n\n\n4.1 Logistic Regression\nLet’s find the columns with the best score.\n\nLR = LogisticRegression()\n\nbest_score_LR = ([], 0)\nbest_score_LR = cross_val(LR, 5, best_score_LR)\nprint(best_score_LR)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9961538461538462)\n\n\nFor logistic regression, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Island with a mean cross-validation score of about 99.6% accuracy.\n\n\n4.2 Random Forest Classifier\nLet’s find the best columns with the best score.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nbest_score_RFC = ([], 0)\n\nbest_score_RFC = cross_val(RFC, 5, best_score_RFC)\n\nprint(best_score_RFC)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9843891402714933)\n\n\nFor the random forest classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98.8% accuracy.\n\n\n4.3 Decision Tree Classifier\nFor the decision tree classifier, we must also find the optimal max depth. We test integer values from 5 to 50 and choose the best value.\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepth = np.arange(5, 50)\nbest_score_DTC = ([], 0)\nbest_d = 10\n\nfor d in depth:\n    DTC = DecisionTreeClassifier(max_depth = d)\n    #print(d)\n    cross_val_DTC = cross_val(DTC, 5, best_score_DTC)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_DTC[1] &gt; best_score_DTC[1]:\n        best_score_DTC = cross_val_DTC\n        best_d = d\n\nprint(best_score_DTC, best_d)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'], 0.9765460030165913) 5\n\n\nFor the decision tree classifier, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 97.6% accuracy using max_depth = 6.\n\n\n4.4 Support Vector Machine\nFor the support vector machine, we must find the best gamma value. We test values from ranging from \\(10^{-5}\\) to \\(10^5\\).\n\nfrom sklearn.svm import SVC\n\ngamma = 10**np.arange(-5, 5, dtype = float)\nbest_score_SVM = ([], 0)\nbest_g = 0\n\nfor g in gamma:\n    SVM = SVC(gamma = g)\n    #print(d)\n    cross_val_SVM = cross_val(SVM, 5, best_score_SVM)\n    #print(cross_val_DTC, best_score_DTC)\n    if cross_val_SVM[1] &gt; best_score_SVM[1]:\n        best_score_SVM = cross_val_SVM\n        best_g = g\n\nprint(best_score_SVM, best_g)\n\n(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'], 0.9805429864253394) 0.1\n\n\nFor the support vector machine, we can see that the best features are Culmen Length (mm), Culmen Depth (mm), and Sex with a mean cross-validation score of about 98% accuracy using gamma = 0.1."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#training-the-data",
    "href": "posts/penguins_post/penguins.html#training-the-data",
    "title": "Palmer Penguins",
    "section": "5 Training the Data",
    "text": "5 Training the Data\nLet’s train the data using each model.\n\n5.1 Logistic Regression\n\ncols_LR = best_score_LR[0]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    LR.fit(X_train[cols_LR], y_train)\n    score_LR = LR.score(X_train[cols_LR], y_train)\n\nscore_LR\n\n0.99609375\n\n\nSo, using logistic regression on Culmen Length (mm), Culmen Depth (mm), and Island has a training accuracy of &gt;99.6%.\n\n\n5.2 Random Forest Classifier\n\ncols_RFC = best_score_RFC[0]\nRFC.fit(X_train[cols_RFC], y_train)\nRFC.score(X_train[cols_RFC], y_train)\n\n1.0\n\n\nSo, using the random forest classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 100%.\n\n\n5.3 Decision Tree Classifier\n\ncols_DTC = best_score_DTC[0]\nDTC = DecisionTreeClassifier(max_depth = best_d)\nDTC.fit(X_train[cols_DTC], y_train)\nDTC.score(X_train[cols_DTC], y_train)\n\n1.0\n\n\nSo, using the decision tree classifier on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of 98.8%.\n\n\n5.4 Support Vector Machine\n\ncols_SVM = best_score_SVM[0]\nSVM = SVC(gamma = best_g)\nSVM.fit(X_train[cols_SVM], y_train)\nSVM.score(X_train[cols_SVM], y_train)\n\n0.9921875\n\n\nSo, using the support vector machine on Culmen Length (mm), Culmen Depth (mm), and Sex has a training accuracy of &gt;99.2%."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#plotting-training-decision-regions",
    "href": "posts/penguins_post/penguins.html#plotting-training-decision-regions",
    "title": "Palmer Penguins",
    "section": "6 Plotting Training Decision Regions",
    "text": "6 Plotting Training Decision Regions\nLet’s plot the decision regions for each model.\n\nCode\nplot_regions(LR, X_train[cols_LR], y_train)\nplot_regions(RFC, X_train[cols_RFC], y_train)\nplot_regions(DTC, X_train[cols_DTC], y_train)\nplot_regions(SVM, X_train[cols_SVM], y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n(b) Random Forest\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Decision Tree\n\n\n\n\n\n\n\n\n\n\n\n(d) Support Vector Machine\n\n\n\n\n\n\n\nFigure 2: Decision regions for logistic regression, RFC, DTC, and SVM.\n\n\n\nFrom these plots, we can see that the RFC, DTC, and SVM models are potentially overfitting on the training data. As a result, we will use the logistic regression model for the testing data."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#testing",
    "href": "posts/penguins_post/penguins.html#testing",
    "title": "Palmer Penguins",
    "section": "7 Testing",
    "text": "7 Testing\nWith our models trained, we can now test the logistic regression model on the test data pulled from here.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest['Species'] = test['Species'].str.split().str.get(0)\nX_test, y_test = prepare_data(test)\n\n\n7.1 Testing Score\n\nLR.score(X_test[cols_LR], y_test)\n\n1.0\n\n\nLogistic regression has a test score of 100% so we have accomplished our objective!\n\n\n7.2 Decision Regions\nNow, let’s look at the decision regions of the logistic regression model for the test data.\n\n\nCode\nplot_regions(LR, X_test[cols_LR], y_test)\n\n\n\n\n\n\n\n\nFigure 3: Decision regions of the logistic regression model for the test data."
  },
  {
    "objectID": "posts/penguins_post/penguins.html#discussion",
    "href": "posts/penguins_post/penguins.html#discussion",
    "title": "Palmer Penguins",
    "section": "8 Discussion",
    "text": "8 Discussion\nThis study experiemented with scikit-learn's logistic regression, random forest classifier, decision tree classifier, and support vector machine models to predict penguin species from the Palmer Penguins data set with 100% testing accuracy. After training the four models, the random forest and decision tree classifiers had a training accuracy of 100%, while the logistic regression and support vector machine models had a training accuracy of 99.6% an 99.2% respectively. However, when looking at the decision regions in Figure 2, we can see that the random forest and decision tree classifers and the support vector machine were potentially overfitting on the training data. As a result, I used the logistic regression model for the testing data and it successfully predicted the penguins’ species with 100% testing accuracy."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html",
    "href": "posts/perceptron_post/perceptron_post.html",
    "title": "Perceptron",
    "section": "",
    "text": "Image source: https://miro.medium.com/v2/resize:fit:1400/1*b7kNF1-TcrcogZAh2i5l4Q.png\nPerceptron Implementation: perceptron.py"
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#abstract",
    "href": "posts/perceptron_post/perceptron_post.html#abstract",
    "title": "Perceptron",
    "section": "1 Abstract",
    "text": "1 Abstract\nThis post implements the perceptron algorithm and minibatch perceptron algorithm and performs various experiments using both. For the simple perceptron algorithm, I run the following experiments:\n\nVisualize the updates of the perceptron for linearly separable data and the final separating line.\nVisualize the updates of the perceptron for not linearly separable data and the final boundary line in the final iteration since it does not converge.\nVisualize the loss of the perceptron for data with 5 feature and determine if it converges or not.\n\nFor the minibatch perceptron algorithm, I run the following experiments:\n\nVisualize the updates of the perceptron with k=1 and show that it performs similarly to the regular perceptron.\nVisualize the updates of the perceptron with k=10 and show it can still find a separating line in 2d.\nVisualize the updates of the perceptron with k=n and show that it can still converge even if the data is not linearly separable, provided that the learning rate \\(\\alpha\\) is not separable.\n\nAt the end, I discuss the runtime complexity of a single iteration of both the regular perceptron algorithm and the minibatch perceptron algorithm."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#implementing-perceptron",
    "href": "posts/perceptron_post/perceptron_post.html#implementing-perceptron",
    "title": "Perceptron",
    "section": "2 Implementing Perceptron",
    "text": "2 Implementing Perceptron\nThe perceptron algorithm aims to find a good choice of weights \\(\\mathbf{w}\\) that makes the loss small using the following algorithm:\n\nStart with a random \\(\\mathbf{w}^{(0)}\\).\nIn each time-step \\(t\\):\n\nPick a random data point \\(i\\in{1,...,n}\\)\nCompute \\(s^{(t)}_i = \\langle\\mathbf{w}^{(t)}, \\mathbf{x}_i\\rangle\\).\nIf \\(s^{(t)}_i y_i \\geq 0\\), then point \\(i\\) is correctly classified – do nothing!\nElse, if \\(s^{(t)}_i y_i &lt; 0\\), then perform update \\[\\mathbf{w}^{(t+1)} =  \\mathbf{w}^{(t)} + y_i\\mathbf{x}_i.\\]\nThese steps can also be written as: \\[\\mathbf{w}^{(t+1)} =  \\mathbf{w}^{(t)} + \\mathbb{1}[s_i y_i &lt; 0]y_i\\mathbf{x}_i.\\]\n\n\nThe loss is defined to be the proportion of points that are misclassified. The only differences with the minibatch algorithm is that for each time-step \\(t\\), it picks \\(k\\) random points \\(i_1,...,i_k\\) instead of a single point and the update is defined as: \\[\\mathbf{w}^{(t+1)} =  \\mathbf{w}^{(t)} + \\frac{\\alpha}{k}\\sum_{\\ell=1}^k\\mathbb{1}[s_{i_\\ell} y_{i_\\ell} &lt; 0]y_{i_\\ell}\\mathbf{x}_{i_\\ell}\\] where \\(\\alpha\\) is the learning rate hyperparameter which can be tuned to achieve good results.\nMy perceptron uses the function perceptron.grad() to calculate \\[\\mathbb{1}[s_i y_i &lt; 0]y_i\\mathbf{x}_i\\] for each iteration. The minibatch perceptron uses the function perceptron.mbgrad() to calculate \\[\\frac{\\alpha}{k}\\sum_{\\ell=1}^k\\mathbb{1}[s_{i_\\ell} y_{i_\\ell} &lt; 0]y_{i_\\ell}\\mathbf{x}_{i_\\ell}\\] for each iteration.\nBelow are my implementations of perceptron.grad() and perceptron.mbgrad() in Python.\ndef grad(self, X, y):\n    s = X@self.w\n    return (s*y &lt; 0) * y*X\n\ndef mbgrad(self, X, y, k = 2, alpha = 0.1):\n    s = X@self.w\n    return sum(((s*y &lt; 0) * y)[:, None] *X) * alpha / k\nTo test my implementation of the regular perceptron algorithm, I will generate a data set and run a minimal training loop that eventually achieves loss = 0.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nBut before running the test, I will first define some functions that will be used to generate and visualize the data.\n\nimport torch\n\n1def perceptron_data(n_points = 300, noise = 0.2, dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nimport matplotlib.pyplot as plt\n\n2def plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n3def draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n1\n\nGenerates perceptron data.\n\n2\n\nPlots perceptron data.\n\n3\n\nDraws decision boundary line.\n\n\n\n\nI will also define a training loop function that runs the perceptron until loss = 0 or the max amount of iterations is reached and a function that plots the decision boundary and loss for only updates during the training loop.\n\nfrom functools import reduce\n\n1def plot_dims(n):\n    lower_factors = [i for i in range(1, int(n**0.5) + 1) if n % i == 0]\n    return lower_factors[-1], n//lower_factors[-1]\n\ndef training_loop(X, y, k = 1, alpha = 1, max_iters = 1000):\n2    p = Perceptron()\n    opt = PerceptronOptimizer(p)\n    torch.manual_seed(1)\n    p.loss(X, y)\n\n3    loss = 1\n    loss_vec = []\n    old_w_vec = []\n    p_w = []\n    local_loss_vec = []\n    ix_vec = []\n    iters = 0\n    updates = 0\n    seed = 1\n\n4    while loss &gt; 0 and iters &lt; max_iters:\n5        old_w = torch.clone(p.w)\n        \n6        torch.manual_seed(seed)\n        \n7        ix = torch.randperm(X.size(0))[:k]\n        x_ix = X[ix,:]\n        y_ix = y[ix]\n        local_loss = opt.step(x_ix, y_ix, alpha = alpha)\n        \n        seed += 1\n\n8        if local_loss &gt; 0:\n            local_loss_vec.append(local_loss)\n            ix_vec.append(ix)\n            old_w_vec.append(old_w)\n            p_w.append(torch.clone(p.w))\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            updates += 1\n        iters += 1\n        if iters &gt;= max_iters:\n            print('Max iterations reached.')\n\n    return ix_vec, loss_vec, old_w_vec, p_w, local_loss_vec, updates\n\n9def plot_updates(X, y, ix_vec, loss_vec, local_loss, old_w, p_w):\n    updates = len(loss_vec)\n    rows, cols = plot_dims(updates)\n    plt.rcParams[\"figure.figsize\"] = (cols*2, rows*2)\n    fig, axarr = plt.subplots(rows, cols, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n    current_ax = 0\n\n    for j in range(updates):\n        ax = axarr.ravel()[current_ax]\n        i = ix_vec[j]\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w[j], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        draw_line(p_w[j], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        for k in i:\n            ax.scatter(X[k,0],X[k,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[k].item()]])\n        ax.set_title(f\"loss = {loss_vec[j]:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\n    plt.tight_layout()\n\n\n1\n\nGenerates dimensions for plotting the updates of the perceptron using the number of updates \\(n\\) as the input.\n\n2\n\nInitialize perceptron.\n\n3\n\nInitialize variables for main loop.\n\n4\n\nLoop until loss &gt; 0 or max iterations reached.\n\n5\n\nSave the old value of \\(\\mathbf{w}\\) for plotting later.\n\n6\n\nSet seed for reproducible results.\n\n7\n\nMake an optimization step using random point x_i.\n\n8\n\nIf a change was made, save values needed for plotting.\n\n9\n\nFunction to plot updates.\n\n\n\n\nNow, I can generate and plot my data set.\n\n\nCode\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 1: Data to test my perceptron algorithm.\n\n\n\n\n\nNext, I run the minimal training loop and plot each update.\n\n\nCode\ni_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X, y, max_iters = 1000)\nplot_updates(X, y, i_vec, loss_vec, local_loss_vec, old_w, p_w)\n\n\n\n\n\n\n\n\nFigure 2: The dotted lines represent the old decision boundaries and the solid lines represent the updated decision boundaries. The highlighted point represents the random point chosen to update \\(\\mathbf{w}\\).\n\n\n\n\n\nWe can see that it eventually achieves loss = 0 showing that my implementation was successful."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#experimentation-perceptron",
    "href": "posts/perceptron_post/perceptron_post.html#experimentation-perceptron",
    "title": "Perceptron",
    "section": "3 Experimentation: Perceptron",
    "text": "3 Experimentation: Perceptron\nFor the regular perceptron, I will run the following experiments:\n\nVisualize the updates of the perceptron for linearly separable data and the final separating line.\nVisualize the updates of the perceptron for not linearly separable data and the final boundary line in the final iteration since it does not converge.\nVisualize the loss of the perceptron for data with 5 feature and determine if it converges or not.\n\n\n3.1 Linearly Separable Data\nFirst, I generate a linearly separable data set with 50 points. Below is the plot of the data.\n\n\nCode\ntorch.manual_seed(1)\nX1, y1 = perceptron_data(n_points = 50, noise = 0.3)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X1, y1, ax)\n\n\n\n\n\n\n\n\nFigure 3: Plot of linearly separable data (n = 50).\n\n\n\n\n\nNow, I can run the minimal training loop and plot the loss and decision boundary for updates only.\n\n\nCode\ni_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X1, y1, max_iters = 1000)\nplot_updates(X1, y1, i_vec, loss_vec, local_loss_vec, old_w, p_w)\n\n\n\n\n\n\n\n\nFigure 4: Updates for P.1.\n\n\n\n\n\nAs we can see from the plots, the perceptron algorithm can successfully achieve loss = 0 and found a separating line for linearly separable data.\n\n\n3.2 Not Linearly Separable\nFirst, I will generate not linearly separable data with 50 points of data.\n\n\nCode\ntorch.manual_seed(1)\nX2, y2 = perceptron_data(n_points = 50, noise = 0.99)\nn = X2.size()[0]\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nFigure 5: Not linearly separable data (n = 50).\n\n\n\n\n\nNow, I can run the minimal training loop on the data. Because the data is not linearly separable, the perceptron will not converge, so I set the max iterations max_iters = 1000.\n\ni_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X2, y2, max_iters = 1000)\n\nMax iterations reached.\n\n\nWe can see that the message confirms that the perceptron algorithm did not converge withing 1000 iterations. Below is the final decision boundary in the final iteration.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\ndraw_line(p_w[-1], -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nFigure 6: The decision boundary in the final iteration.\n\n\n\n\n\n\n\n3.3 Data with Five Features\nThe perceptron algorithm should still work on data with more than two features. In this part, I will run the perceptron on data with five features.\nFirst, I will generate the necessary data.\n\ntorch.manual_seed(37)\nX3, y3 = perceptron_data(n_points = 100, noise = 0.3, dims = 5)\n\nThen, I run the minimal training loop on the data and plot the loss over time.\n\n\nCode\ni_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X3, y3, max_iters = 1000)\n1plt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(updates), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nxticks = torch.arange(0, updates)\nplt.yticks([0, 0.05, 0.10, 0.15, 0.20])\nplt.xticks(xticks);\n\n\n\n1\n\nPlots loss over time for updates only.\n\n\n\n\n\n\n\n\n\n\nFigure 7: Loss over time for updates only.\n\n\n\n\n\nLooking above, we can see that the perceptron algorithm achieved loss = 0 meaning it successfully converged and found a separating line."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#experimentation-minibatch-perceptron",
    "href": "posts/perceptron_post/perceptron_post.html#experimentation-minibatch-perceptron",
    "title": "Perceptron",
    "section": "4 Experimentation: Minibatch Perceptron",
    "text": "4 Experimentation: Minibatch Perceptron\nFor the minibatch perceptron, I will be running the following experiments: 1. Visualize the updates of the perceptron with k=1 and show that it performs similarly to the regular perceptron. 2. Visualize the updates of the perceptron with k=10 and show it can still find a separating line in 2d. 3. Visualize the updates of the perceptron with k=n and show that it can still converge even if the data is not linearly separable, provided that the learning rate \\(\\alpha\\) is not separable.\nFor the experiments, I will be using the data from Section 3.1 and Section 3.2.\n\nCode\nn = X1.size()[0]\nfig, ax = plt.subplots(1, 1, figsize = (4, 4), sharex = True, sharey = True)\nfig1, ax1 = plt.subplots(1, 1, figsize = (4, 4), sharex = True, sharey = True)\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nax1.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X1, y1, ax)\nplot_perceptron_data(X2, y2, ax1)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Linearly separable data from Section 3.1.\n\n\n\n\n\n\n\n\n\n\n\n(b) Not linearly separable data from Section 3.2.\n\n\n\n\n\n\n\nFigure 8: Plots of data to be used.\n\n\n\n\n4.1 k = 1\nFor this first experimentation, we want to show that the minibatch perceptron performs similarly to the regular perceptron. To do that, I will use the data from Figure 8 (a) to compare results for linearly separable data and the data from Figure 8 (b) for not linearly separable data.\n\n\nCode\nix_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X1, y1, k = 1, alpha = 1, max_iters = 1000)\nplot_updates(X1, y1, ix_vec, loss_vec, local_loss_vec, old_w, p_w)\n\n\n\n\n\n\n\n\nFigure 9: Updates for linearly separable data.\n\n\n\n\n\nNext, let’s run the loop for the not linearly separable data.\n\n\nCode\nix_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X2, y2, k = 1, alpha = 1, max_iters = 1000)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\ndraw_line(p_w[-1], -1, 2, ax, color = \"black\")\n\n\nMax iterations reached.\n\n\n\n\n\n\n\n\nFigure 10: Decision boundary during the final iteration.\n\n\n\n\n\nComparing Figure 9 to Figure 4 and Figure 10 to Figure 6, we can see that the final decision boundary is the same which confirms that the minibatch perceptron performs similarly to the regular perceptron for k = 1.\n\n\n4.2 k = 10\nTo show that the minibatch perceptron will converge for k = 10 for linearly separable data, I will once again use the data from Figure 8 (a).\n\n\nCode\nix_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X1, y1, k = 10, alpha = 1, max_iters = 1000)\nplot_updates(X1, y1, ix_vec, loss_vec, local_loss_vec, old_w, p_w)\n\n\n\n\n\n\n\n\nFigure 11: Updates for k = 10.\n\n\n\n\n\nWe can see that that it eventually achieves loss = 0 meaning that the minibatch perceptron will converge for k = 10.\n\n\n4.3 k = n\nTo show that the minibatch perceptron will converge for k = n even if the data is not linearly separable, I use the data from Figure 8 (b). Even though it will not be able to achieve loss = 0, the loss should still even out at some loss &lt; 0.5.\n\nn = X2.size()[0]\nix_vec, loss_vec, old_w, p_w, local_loss_vec, updates = training_loop(X2, y2, k = n, alpha = 0.001, max_iters = 5000)\n\nMax iterations reached.\n\n\nWe can see that it did not achieve loss = 0 within 5000 iterations. Let’s take a look at the decision boundary during the final iteration, and the loss over time.\n\nCode\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X2, y2, ax)\ndraw_line(p_w[-1], -1, 2, ax, color = \"black\")\n\nfig1, ax1 = plt.subplots(1, 1, figsize = (4, 4))\nplt.plot(loss_vec, color = \"slategrey\")\nax1.scatter(torch.arange(updates), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.semilogx();\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Decision boundary in the final iteration.\n\n\n\n\n\n\n\n\n\n\n\n(b) Loss over time for updates only.\n\n\n\n\n\n\n\nFigure 12: Results of MB.3.\n\n\n\nIn Figure 12 (b), we can see that the loss eventually drops down to about 0.24 then stays between 0.24 an 0.3 which confirms that the minibatch perceptron will converge to some value loss &lt; 0.5 for k = n."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#runtime-complexity",
    "href": "posts/perceptron_post/perceptron_post.html#runtime-complexity",
    "title": "Perceptron",
    "section": "5 Runtime Complexity",
    "text": "5 Runtime Complexity\nFrom these experiments, we can conclude that the minibatch perceptron is more powerful than the regular perceptron. However, does it require more processing power and time to run the minibatch perceptron? More specifically, what are the runtime complexities for each iteration of both perceptron algorithms and are they dependent on the number of data points \\(n\\) or the number of features \\(p\\)?\n\n5.1 Perceptron\nTo do this, let’s review the perceptron algorithms and the code implementation of them. Going back to the update equation for the perceptron, we have \\[\\mathbf{w}^{(t+1)} =  \\mathbf{w}^{(t)} + \\mathbb{1}[s_i y_i &lt; 0]y_i\\mathbf{x}_i.\\] where \\[s_i = \\langle\\mathbf{w}^{(t)}, \\mathbf{x}_i\\rangle.\\] To calculate the score \\(s_i\\), we need to calculate the dot product for \\(\\mathbf{w}^{(t)}\\) and \\(\\mathbf{x}_i\\). Given \\(p\\) features, \\(s_i = \\langle\\mathbf{w}^{(t)}, \\mathbf{x}_i\\rangle\\) has a runtime complexity of \\(O(p)\\). Then going through the rest of the equation we get:\n\n\\(s_i y_i \\implies O(p+1)\\)\n\\(\\mathbb{1}[s_i y_i &lt; 0] \\implies O(2p + 1)\\)\n\\(\\mathbb{1}[s_i y_i &lt; 0]y_i \\implies O(2p + 2)\\)\n\\((\\mathbb{1}[s_i y_i &lt; 0]y_i)\\mathbf{x}_i \\implies O(3p+2) = O(p)\\).\n\nSo, the runtime complexity for the regular perceptron algorithm is \\(O(p)\\) meaning it is only dependent on the number of features \\(p\\).\n\n\n5.2 Minibatch Perceptron\nFor the minibatch perceptron, the update equation is defined as: \\[\\mathbf{w}^{(t+1)} =  \\mathbf{w}^{(t)} + \\frac{\\alpha}{k}\\sum_{\\ell=1}^k\\mathbb{1}[s_{i_\\ell} y_{i_\\ell} &lt; 0]y_{i_\\ell}\\mathbf{x}_{i_\\ell}.\\] Similar to \\(s_i\\), \\(s_{i_\\ell}\\) has a runtime complexity of \\(O(p)\\) and likewise \\((\\mathbb{1}[s_{i_\\ell} y_{i_\\ell} &lt; 0]y_{i_\\ell})\\mathbf{x}_{i_\\ell}\\) has runtime complexity \\(O(p)\\). We then do this \\(k\\) times where \\(k\\) is the number of randomly selected points, so the runtime complexity becomes \\(O(kp)\\). Additionally, we must compute the sum of the resulting vectors for \\(k\\) randomly selected points. Because each vector contains \\(p\\) values and there are \\(k\\) total vectors, the operation will have runtime complexity \\(O(kp)\\). Then, the total runtime complexity is \\(O(2kp) = O(kp)\\).\nTo summarize, the regular perceptron has a runtime complexity of \\(O(p)\\) and the minibatch perceptron has a runtime complexity of \\(O(kp)\\) with the worst case scenario being \\(k=n \\implies O(np)\\). From this, we can see that though the minibatch perceptron is more powerful than the regular perceptron but as a tradeoff, requires more processing power and time to run."
  },
  {
    "objectID": "posts/perceptron_post/perceptron_post.html#discussion",
    "href": "posts/perceptron_post/perceptron_post.html#discussion",
    "title": "Perceptron",
    "section": "6 Discussion",
    "text": "6 Discussion\nFrom the experiments with the perceptron, we found that it could converge to a separating line for linearly separable data with two or more features, but will not be able to converge for not linearly separable data. Then from the experiments with the minibatch perceptron, we found that it performed exactly the same as the regular perceptron when k = 1 and \\(\\alpha = 1\\) when using the same dataset and starting \\(\\mathbf{w}\\). That is, it found the same separating line with the same amount of updates for linearly separable data and the same decision boundary in the final iteration for not linearly separable data. This is due to the fact that the update to \\(\\mathbf{w}\\) for the minibatch perceptron becomes the same as the update to \\(\\mathbf{w}\\) for the regular perceptron when k = 1 and \\(\\alpha = 1\\). Additionally, we found that the minibatch perceptron converges for k = 10 for linearly separable data and it converges to a loss &lt; 0.5 for not linearly separable when k = n.\nComparing the two algorithms, we found that the minibatch perceptron is more powerful than the regular perceptron, but as a result, requires more processing time to run."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html",
    "href": "posts/replication_post/bias_replication_post.html",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "",
    "text": "Image source: berkeley.edu"
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#abstract",
    "href": "posts/replication_post/bias_replication_post.html#abstract",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "1 Abstract",
    "text": "1 Abstract\nThis replication study is based on the journal article “Dissecting racial bias in an algorithm used to manage the health of populations”.\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\nThe article analyzes the racial bias caused by healthcare algorithms that predict healthcare costs instead of illness. In the health system studied by the article, patients with risk scores above the 97th percentile are admitted into the care management program while those in the 55th percentile are referred to their primary care physician who are asked to consider whether the patient would benefit from the program. For equal risk scores, Blacks typically have a higher number of illnesses than Whites. This results in Blacks being less likely to be admitted to the care management program than Whites are. However, it also finds that because these algorithms target patients with high costs, the results are inconsistent by algorithmic bias, such as calibration. That is, across the entire risk distribution, predictions of cost are unbiased towards Whites or Blacks by risk score. This study aimed to replicate these findings through Figure 1 and 3 from the article and calculate the cost disparity shown in Figure 3 using linear regression. After replicating the figures and calculating the cost disparity, this study confirmed the findings of the article. At similar risk scores, Blacks typically are typically more ill than Whites and conditional on risk score, there is no significant difference between predicted costs."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#overview-of-the-data",
    "href": "posts/replication_post/bias_replication_post.html#overview-of-the-data",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "2 Overview of the Data",
    "text": "2 Overview of the Data\nThe authors did not share the “real” data to protect patient privacy. Instead, the data contains attributes for each patient with randomized data that preserves many of the same patterns and trends. More information on the data can be found here. The attributes used in this replication study are:\n\nrisk_score_t: the patient’s risk assigned by the algorithm\ncost_t: the patient’s total medical expenditure during the study period\nrace: the patient’s self-reported race (white or black)\ngagne_sum_t: the patient’s total number of active chronic illnesses during the study period\ndem_female: whether a patient is female (1) or not (0)\n\n\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns"
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#reproducing-figure-1",
    "href": "posts/replication_post/bias_replication_post.html#reproducing-figure-1",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "3 Reproducing Figure 1",
    "text": "3 Reproducing Figure 1\nFigure 1 in the article depicts the relationship between the mean number of chronic illnesses and the percentiles of risk scores for patients divided by race and gender. To reproduce Figure 1, each risk_score_t must be converted into its percentile value. The percentiles are stored in the column risk_percentile.\n\nfrom scipy import stats\n\n1df['risk_percentile'] = stats.percentileofscore(df['risk_score_t'], df['risk_score_t'], kind = 'rank').round(decimals = 0)\n\n\n1\n\nGets the percentile for each risk_score_t by kind = 'rank' and rounds it to the nearest integer.\n\n\n\n\nHere’s what the percentiles of risk scores look like compared to the actual risk scores.\n\ndf[['risk_score_t', 'risk_percentile']].head()\n\n\n\n\n\n\n\n\nrisk_score_t\nrisk_percentile\n\n\n\n\n0\n1.987430\n35.0\n\n\n1\n7.677934\n86.0\n\n\n2\n0.407678\n4.0\n\n\n3\n0.798369\n11.0\n\n\n4\n17.513165\n98.0\n\n\n\n\n\n\n\nBelow is the replication of Figure 1.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n1risk_ill = df.groupby(['risk_percentile', 'race', 'dem_female'], observed = False).agg({'gagne_sum_t':'mean'})\n\n2risk_ill_plt = sns.relplot(data=risk_ill,\n                           col='dem_female',\n                           col_wrap=2,\n                           x = 'gagne_sum_t',\n                           y = 'risk_percentile',\n                           hue = 'race',\n                           style = 'race',\n                           kind='scatter',\n                           height=4,\n                           palette = 'viridis')\n\nrisk_ill_plt.set(xlabel = 'Mean Number of Chronic Illnesses',\n                 ylabel = 'Percentile of Risk Score (from algorithm)',\n                 xticks = [0, 2, 4, 6, 8],\n                 yticks = [0, 20, 40, 60, 80, 100],)\n\nrisk_ill_plt.axes[0].set(title = 'Male')\nrisk_ill_plt.axes[1].set(title = 'Female');\n\n\n\n1\n\nCalculates the mean number of chronic illnesses for a patient grouped by their risk score percentile, race, and gender.\n\n2\n\nPlots the relationship between the mean number of chronic illnesses and risk score percentile.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Male patients\n\n\n\n\nRisk percentile by mean number of chronic illnesses by race and gender.\n\nAs seen in Figure 1 of this study, for a given mean number of chronic illnesses, Blacks are typically given a much lower risk score than Whites are. As a result, they will be much less likely to be referred to the high-risk care management program than Whites will. This corroborates the findings of the article."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#reproducing-figure-3",
    "href": "posts/replication_post/bias_replication_post.html#reproducing-figure-3",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "4 Reproducing Figure 3",
    "text": "4 Reproducing Figure 3\nFigure 3 from the article depicts the the total expenditure for patients given their percentile of risk scores and total number of chronic illnesses. Below is the replication of Figure 3.\n\n\nCode\n1risk_cost = df.groupby(['risk_percentile', 'race'], observed = False).agg({'cost_t':'mean'})\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(1, 2, figsize = (10, 5), sharey = True)\n\n2risk_cost_plt = sns.scatterplot(risk_cost,\n                               x = 'risk_percentile',\n                               y = 'cost_t',\n                               hue = 'race',\n                               style = 'race',\n                               palette = 'viridis',\n                               ax = ax[0])\n\nrisk_cost_plt.set(xlabel = 'Percentile of Risk Scores',\n                 ylabel = 'Mean Total Medical Expenditure',\n                 xticks = [0, 20, 40, 60, 80, 100],\n                 title = 'Total Expenditure by Percentile of Risk Score');\n\n3ax[0].set_yscale('log')\n\n4ill_cost = df.groupby(['gagne_sum_t', 'race'], observed = False).agg({'cost_t':'mean'}).reset_index()\n\n5ill_cost_plt = sns.scatterplot(ill_cost,\n                               x = 'gagne_sum_t',\n                               y = 'cost_t',\n                               hue = 'race',\n                               style = 'race',\n                               palette = 'viridis',\n                               ax = ax[1])\n\nill_cost_plt.set(xticks = [0, 5, 10, 15])\n\nill_cost_plt.set(xlabel = 'Total Number of Chronic Illnesses',\n                 title = 'Total Expenditure by Number of Chronic Illnesses');\n\n\n\n1\n\nCalculates the mean total cost of patients grouped by risk score percentile and race.\n\n2\n\nPlots the mean total cost of patients by risk score percentile.\n\n3\n\nSets the y-axis to be of logarithmic scale.\n\n4\n\nCalculates the mean total cost of patients grouped by the total number of chronic illnesses and race.\n\n5\n\nPlots the mean total cost of patients by total number of chronic illnesses.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Replication of Figure 3 from the article.\n\n\n\n\n\nLooking at Figure 2 from this study, we can see that both Blacks and Whites have similar total costs by both the percentile of risk scores and the total number of chronic illnesses. Additionally, it’s also important to note that most patients have five or fewer chronic illnesses, so looking at Figure 2, we can see that Blacks actually generate lower costs than Whites for patients with fewer than five chronic illnesses. In fact, the article states that Blacks generate on average about $1801 less per year in costs which Figure 1 seems to support."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#modeling-cost-disparity",
    "href": "posts/replication_post/bias_replication_post.html#modeling-cost-disparity",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "5 Modeling Cost Disparity",
    "text": "5 Modeling Cost Disparity\nBuilding on the findings from Figure 1, I attempt to quantify the disparity in cost for patients with five or fewer chronic illnesses. The reason for only looking at patients with five or fewer chronic illnesses, as stated above, is because there are much fewer patients with more than five chronic illnesses. To model the mean total expenditure for patients with five or fewer chronic illnesses, I use polynomial linear regression to predict the logarithm of the costs then extract the coefficient, \\(w_b\\), given to Blacks. In the context of a log-transformed linear model, \\(e^{w_b}\\) is an estimate for the percentage of cost generated by Blacks compared to a Whites with an equal number of chronic illnesses. Additionally, the reason for a polynomial linear regression model is to account for nonlinearity.\nTo confirm the claim that most patients have five or fewer chronic illnesses, here is the percentage of patients with five or fewer chronic illnesses.\n\n1(df['gagne_sum_t'] &lt;= 5).mean() * 100\n\n\n1\n\nCalculates the percentage of patients with five or fewer chronic illnesses.\n\n\n\n\n95.53952115447689\n\n\nIndeed, more than 95% of the patients have five or fewer chronic illnesses. Next, I extract the patients with five or fewer and non-zero expenditure and calculate the logarithm of the costs. The reason for only extracting patients with non-zero expenditure is because \\(\\log{(0)}\\) is undefined.\n\n1df_train = df[(df['gagne_sum_t'] &lt;= 5) & (df['cost_t'] != 0)]\n\nimport numpy as np\n\n2df_train['log_cost_t'] = np.log(df_train['cost_t'])\n\n\n1\n\nExtracts all patients with five or fewer illnesses and non-zero expenditure into a new dataframe, df_train.\n\n2\n\nCreates new column, log_cost_t, that is the logarithm of the costs.\n\n\n\n\nThen, I split the data into the target variable log_cost_t and the predictor variables gagne_sum_t, and race. I then one-hot encode race for linear regression.\n\n1y_train = df_train['log_cost_t']\n2X_train = df_train.drop(['log_cost_t'], axis = 1)\n3X_train = X_train[['gagne_sum_t', 'race']]\n4X_train = pd.get_dummies(X_train, columns=['race'])\n\nX_train.head()\n\n\n1\n\nSet \\(y\\) data to log_cost_t.\n\n2\n\nDrops log_cost_t from \\(x\\) data.\n\n3\n\nGet only gagne_sum_t and race for \\(x\\) data.\n\n4\n\nOne-hot encodes race.\n\n\n\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace_black\nrace_white\n\n\n\n\n0\n0\nFalse\nTrue\n\n\n1\n3\nFalse\nTrue\n\n\n2\n0\nFalse\nTrue\n\n\n3\n0\nFalse\nTrue\n\n\n4\n1\nFalse\nTrue\n\n\n\n\n\n\n\nBelow is a function that constructs new columns for polynomials of various degrees.\n\n1def add_polynomial_features(X, degree):\n  X_ = X.copy()\n\n  for j in range(2, degree):\n    X_[f'poly_{j}'] = X_['gagne_sum_t']**j\n  return X_\n\n\n1\n\nCreates a new columns $2 = ^2,… , j = ^j $.\n\n\n\n\nNext, I use cross validation to find the best degree polynomial for the linear regression model.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n1LR = LinearRegression()\n\n2degree = np.arange(1, 21)\n3cross_score = np.zeros(len(degree))\n\n4for i in degree:\n    X = add_polynomial_features(X_train, i)\n    cross_score[i-1] = cross_val_score(LR, X, y_train, cv = 5).mean()\n\n\n1\n\nInitializes linear regression instance.\n\n2\n\nVector to of degrees to test.\n\n3\n\nVector of zeroes to store the mean cross validation scores.\n\n4\n\nTests all degrees in and stores the mean cross validation scores in cross_score. Uses five-fold cross validation.\n\n\n\n\nAfter, I get the degree (14) with the best score for use in the linear regression model.\n\n1best_i = np.argmax(cross_score)\n2best_d = degree[best_i]\nbest_d\n\n\n1\n\nGets index of best mean cross validation score.\n\n2\n\nGets the degree with best mean cross validation score.\n\n\n\n\n14\n\n\nNow, I train the linear regression model using the best degree polynomial and calculate the estimated percentage of cost disparity between Blacks and Whites with the same number of chronic illnesses.\n\n1X = add_polynomial_features(X_train, best_d)\n2LR.fit(X, y_train)\nscore = LR.score(X, y_train)\n3w = LR.coef_\n4np.exp(w[1])\n\n\n1\n\nSet X to be dataset with the 14 degree polynomial of gagne_sum_t.\n\n2\n\nTrain X using linear regression and retrieve the score.\n\n3\n\nGet the coefficients of the trained model and store it in w.\n\n4\n\nCalculates the estimated percentage of cost disparity between Blacks and Whites with the same number of chronic illnesses (\\(e^{w[1]}\\) where \\(w[1] = w_b\\))\n\n\n\n\n0.8680649776385329\n\n\nAs we can see, Blacks generate an estimated 86.8% the cost of Whites. This roughly confirms the findings from the article that Blacks generate less costs than Whites."
  },
  {
    "objectID": "posts/replication_post/bias_replication_post.html#discussion",
    "href": "posts/replication_post/bias_replication_post.html#discussion",
    "title": "Replication Study: Racial Bias in Health Algorithms",
    "section": "6 Discussion",
    "text": "6 Discussion\nFor similar risk scores and number of illnesses, Blacks and Whites generate similar costs (Figure 1). In fact, Blacks generate an estimated 87% less cost than Whites. However, when comparing Blacks and Whites by their risk score based on their chronic number of illnesses, Blacks have a much lower risk score per number of illnesses (Figure 1). As a result, they are less likely to be accepted into the system’s care management program. Though the study meets the calibration criterion for fairness, it violates the independence criterion for fairness as Blacks have a lower acceptance rate into the system’s care managemet program than Whites. This could present a problem, since most industry-wide algorithms predict cost leading to bias against Blacks. This results in a system wide case of Blacks being unable to receive the healthcare they need, which in turn reduces their medical expenditures. To fix this bias, the authors of the article used an index variable which combined cost and health prediction, reducing the bias by 87%. The hope is that by showing that label bias is fixable other manufacturers will be prompted to make similar fixes, minimizing the risks of algorithmic predictions while still benefitting from them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Machine Learning",
    "section": "",
    "text": "Quantitative Trading Model Using LSTM\n\n\n\n\n\nBuilding and testing an LSTM model for stock price prediction.\n\n\n\n\n\nMay 16, 2024\n\n\nAndre Xiao, James Ohr, Donovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nImplementing and experimenting with Newton’s method for logistic regression.\n\n\n\n\n\nMay 2, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nImplementing and experimenting with logistic regression.\n\n\n\n\n\nApr 15, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\n\n\nImplementing and experimenting with the perceptron algorithm.\n\n\n\n\n\nApr 7, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study: Racial Bias in Health Algorithms\n\n\n\n\n\nA replication study of “Dissecting racial bias in an algorithm used to manage the health of populations” by Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan.\n\n\n\n\n\nMar 7, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nBank Loans\n\n\n\n\n\nOptimizing and assessing the impact of an automated decision system for bank loans.\n\n\n\n\n\nMar 3, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nPredicting penguin species from the Palmer Archipelago.\n\n\n\n\n\nFeb 21, 2024\n\n\nAndre Xiao\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/project_post/axiao_research.html",
    "href": "posts/project_post/axiao_research.html",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "import yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})\n\n\n\n\n\nfrom pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')\n\n\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515\n\n\n\n\n\n\n\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#abstract",
    "href": "posts/project_post/axiao_research.html#abstract",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "What problem did we address?\nWhat approach(es) did we use to address it?\nWhat are the big-picture results?"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#introduction",
    "href": "posts/project_post/axiao_research.html#introduction",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Prompt: “Your introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.”\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/project_post/axiao_research.html#values",
    "href": "posts/project_post/axiao_research.html#values",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Who are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?\n\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#model-analysis",
    "href": "posts/project_post/axiao_research.html#model-analysis",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#pytrends",
    "href": "posts/project_post/axiao_research.html#pytrends",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "from pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#basket-analysis",
    "href": "posts/project_post/axiao_research.html#basket-analysis",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "tickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#feature-selection",
    "href": "posts/project_post/axiao_research.html#feature-selection",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "from sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113"
  },
  {
    "objectID": "posts/project_post/axiao_research.html#lstm",
    "href": "posts/project_post/axiao_research.html#lstm",
    "title": "Andre Xiao Preliminary Research",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/project_post/dwood_test.html#donovan-test-code-just-to-get-started",
    "href": "posts/project_post/dwood_test.html#donovan-test-code-just-to-get-started",
    "title": "Donovan test code just to get started",
    "section": "Donovan test code just to get started",
    "text": "Donovan test code just to get started\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Define the ticker and the time period\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch data\ndata = yf.download(ticker, start=start_date, end=end_date)\nprint(data.head())\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  38.722500  39.712502  38.557499  39.480000  37.845047  148158800\n2019-01-03  35.994999  36.430000  35.500000  35.547501  34.075401  365248800\n2019-01-04  36.132500  37.137501  35.950001  37.064999  35.530048  234428400\n2019-01-07  37.174999  37.207500  36.474998  36.982498  35.450970  219111200\n2019-01-08  37.389999  37.955002  37.130001  37.687500  36.126774  164101200\n\n\n\n# Moving Average \n\nshort_window = 40\nlong_window = 100\n\ndata['Short_MAvg'] = data['Close'].rolling(window=short_window, min_periods=1).mean()\ndata['Long_MAvg'] = data['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ndata['Signal'] = 0\ndata['Signal'] = np.where(data['Short_MAvg'] &gt; data['Long_MAvg'], 1, 0)\n\n# Generate trading orders\ndata['Position'] = data['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(data['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(data[data['Position'] == 1].index, data['Short_MAvg'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(data[data['Position'] == -1].index, data['Short_MAvg'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMessing around with mean reversion\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate the moving average and the standard deviation\nwindow = 20\ndata['Moving_Average'] = data['Close'].rolling(window=window).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=window).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['Moving_Average']) / data['Std_Dev']\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ndata['Signal'] = 0\ndata['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ndata['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ndata['Position'] = data['Signal'].replace(0, np.nan).ffill().fillna(0)\n\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Moving_Average'], label='Moving Average', alpha=0.75)\nplt.fill_between(data.index, data['Moving_Average'] - data['Std_Dev'], data['Moving_Average'] + data['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(data[data['Position'] == 1].index, data['Close'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(data[data['Position'] == -1].index, data['Close'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow will actually do some machine learning stuff. Add some random features and try to make it work\n\nimport yfinance as yf\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n#Trying random forest here just for fun, could use LR or something else too\n\n\nticker = 'AAPL'\ndata = yf.download(ticker, start=\"2010-01-01\", end=\"2020-01-01\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate moving averages\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n\n# Data if it goes up or down\n\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n\nfeatures = ['SMA_20', 'SMA_50', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.5080971659919028\n\n\n\ndata['Predicted_Signal'] = model.predict(X)\ndata['Strategy_Returns'] = data['Returns'] * data['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (data['Strategy_Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.legend()\nplt.show()\n\n# Not fully correct but good starting point. Just wanted to start familiarizing myself with the library and the data."
  },
  {
    "objectID": "posts/project_post/prelim-analysis.html",
    "href": "posts/project_post/prelim-analysis.html",
    "title": "James Ohr Preliminary Research",
    "section": "",
    "text": "James Ohr Preliminary Research\n\n#Importing the data and storing it in variable df_all\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_all = pd.read_csv(\"crox-early-data.csv\")\n\ndf_all = df_all.dropna()\n\ndf_train, df_test = train_test_split(df_all, test_size=0.3, random_state=1)\nX_train = df_train.drop(columns=['Close Higher'])\ny_train = df_train['Close Higher']\n\nX_test = df_test.drop(columns=['Close Higher'])\ny_test = df_test['Close Higher']\n\nX_train.head()\n\n\ndf_all.groupby(\"Close Higher\").mean(numeric_only=True)\n\n\n#Choosing features\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\ncols = [\"Previous GT\", \"TTM PE\"]\n\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\nLR.coef_[0]\n\n\ndef linear_score(w, x0, x1):\n    return w[0]*x0 + w[1]*x1\n\n\n#Predict makes binary predictions for data using a supplied score function with weights w and a supplied threshold. Taken from lecture notes from week 2.\n#We begin with a 0 threshold but later on test others to find an optimal threshold\n\nt = 0\n\ndef predict(score_fun, w, threshold, df):\n    \"\"\"\n    make binary predictions for data df using a supplied score function with weights w and supplied threshold. \n    \"\"\"\n    scores = score_fun(w, df[\"Previous GT\"], df[\"TTM PE\"])\n    return 1*(scores &gt; threshold)\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_all)\n(df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()\n\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\niterations = 200\npredictions = []\nfor i in range(iterations):\n    threshold = (-iterations/2)+(i)\n    df_train[\"decision\"] = predict(linear_score, LR.coef_[0], threshold, df_train)\n    predictions.append((threshold, (df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()))\n\n\npredictions_df = pd.DataFrame(data=predictions)\npredictions_df.columns =['Threshold', 'Accuracy']\n\nsns.relplot(data=predictions_df, x=\"Threshold\", y=\"Accuracy\")\n\nt = predictions_df['Threshold'][predictions_df['Accuracy'].idxmax()]\n\npredictions_df['Threshold'][predictions_df['Accuracy'].idxmax()], predictions_df['Accuracy'].max()\n\n\ndf_test[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_test)\n(df_test[\"decision\"] == df_test[\"Close Higher\"]).mean()"
  },
  {
    "objectID": "posts/project_post/quant_research.html",
    "href": "posts/project_post/quant_research.html",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Image source: https://masterthecrypto.com/wp-content/uploads/2019/11/quantitative-trading.jpg"
  },
  {
    "objectID": "posts/project_post/quant_research.html#abstract",
    "href": "posts/project_post/quant_research.html#abstract",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Abstract",
    "text": "Abstract\nWe applied machine learning methods to predict daily stock price movements in a basket of 10 US-listed energy companies. We found the most success using an LSTM model, achieving an accuracy of up to 61% on one stock (PSX). In line with prior literature, we compared our results to a benchmark established by a last value machine, which simply predicts the next day’s price to be the current day’s actual price. Comparing our LSTM results to our benchmark, we find mixed results. We achieve, on average, 53.5% accuracy vs. our benchmark’s 53% accuracy. For the ten companies in our analysis, our model has superior for accuracy for six companies, has equal accuracy for one company, and has worse accuracy for three companies compared to our last value benchmark. This superior accuracy leads to higher simulated portfolio returns using our model compared to our benchmarks."
  },
  {
    "objectID": "posts/project_post/quant_research.html#introduction",
    "href": "posts/project_post/quant_research.html#introduction",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Introduction",
    "text": "Introduction\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Gunduz (2021), Bhandari et al. (2022), and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with MSE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/project_post/quant_research.html#values",
    "href": "posts/project_post/quant_research.html#values",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Values",
    "text": "Values\nThe potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there’s a buyer and a seller, so in every trade, there’s a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program.\nUltimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some “true” value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day’s price, it should accelerate the market’s convergence to an appropriate value.\nA useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an “optimal” allocation of money in society.\nWe are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master’s in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today’s markets. This forces us to adopt new techniques. # Materials and Methods"
  },
  {
    "objectID": "posts/project_post/quant_research.html#our-data",
    "href": "posts/project_post/quant_research.html#our-data",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Our Data",
    "text": "Our Data\nOur data was sourced from Yahoo Finance. We used the yfinance library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 6th, 2014 to May 6th, 2024.\nWithin the yfinance dataset we were given the following columns: Open, High, Low, Close, Adj Close, Volume.\nOpen is the opening price of the stock for the day. High is the highest price of the stock for the day. Low is the lowest price of the stock for the day. Close is the closing price of the stock for the day. Adj Close is the adjusted closing price of the stock for the day. Volume is the number of shares traded for the day.\nWe used the Close column as our target variable for our model. We also created the following features: SMA_20, SMA_50, Std_Dev, Z_Score, RSI, TTM_P/E which will be discussed below. Here’s a look at what the raw data looks like:\n\nimport yfinance as yf\n\nxom = yf.Ticker('XOM')\ndata = xom.history(start='2014-05-06', end='2024-05-07')\ndata.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2014-05-06 00:00:00-04:00\n66.050586\n66.501227\n65.928274\n66.095650\n9669800\n0.00\n0.0\n\n\n2014-05-07 00:00:00-04:00\n66.385376\n66.597816\n66.172931\n66.378937\n11007400\n0.00\n0.0\n\n\n2014-05-08 00:00:00-04:00\n66.366038\n66.494794\n65.773772\n65.870338\n8922500\n0.00\n0.0\n\n\n2014-05-09 00:00:00-04:00\n65.922184\n66.226810\n65.630524\n66.077736\n8948800\n0.69\n0.0\n\n\n2014-05-12 00:00:00-04:00\n66.324029\n66.336990\n65.805516\n66.259216\n8830500\n0.00\n0.0\n\n\n\n\n\n\n\nYou can find the full implementation of our data at lstm_data.py under the function prepare_data()."
  },
  {
    "objectID": "posts/project_post/quant_research.html#our-approach",
    "href": "posts/project_post/quant_research.html#our-approach",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Our Approach",
    "text": "Our Approach\n\nFeatures of Our Data & Target Variable\nWe used SMA_20, SMA_50, Std_Dev, Z_Score, RSI, Close, TTM_P/E as predictors for our models.\nThe SMA_20 and SMA_50 are the 20-day and 50-day simple moving averages of the stock price. This means that the average closing price of the stock over the last 20 and 50 days, respectively.\nThe Std_Dev is the standard deviation of the stock price meaning how much the stock price deviates from the mean.\nThe Z_Score is the z-score of the stock price meaning how many standard deviations the stock price is from the mean.\nThe RSI is the relative strength index of the stock price meaning how strong the stock price is relative to its past performance. It is calculated by taking the average of the gains and losses over a certain period of time.\nThe Close is the closing price of the stock per day.\nThe TTM_P/E is the trailing twelve months price-to-earnings ratio of the stock.\nWe used the next day’s Close price as the target variable for our model.\n\n\nData Manipulation\nWe collected 10 years of data from May 7th, 2014 to May 7th, 2024 and used a train-test split of 90-10 in order to train our model on the first 9 years worth of the data and test it on the remaining 1 year’s worth of data. We used a standard scaler for scaling our data in order to ensure that the data was normalized. We fit the scaler on the training data and then applied it to the test data to avoid any information leaking. We then combined the training data for each stock into one dataset. We used the closing price of the stock as the target variable for our model.\nHere’s what our data looks like after creating our features and scaling the data:\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'EPD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX', 'OXY']\nstart = '2014-05-06'\nend = '2024-05-07'\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)\nX_train\n\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nSMA_250\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nTicker\n\n\n\n\n2015-05-01\n-0.139877\n-0.131185\n-0.112171\n-0.102371\n-0.540891\n-0.127836\n0.00000\n-0.149377\n-0.173825\n-0.178037\n-0.063809\n0.319177\n-0.679901\n1.186546\n0.879810\n0.650885\n-0.095652\n0.893759\nXOM\n\n\n2015-05-04\n-0.080169\n-0.096257\n-0.064154\n-0.092513\n-0.732495\n-0.127836\n0.00000\n-0.142218\n-0.166130\n-0.178686\n-0.064625\n0.316818\n-0.653451\n1.177769\n0.973560\n0.650885\n-0.095424\n0.133748\nXOM\n\n\n2015-05-05\n-0.059251\n-0.088369\n-0.080672\n-0.111473\n-0.577151\n-0.127836\n0.00000\n-0.135595\n-0.160600\n-0.179007\n-0.065519\n0.314254\n-0.642313\n0.791839\n0.672019\n0.650885\n-0.095861\n-0.329225\nXOM\n\n\n2015-05-06\n-0.071421\n-0.093251\n-0.108714\n-0.127400\n-0.639387\n-0.127836\n0.00000\n-0.132800\n-0.152615\n-0.179992\n-0.065441\n0.311766\n-0.751693\n0.518627\n0.427504\n0.650885\n-0.096229\n-0.281992\nXOM\n\n\n2015-05-07\n-0.134173\n-0.163112\n-0.142135\n-0.149774\n-0.767664\n-0.127836\n0.00000\n-0.132571\n-0.146911\n-0.181595\n-0.065840\n0.309079\n-0.859104\n0.047923\n0.100192\n0.650885\n-0.096744\n-0.387753\nXOM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-02\n0.610803\n0.625792\n0.636749\n0.650958\n0.109535\n-0.105084\n-0.02214\n0.607044\n0.598059\n0.727834\n0.786385\n0.982249\n-0.739053\n0.869582\n0.136220\n1.924677\n-0.222355\n0.810058\nOXY\n\n\n2023-06-05\n0.714326\n0.672373\n0.676732\n0.633996\n-0.312158\n-0.105084\n-0.02214\n0.608629\n0.594259\n0.728931\n0.783608\n0.979744\n-0.834725\n0.776482\n0.011823\n1.924677\n-0.222464\n-0.165726\nOXY\n\n\n2023-06-06\n0.590332\n0.602501\n0.623814\n0.636336\n-0.325573\n-0.105084\n-0.02214\n0.610977\n0.595673\n0.730434\n0.780891\n0.977007\n-0.819762\n0.773031\n0.029482\n1.924677\n-0.222449\n0.003637\nOXY\n\n\n2023-06-07\n0.652329\n0.667133\n0.694371\n0.671429\n-0.292849\n-0.105084\n-0.02214\n0.613090\n0.598854\n0.730900\n0.777538\n0.974819\n-0.772194\n1.238762\n0.294437\n1.924677\n-0.222223\n0.292064\nOXY\n\n\n2023-06-08\n0.669287\n0.645055\n0.635925\n0.662629\n-0.147267\n2.262466\n-0.02214\n0.616022\n0.607809\n0.728131\n0.774355\n0.973557\n-0.847555\n1.077118\n0.218085\n1.924677\n-0.222280\n-0.093700\nOXY\n\n\n\n\n20410 rows × 19 columns\n\n\n\n\n\nModels We Employed\nOriginally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. LSTM models are a type of recurrent neural network (RNN) with the addition of “gates” notably the input, forget and output gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\nThe forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step \\(h_{t-1}\\) and the input \\(x_t\\) at the current time step. The forget gate is defined as:\n\\[f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\]\nThe input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components:\n\nGetting the state of the cell that must be updated.\nCreate a new cell state\nUpdate the cell state to the current cell state\n\nThese are defined as:\n\\[\\begin{aligned}\ni_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\nC_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n\\end{aligned}\\]\nThe output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n\\[o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\\]\nThis output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\nWhich is defined as:\n\\[h_{t} = o_{t} \\ast \\tanh(C_{t})\\]\nTaking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\nThe implementation of our LSTM model can be found at: lstm_model.py\n\n\nTraining Our Models\nWe first converted our wanted feature columns into a torch Variable to allow them to be differentiable. Then, we reshaped the data using torch.reshape() and torch.utils.data.DataLoader into [batch_size, seq_len, input_size].\n\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nnext(iter(data_loader_train))[0].shape\n\ntorch.Size([2041, 1, 7])\n\n\nWe trained our model using our own personal devices. We used the Adam optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total) and used the torch.nn.MSELoss() loss function to train the model.\nMSE is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_{i} - y_{i})^2\\]\nWhere \\(y_{i}\\) is the true price and \\(\\hat{y}_{i}\\) is the predicted price.\nAs mentioned previously our model was trained on 90% of the data and tested on the remaining 10%.\nIf the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark.\nBelow is our training code:\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n# training loop\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.91365\nEpoch: 100, loss: 0.00494\nEpoch: 200, loss: 0.00502\nEpoch: 300, loss: 0.00508\nEpoch: 400, loss: 0.00451\nEpoch: 500, loss: 0.00478\nEpoch: 600, loss: 0.00479\nEpoch: 700, loss: 0.00491\nEpoch: 800, loss: 0.00514\nEpoch: 900, loss: 0.00504\n\n\n\n\nModel Evaluation\nWe evaluated our model by comparing the cumulative predicted stock price returns and accuracy to the actual cumulative stock price returns and accuracy and the cumulative last value benchmark returns and accuracy. The last value benchmark is defined as using the previous days value as the prediction for the current day. We would buy the stock at the current day’s close price and sell at the next day’s close price if the predicted returns were positive and do nothing if the predicted returns were negative. We followed the same principle in calculating actual cumulative stock returns and accuracy, and the cumulative last value benchmark returns and accuracy.\nWe define accuracy for our purposes as percentage of times the model correctly predicts an upward or downward movement in the share price of a company.\nConsider a simple test case where the model predicts the stock price to go up and the stock price actually goes up. In this case, the model is correct. If the model predicts the stock price to go up and the stock price actually goes down, the model is incorrect. We calculate the accuracy of the model by dividing the number of correct predictions by the total number of predictions.\nAccuracy per Stock\n\n\nCode\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n        lv_correl_list = np.array([lv_prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n        lv_correl_list = np.append(lv_correl_list, np.array([lv_prediction_correl]))\n\n\nXOM Accuracy: 0.5418502202643172, Correlation: 0.972313389714224, Last Value Accuracy: 0.5242290748898678, Last Value Correlation: 0.9780145037734742\nCVX Accuracy: 0.5154185022026432, Correlation: 0.9613024878319475, Last Value Accuracy: 0.5066079295154186, Last Value Correlation: 0.9635678490020599\nCOP Accuracy: 0.5550660792951542, Correlation: 0.9689221084107935, Last Value Accuracy: 0.5418502202643172, Last Value Correlation: 0.9783667790929887\nEPD Accuracy: 0.5374449339207048, Correlation: 0.9860702298831613, Last Value Accuracy: 0.5506607929515418, Last Value Correlation: 0.9916250412456891\nEOG Accuracy: 0.5506607929515418, Correlation: 0.9686894982563453, Last Value Accuracy: 0.5462555066079295, Last Value Correlation: 0.9721381776147026\nDUK Accuracy: 0.4933920704845815, Correlation: 0.9693304786167968, Last Value Accuracy: 0.4889867841409692, Last Value Correlation: 0.9698313657418474\nMPC Accuracy: 0.5770925110132159, Correlation: 0.9716364908547462, Last Value Accuracy: 0.5550660792951542, Last Value Correlation: 0.9941712442141354\nSLB Accuracy: 0.4669603524229075, Correlation: 0.9745432552946375, Last Value Accuracy: 0.4669603524229075, Last Value Correlation: 0.9752418496648181\nPSX Accuracy: 0.6079295154185022, Correlation: 0.9801502512089683, Last Value Accuracy: 0.6123348017621145, Last Value Correlation: 0.995576915466044\nOXY Accuracy: 0.5110132158590308, Correlation: 0.9636470842152328, Last Value Accuracy: 0.5154185022026432, Last Value Correlation: 0.9638295520210561\n\n\nAverage Accuracy\nBelow shows the overall accuracy, summed across our 10 stocks, vs the Last Value Benchmark.\nWe find that the average accuracy of our model slightly outperforms the last value benchmark, but our correlation slighty underperforms the last value benchmark.\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_correl_list.mean()}')\n\n\nAvg Accuracy: 0.5356828193832598, Avg Correlation: 0.9716605274286853, Avg LV Accuracy: 0.5308370044052864, Avg LV Correlation: 0.9782363277836815\n\n\nCumulative Returns\nThe code below shows the comparison between our strategy returns, the baseline stock returns, and the last value benchmark returns.\nWe find that our strategy outperforms the baseline stock returns and the last value benchmark returns.\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Year Portfolio Returns: {total_strat_returns}')\nprint(f'1 Year Stock Returns: {total_stock_returns}')\nprint(f'1 Year LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Year Portfolio Returns: 1.2360575366734956\n1 Year Stock Returns: 1.2066049840833908\n1 Year LV Returns: 1.195690607517653\n\n\n\n\n\n\n\n\nFigure 1: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark."
  },
  {
    "objectID": "posts/project_post/quant_research.html#live-mock-testing",
    "href": "posts/project_post/quant_research.html#live-mock-testing",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Live Mock Testing",
    "text": "Live Mock Testing\nFor fun, we decided to do a live mock test for the past week (2024/05/07 - 2024/05/16) to see how are model does on current data. We followed the same procedures as above except we trained on 10 years of data prior to our test week.\n\n\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n\n\n\n\nEpoch: 0, loss: 0.89484\nEpoch: 100, loss: 0.00404\nEpoch: 200, loss: 0.00378\nEpoch: 300, loss: 0.00369\nEpoch: 400, loss: 0.00369\nEpoch: 500, loss: 0.00373\nEpoch: 600, loss: 0.00389\nEpoch: 700, loss: 0.00382\nEpoch: 800, loss: 0.00376\nEpoch: 900, loss: 0.00399\n\n\n\n\n['lstm_live.joblib']\n\n\n::: {#cell-26 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=67}\nlstm_live = load('lstm_live.joblib')\n:::\nHere are the accuracies and correlations for each stock based on our strategy and last value:\n\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm_live, X_test_list[i], y_test_list[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n\nXOM Accuracy: 0.5, Correlation: 0.45596059959860424, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.442960265038012\nCVX Accuracy: 0.6666666666666666, Correlation: 0.33422337988058554, Last Value Accuracy: 0.5, Last Value Correlation: 0.34341958688787927\nCOP Accuracy: 0.6666666666666666, Correlation: 0.7971408777137727, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7558464387577114\nEPD Accuracy: 0.3333333333333333, Correlation: 0.8684150010738954, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.8603779889931645\nEOG Accuracy: 0.3333333333333333, Correlation: 0.2363276728915621, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.25582456053077646\nDUK Accuracy: 0.3333333333333333, Correlation: -0.09651392314802673, Last Value Accuracy: 0.5, Last Value Correlation: -0.159262707461855\nMPC Accuracy: 0.6666666666666666, Correlation: 0.7663651315254851, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7016596578784171\nSLB Accuracy: 0.5, Correlation: 0.42526094830678546, Last Value Accuracy: 0.5, Last Value Correlation: 0.44817487130500694\nPSX Accuracy: 0.6666666666666666, Correlation: 0.19340309591226992, Last Value Accuracy: 0.8333333333333334, Last Value Correlation: 0.27552121319099077\nOXY Accuracy: 0.3333333333333333, Correlation: 0.40172522417713447, Last Value Accuracy: 0.16666666666666666, Last Value Correlation: 0.4083298375864765\n\n\nHere are the average accuracies and correlations:\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_prediction_correl.mean()}')\n\n\nAvg Accuracy: 0.4999999999999999, Avg Correlation: 0.43823080079320686, Avg LV Accuracy: 0.4833333333333333, Avg LV Correlation: 0.4083298375864765\n\n\nHere are the returns for the strategy, stocks, and last value:\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test_list[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test_list[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test_list[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Week Portfolio Returns: {total_strat_returns}')\nprint(f'1 Week Stock Returns: {total_stock_returns}')\nprint(f'1 Week LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Week Portfolio Returns: 0.9981978730664818\n1 Week Stock Returns: 0.9968738574121445\n1 Week LV Returns: 0.9952073325941001\n\n\n\n\n\n\n\n\nFigure 2: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark for 2024-05-07 to 2024-05-16."
  },
  {
    "objectID": "posts/project_post/quant_research.html#concluding-discussion",
    "href": "posts/project_post/quant_research.html#concluding-discussion",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project was a success in the sense that we constructed a model that is profitable and more accurate than our benchmarks. We also achieved an average correlation coefficient of 0.9979 between our predicted prices and actual prices, which is on par with the best results from Bhandari et al. (2022) . We didn’t take the same approach as we initially expected of using alternative data, instead using more conventional features, but the ultimate goal was accuracy and profitability, so we aren’t concerned by this change in methodology. We had substantially lower accuracy than Gunduz (2021) , who achieved accuracy of up to 0.675. There are many factors that could have contributed to this difference, including time frames (Gunduz used hourly data vs. our daily data), company geographies (Gunduz studied companies on the Borsa Istanbul), and number of features (Gunduz created a substantially greater number of features).\nThere are a two main key assumptions worth noting. Our tests above make two simplifying assumptions about trading. First, we assume the entire portfolio enters every trade, which any reasonable asset manager would think is incredibly reckless and is a major risk management failure. Second, we assume we are able to buy and sell stocks exactly at their closing price on a given day. This isn’t as problematic an assumption as the first, but it’s still an assumption that may not reflect real-world circumstances, especially when trading small stocks with low trading volumes or, more generally, when trading with enough capital to influence stock prices.\nIf we had more time, data, and computational resources, we would have explored creating and filtering a substantially greater number of features. We also would have liked to have worked with larger baskets of companies. We chose energy companies based on intuition that training a model on data from the same industry would result in better predictions."
  },
  {
    "objectID": "posts/project_post/quant_research.html#group-contributions-statement",
    "href": "posts/project_post/quant_research.html#group-contributions-statement",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nAndre did research on RFE using logistic regression, random forest, and support vector machine before pivoting to an LSTM axiao_research.ipynb. He wrote the source code for the data preparation in lstm_data.py, the LSTM model in lstm_model.py, and the evaluation. He wrote the code for the plots for comparing cumulative returns and the code for calculating the accuracy of the strategy and the benchmarks.\nDonovan provided the initial research and the code for calculating the features SMA_20, SMA_50, RSI, Z_Score, and Std_Dev. He provided visualizations for the moving averages and performed inital tests using logistic regression in dwood_test.ipynb. He wrote the data section.\nJames worked on an early analysis using Google Trends data in prelim-analysis.ipynb, which we pivoted away from after realizing the limited supply of daily data. He created the presentation and wrote the abstract, the introduction, the values, and the conclusion sections. He also wrote the code to calculate the correlation coefficients between predicted prices and actual prices."
  },
  {
    "objectID": "posts/project_post/quant_research.html#personal-reflection",
    "href": "posts/project_post/quant_research.html#personal-reflection",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nI learned a lot about quantitative finance revolving around which features, models, and benchmarks were useful. We started with traditional machine learning models, but through research, I learned about LSTMs which are used for sequential data, which stock prices are. I learned the math and theory behind LSTMs and how to implement an LSTM using PyTorch. I also learned how to process the data required for an LSTM and its evaluation. I also learned about the last value benchmark used in evaluating an LSTM. I reacquainted myself with git and learned how to use VSCode Live Share.\nI think I achieved my goals for this project. I initially wanted to learn how to apply machine learning to quantitative finance through practical implementation and through reading research papers. I managed to do both and additionally, also learned about and how to implement a new neural network model that we didn’t cover in class.\nGoing forward, I think this project helped me figure out how to organize my research and how to build on my code in an iterative manner. In the beginning, my code was a bit messy and hard to build on, but in the end, it became a lot smoother. I also think that working with git will help prepare me for a job in data science, machine learning, software development, or quantitative finance. On the practical side, I think I can definitely build on my research into LSTMs and its effectiveness in predicting stock prices. There are a lot of research papers on the topic that will allow me to explore the topic further."
  },
  {
    "objectID": "posts/project_post/dwood_test.html",
    "href": "posts/project_post/dwood_test.html",
    "title": "Donovan Wood Preliminary Research",
    "section": "",
    "text": "Donovan Wood Preliminary Research\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Define the ticker and the time period\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch data\ndata = yf.download(ticker, start=start_date, end=end_date)\nprint(data.head())\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  38.722500  39.712502  38.557499  39.480000  37.845047  148158800\n2019-01-03  35.994999  36.430000  35.500000  35.547501  34.075401  365248800\n2019-01-04  36.132500  37.137501  35.950001  37.064999  35.530048  234428400\n2019-01-07  37.174999  37.207500  36.474998  36.982498  35.450970  219111200\n2019-01-08  37.389999  37.955002  37.130001  37.687500  36.126774  164101200\n\n\n\n# Moving Average \n\nshort_window = 40\nlong_window = 100\n\ndata['Short_MAvg'] = data['Close'].rolling(window=short_window, min_periods=1).mean()\ndata['Long_MAvg'] = data['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ndata['Signal'] = 0\ndata['Signal'] = np.where(data['Short_MAvg'] &gt; data['Long_MAvg'], 1, 0)\n\n# Generate trading orders\ndata['Position'] = data['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(data['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(data[data['Position'] == 1].index, data['Short_MAvg'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(data[data['Position'] == -1].index, data['Short_MAvg'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMessing around with mean reversion\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate the moving average and the standard deviation\nwindow = 20\ndata['Moving_Average'] = data['Close'].rolling(window=window).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=window).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['Moving_Average']) / data['Std_Dev']\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ndata['Signal'] = 0\ndata['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ndata['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ndata['Position'] = data['Signal'].replace(0, np.nan).ffill().fillna(0)\n\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Moving_Average'], label='Moving Average', alpha=0.75)\nplt.fill_between(data.index, data['Moving_Average'] - data['Std_Dev'], data['Moving_Average'] + data['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(data[data['Position'] == 1].index, data['Close'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(data[data['Position'] == -1].index, data['Close'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow will actually do some machine learning stuff. Add some random features and try to make it work\n\nimport yfinance as yf\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n#Trying random forest here just for fun, could use LR or something else too\n\n\nticker = 'AAPL'\ndata = yf.download(ticker, start=\"2010-01-01\", end=\"2020-01-01\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate moving averages\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n\n# Data if it goes up or down\n\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n\nfeatures = ['SMA_20', 'SMA_50', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.5080971659919028\n\n\n\ndata['Predicted_Signal'] = model.predict(X)\ndata['Strategy_Returns'] = data['Returns'] * data['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (data['Strategy_Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.legend()\nplt.show()\n\n# Not fully correct but good starting point. Just wanted to start familiarizing myself with the library and the data."
  }
]